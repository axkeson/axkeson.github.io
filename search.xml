<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用 Supervisord 管理服务进程]]></title>
    <url>%2F2018%2F07%2F31%2Fdocs%2F06-Linux%2Flinux-supervisord%2F</url>
    <content type="text"><![CDATA[简介在开发中我们通过端口号启动多个网站服务和接口服务，网关服务，如果不使用工具管理这些服务进程，每次开机时都重新启动一次，将浪费我们大量时间。 安装1$ sudo apt install supervisor 配置默认supervisor配置目录为/etc/supervisor/conf.d。为方便操作，这里我们使用自己的配置目录。 1$ mkdir -p supervisor/&#123;conf,logs,run&#125; 添加配置文件supervisor.conf: 12345678910111213141516171819202122[unix_http_server]file=/path/to/your/supervisor/run/supervisor.sock[supervisord]logfile=/path/to/your/supervisor/logs/supervisor.loglogfile_maxbytes=50MBlogfile_backups=10loglevel=infopidfile=/path/to/your/supervisor/run/supervisor.pidnodaemon=falseminfds=1024minprocs=200childlogdir=/path/to/your/supervisor/logs[rpcinterface:supervisor]supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface[supervisorctl]serverurl=unix:///path/to/your/supervisor/run/supervisor.sock[include]files = conf/*.conf 修改配置文件中 /path/to/your/supervisor 为当前目录绝对路径。 启动 supervisor 服务： 1$ supervisord -n -c supervisor.conf 配置与管理进程再启动一个终端，运行命令： 1$ supervisorctl -c supervisor.conf 输入 help 命令查看所有命令： 1234567supervisor&gt; helpdefault commands (type help &lt;topic&gt;):=====================================add exit open reload restart start tail avail fg pid remove shutdown status update clear maintail quit reread signal stop version 运行 status 查看所有管理进程，目前没有配置，所以没有输出。 添加配置 conf/web-test.conf : 123[program:web-test]directory=/path/to/your/web-test/publiccommand=php -S 0:8099 index.php 修改 /path/to/your/web-test 为实际项目地址。 执行 update 命令可以看到配置已经加载： 1234supervisor&gt; updatepet-clinic-web: added process groupsupervisor&gt; statuspet-clinic-web RUNNING pid 5958, uptime 0:00:01 打开网站看看服务是否正确。 开机启动在 /etc/systemd/system 目录中添加我们自己的 supervisor 开机启动配置 my-supervisord.service : 12345678910[Unit]Description=My Supervisord Daemon[Service]ExecStart=/usr/bin/supervisord -n -c /path/to/your/supervisor.confUser=your-usernameGroup=your-username[Install]WantedBy=multi-user.target 替换上面配置文件中路径和用户名。启用开机启动： 123systemctl daemon-reloadsystemctl start my-supervisordsystemctl enable my-supervisord 别名supervisorctl 管理命令需要指定文件名启动，可以通过在 ~/.bashrc 中配置别名： 123function sv &#123; supervisorctl -c /path/to/your/supervisor.conf $@&#125; 现在在终端使用 sv 命令就可以进入管理命令行。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Supervisord</tag>
        <tag>服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TODO AWK 命令详解]]></title>
    <url>%2F2018%2F07%2F23%2Fdocs%2F06-Linux%2Flinux-awk%2F</url>
    <content type="text"></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TODO Nginx+php-fpm 运行原理]]></title>
    <url>%2F2018%2F07%2F20%2Fdocs%2F06-Linux%2Fphp-nginx-run-way%2F</url>
    <content type="text"><![CDATA[前言随着互联网的发展，用户对此接受面广，数据流的增大使得Web端的运行承载压力日益增大,野蛮生长在大数据时代里的WEB语言PHP也找到了比老搭档更优越的活力搭档Nginx. Nginx 是什么Nginx (“engine x”) 是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。 php-fpm 是什么 php-fpm即php-Fastcgi Process Manager. php-fpm是 FastCGI 的实现，并提供了进程管理的功能 进程包含 master 进程和 worker 进程两种进程。master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个(具体数量根据实际需要配置)，每个进程内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方。 要明白nginx 与 php-fpm是如何协同工作的,那么首先要明白CGI (Common Gateway Interface) 和 FastCGI 这两个协议 CGI与fastcgiCGI 是 Web Server 与后台语言交互的协议，有了这个协议，开发者可以使用任何语言处理 Web Server 发来的请求，动态的生成内容。但 CGI 有一个致命的缺点，那就是每处理一个请求都需要 fork 一个全新的进程，随着 Web 的兴起，高并发越来越成为常态，这样低效的方式明显不能满足需求。就这样，FastCGI 诞生了，CGI 很快就退出了历史的舞台。 FastCGI，顾名思义为更快的CGI, 它允许在一个进程内处理多个请求，而不是一个请求处理完毕就直接结束进程，性能上有了很大的提高。 FastCGI是一个可伸缩地、高速地在HTTP server和动态脚本语言间通信的接口。多数流行的HTTP server都支持FastCGI，包括Apache、Nginx和lighttpd等。同时，FastCGI也被许多脚本语言支持，其中就有PHP FastCGI接口方式采用C/S结构，可以将HTTP服务器和脚本解析服务器分开，同时在脚本解析服务器上启动一个或者多个脚本解析守护进程。当HTTP服务器每次遇到动态程序时，可以将其直接交付给FastCGI进程来执行，然后将得到的结果返回给浏览器。这种方式可以让HTTP服务器[如nginx]专一地处理静态请求或者将动态脚本服务器的结果返回给客户端，这在很大程度上提高了整个应用系统的性能。 至于 FPM (FastCGI Process Manager)，它是 FastCGI 的实现，任何实现了 FastCGI 协议的 Web Server 都能够与之通信。FPM 之于标准的 FastCGI，也提供了一些增强功能. FPM 是一个 PHP 进程管理器，包含 master 进程和 worker 进程两种进程：master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个 (具体数量根据实际需要配置)，每个进程内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>nginx</tag>
        <tag>PHP-FPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PsySh PHP交互控制台]]></title>
    <url>%2F2018%2F07%2F17%2Fdocs%2F02-tools%2Fpsysh%2F</url>
    <content type="text"><![CDATA[简介psysh是一个PHP的运行时开发平台，交互式调试器和Read-Eval-Print Loop (REPL)。说的简单点,就跟你用Chrome的时候firebug的console调试你的JavaScript代码一样。 官网 GitHub Packagist 安装直接下载123$ wget https://git.io/psysh$ chmod +x psysh$ ./psysh 使用Composer安装12$ composer g require psy/psysh:@stable$ psysh 以下教程以OS X和Windows为例，在这之前您已经将安装了php和composer，并且把加入了环境变量 OS x1. 下载12$ composer global require psy/psysh` 2. 安装完毕后，PsySH已经安装到/Users/{用户名}/.composer/vendor/psy/psysh目录下,这个时候你可以这样来直接运行1$ /Users/&#123;用户名&#125;/.composer/vendor/psy/psysh/bin/psysh 3. 为了使用方便，建议将它加入到环境变量：12$ echo &apos;export PATH=&quot;/Users/&#123;用户名&#125;/.composer/vendor/psy/psysh/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrc$ source ~/.bashrc Windows1. 我们还是用的composer来安装，win+r召唤控制台，然后1composer global require psy/psysh 2. 安装完成后，PsySH被安装到C:Users{用户名}AppDataRoamingComposervendorpsypsysh因为bin/psysh文件并不是windows的可执行文件，所以需要使用以下命令运行PsySH 1php C:\Users\&#123;用户名&#125;\AppData\Roaming\Composer\vendor\psy\psysh\bin\psysh 3. 为了使用方便，在C:Users{用户名}AppDataRoamingComposervendorpsypsyshbin目录下新建一个名为psysh.bat的文件，其内容如下12@ECHO OFFphp &quot;%~dp0psysh&quot; %* 4. 此时，把C:Users{用户名}A ppDataRoamingComposervendorpsypsyshbin 加入到系统的环境变量PATH，以后可以直接在cmd下运行psysh了123C:\Users\Vergil&gt;psyshPsy Shell v0.6.1 (PHP 5.6.8 — cli) by Justin Hileman&gt;&gt;&gt; 神器特性psysh是一个交互式的PHP运行控制台，在这里，你可以写php代码运行，并且可以清楚看到每次的返回值： 能够很智能的知道你的代码是否已经结束 自动完成，psysh可以像控制台那样，按下两次[tab]键自动补全，帮你自动完成变量名，函数，类，方法，属性，甚至是文件 文档在运行时忘记参数怎么办？psysh的文档功能可以上你及时查看文档。 PsySH的文档存放在~/.local/share/psysh/。（windows系统存放在C:\Users\{用户名}\AppData\Roaming\PsySH\） 下载中文文档： 1234$ cd ~/.local/share $ mkdir psysh$ cd psydh$ wget http://psysh.org/manual/zh/php_manual.sqlite OK，完成后重新打开PsySH 查看源代码轻松展现任何用户级的对象，类，接口，特质，常数，方法或属性的源代码： 反射列表list命令知道所有关于你的代码 - 和其他人的。轻松地列出并搜索所有的变量，常量，类，接口，特点，功能，方法和属性。 获取最后的异常信息如果忘记catch异常，可以使用wtf命令（wtf是what the fuck的意思么？）查看异常的信息： 历史记录可以像类Unix系统的history命令一样，在PsySH可以查看你运行过的PHP代码或命令。详情运行help history命令查看。 退出使用exit命令退出你的PsySH]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>psysh</tag>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）HTTP和HTTPS的区别与联系]]></title>
    <url>%2F2018%2F05%2F13%2Fdocs%2F09-pc-base%2Fnetwork-http-https%2F</url>
    <content type="text"><![CDATA[超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息，HTTP协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。 为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS，为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 HTTP和HTTPS的基本概念HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。 HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 HTTP与HTTPS有什么区别HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。 简单来说，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。 HTTPS和HTTP的区别主要如下： https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 HTTPS的工作原理客户端发起HTTPS请求这个没什么好说的，就是用户在浏览器里输入一个https网址，然后连接到server的443端口。 服务端的配置采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl就是个不错的选择，有1年的免费服务)。 这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。 传送证书这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。 客户端解析证书这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。 如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。 传送加密信息这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 服务段解密信息服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。 传输加密后的信息这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。 客户端解密信息客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。 HTTPS的优点正是由于HTTPS非常的安全，攻击者无法从中找到下手的地方，从站长的角度来说，HTTPS的优点有以下2点： SEO方面谷歌曾在2014年8月份调整搜索引擎算法，并称“比起同等HTTP网站，采用HTTPS加密的网站在搜索结果中的排名将会更高”。 安全性尽管HTTPS并非绝对安全，掌握根证书的机构、掌握加密算法的组织同样可以进行中间人形式的攻击，但HTTPS仍是现行架构下最安全的解决方案，主要有以下几个好处： 使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器； HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。 HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。 HTTPS的缺点虽然说HTTPS有很大的优势，但其相对来说，还是有些不足之处的，具体来说，有以下2点： SEO方面据ACM CoNEXT数据显示，使用HTTPS协议会使页面的加载时间延长近50%，增加10%到20%的耗电，此外，HTTPS协议还会影响缓存，增加数据开销和功耗，甚至已有安全措施也会受到影响也会因此而受到影响。 而且HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。 最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。 经济方面 SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。 SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗（SSL有扩展可以部分解决这个问题，但是比较麻烦，而且要求浏览器、操作系统支持，Windows XP就不支持这个扩展，考虑到XP的装机量，这个特性几乎没用）。 HTTPS连接缓存不如HTTP高效，大流量网站如非必要也不会采用，流量成本太高。 HTTPS连接服务器端资源占用高很多，支持访客稍多的网站需要投入更大的成本，如果全部采用HTTPS，基于大部分计算资源闲置的假设的VPS的平均成本会上去。 HTTPS协议握手阶段比较费时，对网站的相应速度有负面影响，如非必要，没有理由牺牲用户体验。 本文转载自 https://blog.csdn.net/xionghuixionghui/article/details/68569282 相关文章 https://www.runoob.com/w3cnote/http-vs-https.htmlhttps://www.jianshu.com/p/37654eb66b58]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TODO 从输入 URL 到页面加载完成的过程中都发生了什么事情？]]></title>
    <url>%2F2018%2F05%2F12%2Fdocs%2F09-pc-base%2Fwhenyouenteraurl%2F</url>
    <content type="text"><![CDATA[背景具体过程DNS 解析DNS（Domain Name System，域名系统）解析DNS解析的过程就是寻找哪台机器上有你需要资源的过程。当你在浏览器中输入一个地址时，例如www.baidu.com，其实不是百度网站真正意义上的地址。互联网上每一台计算机的唯一标识是它的IP地址，但是IP地址并不方便记忆。用户更喜欢用方便记忆的网址去寻找互联网上的其它计算机，也就是上面提到的百度的网址。所以互联网设计者需要在用户的方便性与可用性方面做一个权衡，这个权衡就是一个网址到IP地址的转换，这个过程就是DNS解析。它实际上充当了一个翻译的角色，实现了网址到IP地址的转换。网址到IP地址转换的过程是如何进行的? TCP 连接发送HTTP请求服务器处理请求并返回HTTP报文浏览器解析渲染页面连接结束相关文档 https://dailc.github.io/2018/03/12/whenyouenteraurl.htmlhttp://fex.baidu.com/blog/2014/05/what-happen]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）CentOS7.4搭建shadowsocks，以及配置BBR加速]]></title>
    <url>%2F2018%2F04%2F13%2Fdocs%2F02-tools%2Fshadowsocks-setting%2F</url>
    <content type="text"><![CDATA[前言作为一个新世纪的码农，我们经常需要使用百度以及Google等搜索引擎搜索资料或搜索一些错误的解决方案，如果English好的还可能需要到stackoverflow里查看或提问一些开发中遇到的问题，再者可能还需要到youtube上查找一下教学、科普视频等等。还好的是stackoverflow部分不牵扯Google的内容在国内还是能够正常访问的，但是Google和youtube嘛大家都懂，所以本文就介绍一下如何在vps上搭建shadowsocks，让我们能够访问这些网站，以便于我门查阅资料，切勿用做其他不法用途。 常见VPS的购买地址活跃于大街小巷的搬瓦工，也是适合新手使用的： https://bwh1.net/ vultr https://www.vultr.com/?ref=7315390 SugarHosts https://www.sugarhosts.com/zh-cn/ Linode https://www.linode.com/ Virmach https://billing.virmach.com/cart.php?gid=1 RAKSmart https://billing.raksmart.com/ Bluehost https://cn.bluehost.com/ DigitalOcean https://www.digitalocean.com 以上这些都是国外的vps，国内的可以购买阿里云或者腾讯云等，国内没有遇到优惠的话就比较贵。 安装 pippip是python的包管理工具。在本文中将使用python版本的shadowsocks，此版本的shadowsocks已发不到pip上，因此我们需要通过pip命令来安装。 在控制台执行以下命令安装 pip： 12$ curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"$ python get-pip.py 安装配置 shadowsocks在控制台执行以下命令安装shadowsocks: 12$ pip install --upgrade pip$ pip install shadowsocks 安装完成后，需要创建shadowsocks的配置文件/etc/shadowsocks.json，编辑内容如下： 12345678910111213$ vim /etc/shadowsocks.json&#123; "server": "0.0.0.0", "local_address": "127.0.0.1", "local_port": 1080, "port_password": &#123; "8080": "填写密码", "8081": "填写密码" &#125;, "timeout": 600, "method": "aes-256-cfb"&#125; 说明 method为加密方法，可选aes-128-cfb,aes-192-cfb,aes-256-cfb,bf-cfb,cast5-cfb,des-cfb,rc4-md5,chacha20,rc4,table port_password为端口对应的密码，可使用密码生成工具生成一个随机密码 以上两项信息在配置shadowsocks客户端时需要配置一致，具体说明可查看shadowsocks的帮助文档。 如果你不需要配置多个端口的话，仅配置单个端口，则可以使用以下配置： 123456&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 8080, &quot;password&quot;: &quot;填写密码&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;&#125; 说明： server_port为服务监听端口 password为密码 同样的以上两项信息在配置 shadowsocks 客户端时需要配置一致。 配置自启动编辑shadowsocks 服务的启动脚本文件，内容如下： 12345678910$ vim /etc/systemd/system/shadowsocks.service[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c /etc/shadowsocks.json[Install]WantedBy=multi-user.target 执行以下命令启动 shadowsocks 服务： 12$ systemctl enable shadowsocks$ systemctl start shadowsocks 检查 shadowsocks 服务是否已成功启动，可以执行以下命令查看服务的状态： 1$ systemctl status shadowsocks -l 确认服务启动成功后，配置防火墙规则，开放你配置的端口，不然客户端是无法连接的： 123456$ firewall-cmd --zone=public --add-port=8080/tcp --permanentsuccess$ firewall-cmd --zone=public --add-port=8081/tcp --permanentsuccess$ firewall-cmd --reloadsuccess 一键安装脚本代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/bin/bash# Install Shadowsocks on CentOS 7echo "Installing Shadowsocks..."random-string()&#123; cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w $&#123;1:-32&#125; | head -n 1&#125;CONFIG_FILE=/etc/shadowsocks.jsonSERVICE_FILE=/etc/systemd/system/shadowsocks.serviceSS_PASSWORD=$(random-string 32)SS_PORT=8388SS_METHOD=aes-256-cfbSS_IP=`ip route get 1 | awk '&#123;print $NF;exit&#125;'`GET_PIP_FILE=/tmp/get-pip.py# install pipcurl "https://bootstrap.pypa.io/get-pip.py" -o "$&#123;GET_PIP_FILE&#125;"python $&#123;GET_PIP_FILE&#125;# install shadowsockspip install --upgrade pippip install shadowsocks# create shadowsocls configcat &lt;&lt;EOF | sudo tee $&#123;CONFIG_FILE&#125;&#123; "server": "0.0.0.0", "server_port": $&#123;SS_PORT&#125;, "password": "$&#123;SS_PASSWORD&#125;", "method": "$&#123;SS_METHOD&#125;"&#125;EOF# create servicecat &lt;&lt;EOF | sudo tee $&#123;SERVICE_FILE&#125;[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c $&#123;CONFIG_FILE&#125;[Install]WantedBy=multi-user.targetEOF# start servicesystemctl enable shadowsockssystemctl start shadowsocks# view service statussleep 5systemctl status shadowsocks -lecho "================================"echo ""echo "Congratulations! Shadowsocks has been installed on your system."echo "You shadowsocks connection info:"echo "--------------------------------"echo "server: $&#123;SS_IP&#125;"echo "server_port: $&#123;SS_PORT&#125;"echo "password: $&#123;SS_PASSWORD&#125;"echo "method: $&#123;SS_METHOD&#125;"echo "--------------------------------" 配置客户端我这里配置的是windows的客户端，挺方便的，点击即用，不需要安装。 Windows客户端下载地址： https://github.com/shadowsocks/shadowsocks-windows/releases Mac客户端下载地址： https://github.com/shadowsocks/ShadowsocksX-NG/releases Android客户端下载地址： https://github.com/shadowsocks/shadowsocks-android/releases 接着测试能否上Google搜索即可，以下的配置BBR加速则是选看，不配置也是可以正常使用shadowsocks的。 配置BBR加速什么是BBR： TCP BBR是谷歌出品的TCP拥塞控制算法。BBR目的是要尽量跑满带宽，并且尽量不要有排队的情况。BBR可以起到单边加速TCP连接的效果。 Google提交到Linux主线并发表在ACM queue期刊上的TCP-BBR拥塞控制算法。继承了Google“先在生产环境上部署，再开源和发论文”的研究传统。TCP-BBR已经再YouTube服务器和Google跨数据中心的内部广域网(B4)上部署。由此可见出该算法的前途。 TCP-BBR的目标就是最大化利用网络上瓶颈链路的带宽。一条网络链路就像一条水管，要想最大化利用这条水管，最好的办法就是给这跟水管灌满水。 BBR解决了两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟，高带宽的网络链路。 降低网络链路上的buffer占用率，从而降低延迟。非常适合慢速接入网络的用户。Google 在 2016年9月份开源了他们的优化网络拥堵算法BBR，最新版本的 Linux内核(4.9-rc8)中已经集成了该算法。 对于TCP单边加速，并非所有人都很熟悉，不过有另外一个大名鼎鼎的商业软件“锐速”，相信很多人都清楚。特别是对于使用国外服务器或者VPS的人来说，效果更佳。 BBR项目地址： https://github.com/google/bbr 升级内核，第一步首先是升级内核到支持BBR的版本：1.yum更新系统版本： 1$ yum update 2.查看系统版本： 12$ cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) 3.安装elrepo并升级内核： 123$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org$ rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm$ yum --enablerepo=elrepo-kernel install kernel-ml -y 4.更新grub文件并重启系统： 12345678$ egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \'CentOS Linux 7 Rescue 8619ff5e1306499eac41c02d3b23868e (4.14.14-1.el7.elrepo.x86_64)CentOS Linux (4.14.14-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-693.11.6.el7.x86_64) 7 (Core)CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-c73a5ccf3b8145c3a675b64c4c3ab1d4) 7 (Core)$ grub2-set-default 0$ reboot 5.重启完成后查看内核是否已更换为4.14版本： 12$ uname -r4.14.14-1.el7.elrepo.x86_64 6.开启bbr： 123$ vim /etc/sysctl.conf # 在文件末尾添加如下内容net.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbr 7.加载系统参数： 12345$ sysctl -pnet.ipv6.conf.all.accept_ra = 2net.ipv6.conf.eth0.accept_ra = 2net.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbr 如上，输出了我们添加的那两行配置代表正常。 8.确定bbr已经成功开启： 1234$ sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = bbr cubic reno$ lsmod | grep bbrtcp_bbr 20480 2 输出内容如上，则表示bbr已经成功开启。 相关文章 http://blog.51cto.com/zero01/2064660 https://github.com/shadowsocks/shadowsocks-libev https://www.jianshu.com/p/4984f324010f 原文链接（如需转载，请注明出处）http://blog.51cto.com/zero01/2064660]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>shadowsocks</tag>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 客户端多账号管理]]></title>
    <url>%2F2018%2F02%2F25%2Fdocs%2F02-tools%2Fgit-multiple-accounts%2F</url>
    <content type="text"><![CDATA[前言在开发的过程中，经常会遇到这样的状况：需要在一台电脑上同时使用两个甚至多个 git 账号，负责不同的用途，比如：一个用来写个人项目，一个用来写公司的项目。为此我们需要为不同的账号生成不同的密钥，那对这些不同的账号和不同的密钥，我们该怎么处理呢？ SSH配置取消全局设置的用户名和邮箱12$ git config --global --unset user.name$ git config --global --unset user.email 生成私钥和公钥123456789101112131415$ cd ~/.ssh &amp;&amp; mkdir -pv &#123;github,company&#125;$ ssh-keygen -t rsa -C "youremail@example.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/Administrator/.ssh/id_rsa):/Users/Administrator/.ssh/github/id_rsa_github # 在回车提示中输入完整路径，如：/Users/Administrator/.ssh/github/id_rsa_github #文件命名后，按2次回车，密码为空 Enter passphrase (empty for no passphrase):Enter same passphrase again:$ ssh-keygen -t rsa -C "youremail@example.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/Administrator/.ssh/id_rsa):/Users/Administrator/.ssh/company/id_rsa_companyEnter passphrase (empty for no passphrase):Enter same passphrase again: 如果用户家目录中没有 .ssh 目录请自行创建在这里我创建了两个目录 github 和 company ，分别用来存储不同项目的密钥，进行分类管理 New SSH key 把 ~/.ssh/github/id_rsa_github.pub 的内容添加到Github的SSH keys中 把 ~/.ssh/company/id_rsa_company.pub 的内容添加到公司Gitlab的SSH keys中 新密钥添加到SSH agent中添加新密钥到SSH agent，因为默认只读取id_rsa，为了让SSH识别新的私钥，需将其添加到SSH agent中： 12$ ssh-add -K ~/.ssh/github/id_rsa_github$ ssh-add -K ~/.ssh/company/id_rsa_company 如果出现Could not open a connection to your authentication agent的错误，就试着用以下命令： 12$ ssh-agent bash$ ssh-add -K ~/.ssh/github/id_rsa_github 使用 ssh-add -l 查看 ssh key 的设置 修改 config 文件1234567891011121314151617181920$ vim ~/.ssh/configHost * KexAlgorithms +diffie-hellman-group1-sha1# default: myfirstHost github.com HostName github.com User myfirst PreferredAuthentications publickey IdentityFile ~/.ssh/github/id_rsa_github1# mysecondHost mysecond.github.com HostName github.com User mysecond PreferredAuthentications publickey IdentityFile ~/.ssh/github/id_rsa_github2Host company.com HostName company.com User company PreferredAuthentications publickey IdentityFile ~/.ssh/company/id_rsa_company 其规则就是：从上至下读取config的内容，在每个Host下寻找对应的私钥。这里将GitHub SSH仓库地址中的git@github.com替换成新建的Host别名如：mysecond.github.com，那么原地址是：git@github.com:test/Mywork.git，替换后应该是：mysecond.github.com:test/Mywork.git. 测试连通性123$ ssh -T git@github.comHi youremail! You've successfully authenticated, but GitHub does not provide shell access. 项目测试初始化项目 a1234567891011121314$ cd ~/a$ git init$ echo "myfirst" &gt; README.md$ git add README.md$ git config user.name "myfirst"$ git config user.email "myfirst@gmail.com"$ git remote add github git@github.com:myfirst/test.git$ git push -u github masterCounting objects: 3, done.Writing objects: 100% (3/3), 213 bytes | 213.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To github.com:myfirst/test.git * [new branch] master -&gt; masterBranch master set up to track remote branch master from github. 初始化项目 b1234567891011121314$ cd ~/b$ git init$ echo "mysecond" &gt; README.md$ git add README.md$ git config user.name "mysecond"$ git config user.email "mysecond@gmail.com"$ git remote add github git@mysecond.github.com:mysecond/test.git$ git push -u github masterCounting objects: 3, done.Writing objects: 100% (3/3), 218 bytes | 218.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To mysecond.github.com:mysecond/test.git * [new branch] master -&gt; masterBranch master set up to track remote branch master from github.]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Git</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homestead 下安装Swoole扩展]]></title>
    <url>%2F2018%2F02%2F21%2Fdocs%2F06-Linux%2Fphp-extension-swoole-install%2F</url>
    <content type="text"><![CDATA[前言尽管PHP软件源提供了不少PHP扩展，但并不是提供所有的扩展，那么如果我们需要安装一个软件源没有提供的扩展怎么办呢？利用 php-dev 就可以很方便的进行自行编译 PHP 扩展了，但是由于Homestaed内置了多个PHP版本，安装方式略有不同，不能简单粗暴的使用 pecl 安装了，因为安装好了不知道是给谁用的。下面分别介绍单一PHP和多版本PHP如何安装PHP扩展 注意：命令权限不够时请自动在命令前加上sudo前缀提权； 单一版本PHP安装 php-dev ，如果不是 7.2 需要自己修改一下版本号1$ apt install php7.2-dev 安装Swoole扩展1$ pecl install swoole 添加配置文件123$ cd /etc/php/7.2/fpm/conf.d$ touch swoole.ini$ echo "extension=swoole.so" | tee -a swoole.ini 重启php-fpm生效1$ systemctl restart php7.2-fpm 多PHP版本安装php-dev1$ apt install php5.6-dev php7.2-dev 下载swoole源码1234$ cd /usr/src$ wget http://pecl.php.net/get/swoole-1.9.15.tgz$ tar xzf swoole-1.9.15.tgz$ cd swoole-1.9.15 为PHP5.6进行编译1234$ cd /usr/src/swoole-1.9.15$ /usr/bin/phpize5.6$ ./configure --with-php-config=/usr/bin/php-config5.6$ make &amp;&amp; make install 为PHP7.2进行编译1234$ cd /usr/src/swoole-1.9.15$ /usr/bin/phpize7.2$ ./configure --with-php-config=/usr/bin/php-config7.2$ make &amp;&amp; make install 编译完成后扩展在module目录中，它的文件名是swoole.so 查看php的extension_dir1234$ php -i|grep extension_dirextension_dir =&gt; /usr/lib/php/20180731 =&gt; /usr/lib/php/20180731sqlite3.extension_dir =&gt; no value =&gt; no value 这里可以将php替换成指定版本，就可以查看指定版本的extension_dir 添加php配置文件12$ cd /etc/php/7.2/mods-available/$ sudo touch swoole.ini 添加以下内容 123; configuration for php swoole module; priorit=20extension=swoole.so 建立链接文件 123$ sudo ln -s /etc/php/7.0/mods-available/swoole.ini /etc/php/7.2/cli/conf.d/20-swoole.ini $ sudo ln -s /etc/php/7.0/mods-available/swoole.ini /etc/php/7.2/fpm/conf.d/20-swoole.ini 重启php-fpm1$ sudo service php7.2-fpm restart 将7.2替换成5.6为php56添加swoole扩展 同理，什么mongodb、redis的扩展等等，也都能够通过类似的方法完成安装 扩展：Homestead下修改cli模式下默认php版本我们通过命令 ll /usr/bin/php 可以看到，php是/etc/alternatives/php 建立的链接文件 1lrwxrwxrwx 1 root root 21 Feb 3 19:53 /usr/bin/php -&gt; /etc/alternatives/php 然后通过命令 ll /etc/alternatives/php 可以看到是通过 usr/bin/php7.0 建立的链接文件 1lrwxrwxrwx 1 root root 15 Feb 19 05:15 /etc/alternatives/php -&gt; /usr/bin/php7.2* 所以我们只要修改 /etc/alternatives/php 的源文件即可修改cli模式下php的默认版本，命令如下： 12$ sudo mv /etc/alternatives/php$ sudo ln -s /usr/bin/php7.0 /etc/alternatives/php]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Linux</tag>
        <tag>Swoole</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 分片]]></title>
    <url>%2F2017%2F06%2F14%2Fdocs%2F05-nosql%2Fmongodb-sharding%2F</url>
    <content type="text"><![CDATA[简介在Mongodb里面存在另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。当MongoDB存储海量的数据时，一台机器可能不足以存储数据，也可能不足以提供可接受的读写吞吐量。这时，我们就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 为什么使用片 复制所有的写入操作到主节点 延迟的敏感数据会在主节点查询 单个副本集限制在12个节点 当请求量巨大时会出现内存不足。 本地磁盘不足 垂直扩展价格昂贵 MongoDB分片集群组成MongoDB分片群集主要有如下三个主要组件： Shard：分片服务器，用于存储实际的数据块，实际生产环境中一个shard server角色可由几台服务器组成一个Replica Set 承担，防止主机节点故障。 Config Server：配置服务器，存储了整个分片群集的配置信息，其中包括chunk信息。 Routers：前端路由，客户端由此接入，且让整个群集看上去像单一数据库，前端应用可以透明使用。 下图展示了在MongoDB中使用分片集群结构分布： 分片集群的简单配置在这里在一台物理服务器上部署一个简单结构的MongoDB分片集群： 1台路由实例（端口27017）1台配置实例（端口37017）3台Shard实例（端口47017、47018、47019） 安装Mongodb具体安装参考MongoDB 基础教程 123456$ mkdir -p /usr/local/mongodb/data/db&#123;1,2,3,4&#125; # 创建数据存储目录 $ mkdir /usr/local/mongodb/data/logs # 创建日志文件存储目录 $ touch /usr/local/mongodb/data/logs/mongodb&#123;1,2,3,4&#125;.log # 创建日志文件 $ chmod -R 777 /usr/local/mongodb/data/logs/*.log $ ulimit -n 25000 # 最多打开文件个数，重启后失效 $ ulimit -u 25000 # 最多打开进程数，重启后失效 配置服务器123$ cd /usr/local/mongodb/data$ mkdir conf$ vim mongodb1.conf 12345678port=37017 # 端口号 dbpath=/usr/local/mongodb/data/db/db1 # 数据存储目录 logpath=/usr/local/mongodb/data/logs/mongodb1.log # 日志文件存储目录 logappend=truefork=truemaxConns=5000storageEngine=mmapv1configsvr=true 分片服务器三台分片服务器配置相同，只需更改端口号、数据存储目录和日志存储目录即可； 123$ cp -p mongodb1.conf mongodb2.conf$ vim mongodb2.conf 12345678port=47017 # 端口号dbpath=/usr/local/mongodb/data/db/db2 # 数据存储目录logpath=/usr/local/mongodb/data/logs/mongodb2.log # 日志文件存储目录logappend=truefork=truemaxConns=5000storageEngine=mmapv1shardsvr=true 重复以上步骤分别配置其他分片服务器 启动服务器 1234$ mongod -f /usr/local/mongodb/data/conf/mongodb1.conf$ mongod -f /usr/local/mongodb/data/conf/mongodb2.conf$ mongod -f /usr/local/mongodb/data/conf/mongodb3.conf$ mongod -f /usr/local/mongodb/data/conf/mongodb4.conf 启动路由服务器123456$ ./mongos --port 27017 --fork --logpath=/usr/local/mongodb/data/log/route.log --configdb 192.168.33.12:37017 --chunkSize 1#--port指定对方连接入口#--fork后台运行#--logpath指定日志文件存储路径#--configdb指定给谁处理 启用分片服务器12345$ ./bin/mongo&gt; sh.addShard("192.168.27.153:47017")&gt; sh.addShard("192.168.27.153:47018")&gt; sh.status() 启用分片存储功能123456&gt; use kgc&gt; for (var i=1;i&lt;=50000;i++)db.users.insert(&#123;"id":i,"name":"zhangsan"+i&#125;)&gt; db.users.createIndex(&#123;"id":1&#125;) #对users表创建索引&gt; sh.enableSharding("kgc") #启用kgc数据库分片&gt; sh.shardCollection("kgc.users",&#123;"id":1&#125;) #表分片&gt; sh.status() 给分片添加标签123&gt; sh.addShardTag("shard0000","test01")&gt; sh.addShardTag("shard0001","test02")&gt; sh.status() 添加或删除分片服务器1234&gt; sh.addShard("192.168.33.12:47019")&gt; use admin&gt; db.runCommand(&#123;"removeshard":"192.168.27.153:47019"&#125;)&gt; sh.status() 基本操作查看数据分布 12&gt; use kgc&gt; db.users.getShardDistribution() 查看集合是否分片 1&gt; db.collectionName.stats().sharded # 简单的返回true或者false TODO分片策略 参考文档 https://www.runoob.com/mongodb/mongodb-sharding.htmlhttps://blog.51cto.com/13659182/2149307https://www.jianshu.com/p/cb55bb333e2d]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 复制（副本集）]]></title>
    <url>%2F2017%2F06%2F13%2Fdocs%2F05-nosql%2Fmongodb-replication%2F</url>
    <content type="text"><![CDATA[简介MongoDB复制是将数据同步在多个服务器的过程。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。复制还允许您从硬件故障和服务中断中恢复数据。Mongodb复制集由一组Mongod实例（进程）组成，包含一个Primary节点和多个Secondary节点。Mongodb Driver（客户端）的所有数据都写入Primary，Secondary从Primary同步写入的数据，以保持复制集内所有成员存储相同的数据集，实现数据的高可用。 使用场景 数据冗余，用做故障恢复使用，当发生硬件故障或者其它原因造成的宕机时，可以使用副本进行恢复。 读写分离，读的请求分流到副本上，减轻主节点的读压力。 mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。mongodb各个节点常见的搭配方式为：一主一从、一主多从。主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 一个典型的副本集架构如下图所示： 以上结构图中，客户端从主节点读取数据，在客户端写入数据到主节点时， 主节点与从节点进行数据交互保障数据的一致性。 MongoDB副本集设置通过指定 –replSet 选项来启动mongoDB。–replSet 基本语法格式如下： 1mongod --port &quot;PORT&quot; --dbpath &quot;YOUR_DB_DATA_PATH&quot; --replSet &quot;REPLICA_SET_INSTANCE_NAME&quot; 实例 1./bin/mongod --port=27017 --dbpath=./data/db/ --replSet=rs0 以上实例会启动一个名为rs0的MongoDB实例，其端口号为27017。启动后打开命令提示框并连接上mongoDB服务。在Mongo客户端使用命令rs.initiate()来启动一个新的副本集。我们可以使用rs.conf()来查看副本集的配置。查看副本集状态使用rs.status()命令 。 副本集特征： N 个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复 副本集添加成员添加副本集的成员，我们需要使用多台服务器来启动mongo服务。进入Mongo客户端，并使用rs.add()方法来添加副本集的成员。 1&gt; rs.add(HOST_NAME:PORT) MongoDB中你只能通过主节点将Mongo服务添加到副本集中， 判断当前运行的Mongo服务是否为主节点可以使用命令db.isMaster() 。MongoDB的副本集与我们常见的主从有所不同，主从在主机宕机后所有服务将停止，而副本集在主机宕机后，副本会接管主节点成为主节点，不会出现宕机的情况。 副本集角色 主节点（Primary） 接收所有的写请求，然后把修改同步到所有Secondary。一个Replica Set只能有一个Primary节点，当Primary挂掉后，其他Secondary或者Arbiter节点会重新选举出来一个主节点。默认读请求也是发到Primary节点处理的，可以通过修改客户端连接配置以支持读取Secondary节点。 副本节点（Secondary） 与主节点保持同样的数据集。当主节点挂掉的时候，参与选主。 仲裁者（Arbiter） 不保有数据，不参与选主，只进行选主投票。使用Arbiter可以减轻数据存储的硬件需求，Arbiter几乎没什么大的硬件资源需求，但重要的一点是，在生产环境下它和其他数据节点不要部署在同一台机器上。 两种架构模式 PSS Primary + Secondary + Secondary模式，通过Primary和Secondary搭建的Replica SetDiagram of a 3 member replica set that consists of a primary and two secondaries. 该模式下 Replica Set节点数必须为奇数，目的是选主投票的时候要出现大多数才能进行选主决策。 PSA Primary + Secondary + Arbiter模式，使用Arbiter搭建Replica Set 偶数个数据节点，加一个Arbiter构成的Replica Set 选举机制复制集通过 replSetInitiate 命令或 rs.initiate() 命令进行初始化。初始化后各个成员间开始发送心跳消息，并发起 Primary 选举操作，获得大多数成员投票支持的节点，会成为 Primary，其余节点成为 Secondary。 123456789config = &#123; _id : &quot;my_replica_set&quot;, members : [ &#123;_id : 0, host : &quot;rs1.example.net:27017&quot;&#125;, &#123;_id : 1, host : &quot;rs2.example.net:27017&quot;&#125;, &#123;_id : 2, host : &quot;rs3.example.net:27017&quot;&#125;, ]&#125;rs.initiate(config) 大多数假设复制集内投票成员（后续介绍）数量为 N，则大多数为 N/2 + 1，当复制集内存活成员数量不足大多数时，整个复制集将无法选举出 Primary，复制集将无法提供写服务，处于只读状态 关于大多数的计算如下表所示 投票成员数 大多数 容忍失效数 1 1 0 2 2 0 3 2 1 4 3 1 5 3 2 Mongodb副本集的选举基于Bully算法，这是一种协调者竞选算法，详细解析可以参考这里Primary 的选举受节点间心跳、优先级、最新的 oplog 时间等多种因素影响。官方文档对于选举机制的说明选举机制的说明 特殊角色 ArbiterArbiter 节点只参与投票，不能被选为 Primary，并且不从 Primary 同步数据。当节点宕机导致复制集无法选出 Primary时，可以给复制集添加一个 Arbiter 节点，即使有节点宕机，仍能选出 Primary。Arbiter 本身不存储数据，是非常轻量级的服务，当复制集成员为偶数时，最好加入一个 Arbiter 节点，以提升复制集可用性。 Priority0Priority0节点的选举优先级为0，不会被选举为 Primary。比如你跨机房 A、B 部署了一个复制集，并且想指定 Primary 必须在 A 机房，这时可以将 B 机房的复制集成员 Priority 设置为0，这样 Primary 就一定会是 A 机房的成员。（注意：如果这样部署，最好将大多数节点部署在 A 机房，否则网络分区时可能无法选出 Primary。） Vote0Mongodb 3.0里，复制集成员最多50个，参与 Primary 选举投票的成员最多7个，其他成员（Vote0）的 vote 属性必须设置为0，即不参与投票。 HiddenHidden 节点不能被选为主（Priority 为0），并且对 Driver 不可见。因 Hidden 节点不会接受 Driver 的请求，可使用 Hidden 节点做一些数据备份、离线计算的任务，不会影响复制集的服务。 DelayedDelayed 节点必须是 Hidden 节点，并且其数据落后与 Primary 一段时间（可配置，比如1个小时）。因 Delayed 节点的数据比 Primary 落后一段时间，当错误或者无效的数据写入 Primary 时，可通过 Delayed 节点的数据来恢复到之前的时间点。 触发选举条件 初始化一个副本集时。 从库不能连接到主库(默认超过10s，可通过heartbeatTimeoutSecs参数控制)，由从库发起选举 主库放弃primary 角色，比如执行rs.stepdown 命令 Mongodb副本集通过心跳检测实现自动failover机制，进而实现高可用 MongoDB复制流程Primary 与 Secondary 之间通过 oplog 来同步数据，Primary 上的写操作完成后，会向特殊的 local.oplog.rs 特殊集合写入一条 oplog，Secondary 不断的从 Primary 取新的 oplog 并应用。因 oplog 的数据会不断增加，local.oplog.rs 被设置成为一个 capped 集合，当容量达到配置上限时，会将最旧的数据删除掉。另外考虑到 oplog 在 Secondary 上可能重复应用，oplog 必须具有幂等性，即重复应用也会得到相同的结果。如下 oplog 的格式，包含 ts、h、op、ns、o 等字段。 123456789101112&#123; &quot;ts&quot; : Timestamp(1446011584, 2), &quot;h&quot; : NumberLong(&quot;1687359108795812092&quot;), &quot;v&quot; : 2, &quot;op&quot; : &quot;i&quot;, &quot;ns&quot; : &quot;test.nosql&quot;, &quot;o&quot; : &#123; &quot;_id&quot; : ObjectId(&quot;563062c0b085733f34ab4129&quot;), &quot;name&quot; : &quot;mongodb&quot;, &quot;score&quot; : &quot;100&quot; &#125;&#125; 属性 说明 ts 操作时间，当前 timestamp + 计数器，计数器每秒都被重置 h 操作的全局唯一标识 v oplog 版本信息 op 操作类型 op.i 插入操作 op.u 更新操作 op.d 删除操作 op.c 执行命令（如 createDatabase，dropDatabase） op.n 空操作，特殊用途 ns 操作针对的集合 o 操作内容 o2 操作查询条件，仅 update 操作包含该字段。 Secondary 初次同步数据时，会先执行 init sync，从 Primary（或其他数据更新的 Secondary）同步全量数据，然后不断通过执行tailable cursor从 Primary 的 local.oplog.rs 集合里查询最新的 oplog 并应用到自身。 异常回滚 当 Primary 宕机时，如果有数据未同步到 Secondary，当 Primary 重新加入时，如果新的 Primary 上已经发生了写操作，则旧 Primary 需要回滚部分操作，以保证数据集与新的 Primary 一致。旧 Primary 将回滚的数据写到单独的 rollback 目录下，数据库管理员可根据需要使用 mongorestore 进行恢复 读写配置默认情况下，复制集的所有读请求都发到 Primary，Driver 可通过设置 Read Preference 来将读请求路由到其他的节点。 primary：默认规则，所有读请求发到 Primary； primaryPreferred：Primary 优先，如果 Primary 不可达，请求 Secondary； secondary：所有的读请求都发到 secondary； secondaryPreferred：Secondary 优先，当所有 Secondary 不可达时，请求 Primary； nearest：读请求发送到最近的可达节点上（通过 ping 探测得出最近的节点）。 关于read-preference Write Concern默认情况下，Primary 完成写操作即返回，Driver 可通过设置 Write Concern 来设置写成功的规则。如下的 write concern 规则设置写必须在大多数节点上成功，超时时间为5s。 1234db.products.insert( &#123; item: &quot;envelopes&quot;, qty : 100, type: &quot;Clasp&quot; &#125;, &#123; writeConcern: &#123; w: majority, wtimeout: 5000 &#125; &#125;) 关于write-concern 参考文档 https://www.runoob.com/mongodb/mongodb-replication.htmlhttps://www.cnblogs.com/littleatp/p/8562842.htmlhttp://www.mongoing.com/archives/5200]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 基础教程]]></title>
    <url>%2F2017%2F06%2F11%2Fdocs%2F05-nosql%2Fmongodb-tutorial-base%2F</url>
    <content type="text"><![CDATA[MongoDB 简介什么是MongoDBMongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多地节点，可以保证服务器的性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值（key=&gt;value）对组成。MongoDB 文档类似于JSON对象。字段值可以包含其他文档，数组以及文档数据。 主要特点 MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。 Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。 Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。 Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。 GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。 MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。 MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 MongoDB安装简单。 安装Linux平台安装MongoDBMongoDB 提供了linux各发行版本64位的安装包，你可以在官网下载安装包。下载地址：https://www.mongodb.com/download-center/community 下载完安装包，并解压 tgz（以下演示的是 64 位 Linux上的安装） 。 123$ wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.6.tgz$ tar -zxvf mongodb-linux-x86_64-3.0.6.tgz$ mv mongodb-linux-x86_64-3.0.6/ mongodb MongoDB 的可执行文件位于 bin 目录下，所以可以将其添加到 PATH 路径中： 1$ export PATH=mongodb-install-directory/bin:$PATH mongodb-install-directory 为你 MongoDB 的安装路径。如本文的 /usr/local/mongodb 。 创建数据目录MongoDB的数据存储在data目录的db目录下，但是这个目录在安装过程不会自动创建，所以你需要手动创建data目录，并在data目录中创建db目录。 以下实例中我们将data目录创建于/usr/local/mongodb/目录下。 1$ mkdir -p /usr/local/mongodb/data/db 启动MongoDB服务你可以再命令行中执行mongodb安装目录中的bin目录执行mongod命令来启动mongdb服务。 注意：如果你的数据库目录不是/data/db（MongoDB 默认的启动的数据库路径），可以通过 –dbpath 来指定。 1$ mongod --dbpath=/usr/local/mongodb/data/db MongoDB后台管理Shell如果你需要进入MongoDB后台管理，你需要先打开mongodb装目录的下的bin目录，然后执行mongo命令文件。MongoDB Shell是MongoDB自带的交互式Javascript shell,用来对MongoDB进行操作和管理的交互式环境。当你进入mongoDB后台后，它默认会链接到 test 文档（数据库）： 1234567891011$ mongoMongoDB shell version: 3.0.6connecting to: testWelcome to the MongoDB shell.For interactive help, type "help".For more comprehensive documentation, see http://docs.mongodb.org/Questions? Try the support group http://groups.google.com/group/mongodb-user&gt; 现在让我们插入一些简单的数据，并对插入的数据进行检索： 12345&gt; db.runoob.insert(&#123;test:1&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.runoob.find()&#123; &quot;_id&quot; : ObjectId(&quot;5d26b208168b6593d96e6387&quot;), &quot;test&quot; : 1 &#125;&gt; OK，至此我们的MongoDB已经安装完成。 概念解析不管我们学习什么数据库都应该学习其中的基础概念，在mongodb中基本的概念是文档、集合、数据库，下面我们挨个介绍。下表将帮助您更容易理解Mongo中的一些概念： SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table join 表连接，MongoDB不支持 primary key primary key 主键，MongoDB自动将_id字段设置为主键 数据库一个mongodb中可以建立多个数据库。MongoDB的默认数据库为”db”，该数据库存储在data目录中。MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。 有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。 admin： 从权限的角度来看，这是”root”数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。 local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合 config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 文档(Document)文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 一个简单的文档例子如下： 1&#123;"site":"www.axkeson.com", "name":"Axkeson"&#125; 需要注意的是： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 集合集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。 集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 比如，我们可以将以下不同数据结构的文档插入到集合中： 123&#123;&quot;site&quot;:&quot;www.baidu.com&quot;&#125;&#123;&quot;site&quot;:&quot;www.google.com&quot;,&quot;name&quot;:&quot;Google&quot;&#125;&#123;&quot;site&quot;:&quot;www.axkeson.com&quot;, &quot;name&quot;:&quot;Axkeson&quot;&#125; 当第一个文档插入时，集合就会被创建。 capped collections Capped collections 就是固定大小的collection。它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 “RRD” 概念类似。Capped collections 是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能和标准的 collection 不同，你必须要显式的创建一个capped collection，指定一个 collection 的大小，单位是字节。collection 的数据存储空间值提前分配的。Capped collections 可以按照文档的插入顺序保存到集合中，而且这些文档在磁盘上存放位置也是按照插入顺序来保存的，所以当我们更新Capped collections 中文档的时候，更新后的文档不可以超过之前文档的大小，这样话就可以确保所有文档在磁盘上的位置一直保持不变。由于 Capped collection 是按照文档的插入顺序而不是使用索引确定插入位置，这样的话可以提高增添数据的效率。MongoDB 的操作日志文件 oplog.rs 就是利用 Capped Collection 来实现的。要注意的是指定的存储大小包含了数据库的头信息。 1&gt; db.createCollection(&quot;mycoll&quot;, &#123;capped:true, size:100000&#125;) 在 capped collection 中，你能添加新的对象。 能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。 使用 Capped Collection 不能删除一个文档，可以使用 drop() 方法删除 collection 所有的行。 删除之后，你必须显式的重新创建这个 collection。 常用操作数据库创建数据库 1use &lt;DATABASE_NAME&gt; 查看所有数据库 1show dbs 查看当数据库 1db 删除数据库 1db.dropDatabase() 注意: 在 MongoDB 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。 集合创建集合 1db.createCollection(COLLECTION_NAME, options) 查看已有集合 1show collections 或 show tables 删除集合 1db.COLLECTION_NAME.drop() 实例 1234&gt; db.createCollection(&quot;mycol&quot;, &#123; capped : true, autoIndexId : true, size : 6142800, max : 10000 &#125; )&#123; &quot;ok&quot; : 1 &#125;&gt; 在 MongoDB 中，你不需要创建集合。当你插入一些文档时，MongoDB 会自动创建集合。 文档插入文档 1db.COLLECTION_NAME.insert(document) 更新文档 123456789db.COLLECTION_NAME.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;) 参数说明： query : update的查询条件，类似sql update查询内where后面的。 update : update的对象和一些更新的操作符（如$,$inc…）等，也可以理解为sql update查询内set后面的 upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入 multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 writeConcern :可选，抛出异常的级别。 实例 1234567891011121314151617181920212223242526&gt;db.col.insert(&#123; title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;Axkeson教程&apos;, url: &apos;http://www.axkeson.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;)&gt;db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.col.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;56064f89ade2f21f36b03136&quot;), &quot;title&quot; : &quot;MongoDB&quot;, &quot;description&quot; : &quot;MongoDB 是一个 Nosql 数据库&quot;, &quot;by&quot; : &quot;Axkeson教程&quot;, &quot;url&quot; : &quot;http://www.axkeson.com&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;database&quot;, &quot;NoSQL&quot; ], &quot;likes&quot; : 100&#125;&gt; save() 方法 通过传入的文档来替换已有文档。语法格式如下：参数说明： document : 文档数据。 writeConcern :可选，抛出异常的级别。 123456db.COLLECTION_NAME.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;) 更多实例 12345db.col.update( &#123; &quot;count&quot; : &#123; $gt : 1 &#125; &#125; , &#123; $set : &#123; &quot;test2&quot; : &quot;OK&quot;&#125; &#125; ); // 只更新第一条记录：db.col.update( &#123; &quot;count&quot; : &#123; $gt : 3 &#125; &#125; , &#123; $set : &#123; &quot;test2&quot; : &quot;OK&quot;&#125; &#125;,false,true ); // 全部更新：db.col.update( &#123; &quot;count&quot; : &#123; $gt : 5 &#125; &#125; , &#123; $set : &#123; &quot;test5&quot; : &quot;OK&quot;&#125; &#125;,true,true ); // 全部添加进去:db.col.update( &#123; &quot;count&quot; : &#123; $gt : 15 &#125; &#125; , &#123; $inc : &#123; &quot;count&quot; : 1&#125; &#125;,false,true ); // 全部更新：db.col.update( &#123; &quot;count&quot; : &#123; $gt : 10 &#125; &#125; , &#123; $inc : &#123; &quot;count&quot; : 1&#125; &#125;,false,false ); // 只更新第一条记录 删除文档 1234db.COLLECTION_NAME.remove( &lt;query&gt;, &lt;justOne&gt;) 查询文档 123db.COLLECTION_NAME.findOne(query, projection)db.COLLECTION_NAME.find(query, projection)db.COLLECTION_NAME.find(query, projection).pretty() 索引创建索引 1db.COLLECTION_NAME.createIndex(keys, options) 参考文档 https://docs.mongodb.com/manual/https://www.runoob.com/mongodb/mongodb-replication.html]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 垃圾回收机制]]></title>
    <url>%2F2017%2F01%2F19%2Fdocs%2F03-php%2Fphp-gc%2F</url>
    <content type="text"><![CDATA[简介在PHP中，没有任何变量指向这个对象时，这个对象就成为垃圾；PHP会将其在内存中销毁。这是PHP的 GC 垃圾回收机制，目的是防止内存溢出； PHP进行内存管理的核心算法一共两项：一是引用计数，二是写时拷贝。当你声明一个PHP变量的时候，C语言就在底层给你搞了一个叫做zval的struct（结构体）；如果你还给这个变量赋值了，比如“hello world”，那么C语言就在底层再给你搞一个叫做zend_value的union（联合体） PHP垃圾回收机制是 php5 之后才有的这个东西，php5.3 之前使用的垃圾回收机制是单纯的“引用计数”，就是每个内存对象都分配一个计数器，当内存对象被变量引用时，计数器+ 1;当变量引用撇掉后，计数器 -1 ；当计数器 =0 时，表名内存中对象没有被使用，该内存对象进行销毁，垃圾回收完成； php5.3开始，使用了新的垃圾回收机制，在引用计数基础上，实现了一种复杂的算法，来检测内存对象中 引用环 的存在，以避免内存泄露； 引用计数基本知识每个php变量存在一个叫”zval”的变量容器中。一个zval变量容器，除了包含变量的类型和值，还包括两个字节的额外信息。第一个是”is_ref”，是个bool值，用来标识这个变量是否是属于引用集合(reference set)。通过这个字节，php引擎才能把普通变量和引用变量区分开来，由于php允许用户通过使用&amp;来使用自定义引用，zval变量容器中还有一个内部引用计数机制，来优化内存使用。第二个额外字节是”refcount”，用以表示指向这个zval变量容器的变量(也称符号即symbol)个数。所有的符号存在一个符号表中，其中每个符号都有作用域(scope)，那些主脚本(比如：通过浏览器请求的的脚本)和每个函数或者方法也都有作用域。 12345678910zval &#123; string &quot;a&quot; //变量的名字是a value zend_value //变量的值 type string //变量是字符串类型&#125;zend_value &#123; string &quot;hello916&quot; //值的内容 refcount 1 //引用计数&#125; 12345678910111213141516171819&lt;?php $a = 'hello'. mt_rand( 1, 1000 );xdebug_debug_zval( 'a');$b = $a;xdebug_debug_zval('a');$c = $a;xdebug_debug_zval('a');unset( $c );xdebug_debug_zval( 'a');// 输出结果a: (refcount=1, is_ref=0)='hello517'a: (refcount=2, is_ref=0)='hello517'a: (refcount=3, is_ref=0)='hello517'a: (refcount=2, is_ref=0)='hello517' 其中，zval struct结构体用于保存$a，zend_value union联合体用于保存数据内容也就是’hello517’。由于后面又声明了b和c，所以C不得不又在底层给你搞出两个zval struct结构体来。 那么写时拷贝是什么意思呢？看下面代码： 123456789101112&lt;?php $a = 'hello'. mt_rand( 1, 1000 );$b = $a;xdebug_debug_zval( 'a');$a = 'world'. mt_rand( 2, 2000 );xdebug_debug_zval( 'a');// 输出结果a: (refcount=2, is_ref=0)='hello834'a: (refcount=1, is_ref=0)='world1198' 引用计数和写时拷贝，那么垃圾回收也该来了。当一个zval在被unset的时候、或者从一个函数中运行完毕出来（就是局部变量）的时候等等很多地方，都会产生zval与zend_value发生断开的行为，这个时候zend引擎需要检测的就是zend_value的refcount是否为0，如果为0，则直接KO free空出内容来。如果zend_value的recount不为0（废话一定是大于0），这个value不能被释放，但是也不代表这个zend_value是清白的，因为此zend_value依然可能是个垃圾。 什么样的情况会导致zend_value的refcount不为0，但是这个zend_value却是个垃圾呢？PHP7种两种情况： 数组：a数组的某个成员使用&amp;引用a自己对象：对象的某个成员引用对象自己 12345&lt;?php$arr = [ 1 ];$arr[] = &amp;$arr;unset( $arr ); 这种情况下，zend_value不会能释放，但也不能放过它，不然一定会产生内存泄漏，所以这会儿zend_value会被扔到一个叫做垃圾回收堆中，然后zend引擎会依次对垃圾回收堆中的这些zend_value进行二次检测，检测是不是由于上述两种情况造成的refcount为1但是自身却确实没有人再用了，如果一旦确定是上述两种情况造成的，那么就会将zend_value彻底抹掉释放内存。 那么垃圾回收发生在什么时候？有些同学可能有疑问，就是php不是运行一次就销毁了吗，我要着gc有何用？并不是啦，首先当一次fpm运行完毕后，最后一定还有gc的，这个销毁就是gc；其次是，内存都是即用即释放的，而不是攒着非得到最后，你想想一个典型的场景，你的控制器里的某个方法里用了一个函数，函数需要一个巨大的数组参数，然后函数还需要修改这个巨大的数组参数，你们应该是函数的运行范围里面修改这个数组，所以此时会发生写时拷贝了，当函数运行完毕后，就得赶紧释放掉这块儿内存以供给其他进程使用，而不是非得等到本地fpm request彻底完成后才销毁。 PHP5 和 PHP7的来及回收机制有什么不同PHP5 和 PHP7 的垃圾回收机制都属于引用计数，但是复杂数据类型的算法处理上：在PHP7中zval有了新的实现方式。 最基础的变化是*zval需要的内存不再是单独从堆上分配，不再自己存储引用计数复杂数据类型（比如字符串、数组和对象）的引用计数由其自身来存储 这种实现方式有以下好处： 简单数据类型不需要单独分配内存，也不需要计数； 不会再有两次计数的情况。在对象中，只有对象自身存储的计数是有效的； 由于现在计数由数值自身存储，所以也就可以和非 zval 结构的数据共享，比如 zval 和 hashtable key 之间； 相关文档 https://www.php.net/manual/zh/features.gc.phphttps://www.sohu.com/a/252341086_470018]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>垃圾回收</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 事务详解]]></title>
    <url>%2F2016%2F10%2F06%2Fdocs%2F04-mysql%2Fmysql-transaction%2F</url>
    <content type="text"><![CDATA[简介MySQL事务主要用于处理操作量大，复杂度高的数据。由一步或几步数据库操作序列组成逻辑执行单元，这系列操作要么全部执行，要么全部放弃执行。在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。事务用来管理 insert,update,delete 语句。 事务的基本特征（ACID）一般来说，事务是必须满足4个条件的（ACID） 原子性： Atomicity，或称不可分割性。一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务的执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性： Consistency，在事务开始之前和事务结束以后，数据库的完整性没有破坏。这表示写入的资料必须完全符合所有的预设规则，这包括资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性： Isolation，又称独立性。数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同的级别，包扩未提交（Read uncommitted）、读提交（Read committed）、可重复读（Repeatable read）和串行化（Serializable）。 持久性： Durability，事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务的隔离级别为什么需要隔离性如果事务之间不是互相隔离的，可能将会出现以下问题。 脏读 脏读（dirty read），简单来说，就是一个事务在处理过程中读取了另外一个事务未提交的数据。这种未提交的数据我们称之为脏数据。依据脏数据所做的操作肯定是不正确的。 不可重复读 不可重复读（non-repeatable read），是指一个事务范围内，多次查询某个数据，却得到不同的结果。在第一个事务中的两次读取数据之间，由于第二个事务的修改，第一个事务两次读到的数据可能就是不一样的。 幻读 幻读（plantom read），是事务非独立执行时发生的一种现象。例如事务 T1 对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务 T2 又对这个表中插入了一行数据项为“1”的数据，并且提交给数据库，而操作事务 T1 的用户如果再查看刚刚修改的数据，会发现数据怎么还是 1？其实这行是从事务 T2 中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 四种隔离级别为了解决上面可能出现的问题，我们就需要设置隔离级别，也就是事务之间按照什么规则进行隔离，将事务隔离到什么程度。 首先，需要明白一点，隔离程度越强，越能保证数据的完整性和一致性，但是付出的代价却是并发执行效率的低下。 ANSI/ISO SQL 定义了 4 种标准隔离级别： Serializable（串行化） 我的事务尚未提交，别人就别想改数据。 花费最高代价但最可靠的事务隔离级别。“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。事务 100% 隔离，可避免脏读、不可重复读、幻读的发生。 Repeatable read（可重复读，默认级别） 别人改数据的事务已经提交，我在我的事务中也不去读。 多次读取同一范围的数据会返回第一次查询的快照，即使其他事务对该数据做了更新修改。事务在执行期间看到的数据前后必须是一致的。但如果这个事务在读取某个范围内的记录时，其他事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行，这就是幻读。 Read committed (读已提交) 别人改数据的事务已经提交，我在我的事务中才能读到。 保证一个事务提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。可避免脏读的发生，但是可能会造成不可重复读。大多数数据库的默认级别就是 Read committed，比如 Sql Server , Oracle。 Read uncommitted (读未提交) 别人改数据的事务尚未提交，我在我的事务中也能读到。 最低的事务隔离级别，一个事务还没提交时，它做的变更就能被别的事务看到。任何情况都无法保证。 隔离级别与一致性关系 隔离级别 脏读 不可重复读 幻读 Read uncommitted 可能 可能 可能 Read committed 不可能 可能 可能 Repeatable Read 不可能 不可能 可能 Serializable 不可能 不可能 不可能 隔离级别的一些基本操作设置事务隔离级别 可以在my.ini文件中使用transaction-isolation选项来设置服务器的缺省事务隔离级别 该选项值可以是 12345678– READ-UNCOMMITTED– READ-COMMITTED– REPEATABLE-READ– SERIALIZABLE# 例如：[mysqld]transaction-isolation = READ-COMMITTED 通过命令动态设置隔离级别 隔离级别也可以在运行的服务器中动态设置，应使用SET TRANSACTION ISOLATION LEVEL语句 123456789101112131415SET [GLOBAL | SESSION] TRANSACTION ISOLATION LEVEL &lt;isolation-level&gt; 其中的&lt;isolation-level&gt;可以是： – READ UNCOMMITTED – READ COMMITTED – REPEATABLE READ – SERIALIZABLE# 例如： • 事务隔离级别的作用范围分为两种： – 全局级：对所有的会话有效 – 会话级：只对当前的会话有效 SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED； # 会话级SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED； # 全局级 查看隔离级别1mysql&gt; select @@tx_isolation; 隔离级别的实现事务的机制是通过视图（read-view）来实现的并发版本控制（MVCC），不同的事务隔离级别创建读视图的时间点不同。 可重复读是每个事务重建读视图，整个事务存在期间都用这个视图。 读已提交是每条 SQL 创建读视图，在每个 SQL 语句开始执行的时候创建的。隔离作用域仅限该条 SQL 语句。 读未提交是不创建，直接返回记录上的最新值 串行化隔离级别下直接用加锁的方式来避免并行访问。 这里的视图可以理解为数据副本，每次创建视图时，将当前已持久化的数据创建副本，后续直接从副本读取，从而达到数据隔离效果。 事务控制语句 BEGIN 或 START TRANSACTION 显式地开启一个事务； COMMIT 也可以使用 COMMIT WORK，不过二者是等价的。COMMIT 会提交事务，并使已对数据库进行的所有修改成为永久性的； ROLLBACK 也可以使用 ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier，SAVEPOINT 允许在事务中创建一个保存点，一个事务中可以有多个 SAVEPOINT； RELEASE SAVEPOINT identifier 删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier 把事务回滚到标记点； SET TRANSACTION 用来设置事务的隔离级别。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748mysql&gt; use RUNOOB;Database changedmysql&gt; CREATE TABLE runoob_transaction_test( id int(5)) engine=innodb; # 创建数据表Query OK, 0 rows affected (0.04 sec) mysql&gt; select * from runoob_transaction_test;Empty set (0.01 sec) mysql&gt; begin; # 开始事务Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into runoob_transaction_test value(5);Query OK, 1 rows affected (0.01 sec) mysql&gt; insert into runoob_transaction_test value(6);Query OK, 1 rows affected (0.00 sec) mysql&gt; commit; # 提交事务Query OK, 0 rows affected (0.01 sec) mysql&gt; select * from runoob_transaction_test;+------+| id |+------+| 5 || 6 |+------+2 rows in set (0.01 sec) mysql&gt; begin; # 开始事务Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into runoob_transaction_test values(7);Query OK, 1 rows affected (0.00 sec) mysql&gt; rollback; # 回滚Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from runoob_transaction_test; # 因为回滚所以数据没有插入+------+| id |+------+| 5 || 6 |+------+2 rows in set (0.01 sec) mysql&gt; PHP中使用事务实例 1234567891011121314151617181920212223242526&lt;?php$dbhost = 'localhost:3306'; // mysql服务器主机地址$dbuser = 'root'; // mysql用户名$dbpass = '123456'; // mysql用户名密码$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('连接失败: ' . mysqli_error($conn));&#125;// 设置编码，防止中文乱码mysqli_query($conn, "set names utf8");mysqli_select_db( $conn, 'RUNOOB' );mysqli_query($conn, "SET AUTOCOMMIT=0"); // 设置为不自动提交，因为MYSQL默认立即执行mysqli_begin_transaction($conn); // 开始事务定义 if(!mysqli_query($conn, "insert into runoob_transaction_test (id) values(8)"))&#123; mysqli_query($conn, "ROLLBACK"); // 判断当执行失败时回滚&#125; if(!mysqli_query($conn, "insert into runoob_transaction_test (id) values(9)"))&#123; mysqli_query($conn, "ROLLBACK"); // 判断执行失败时回滚&#125;mysqli_commit($conn); //执行事务mysqli_close($conn); 什么是大事务 定义 运行时间比较长，操作的数据比较多的事务。 大事务风险 锁定太多的数据，造成大量的阻塞和锁超时，回滚所需要的时间比较长。执行时间长，容易造成主从延迟 如何处理大事务 避免一次处理太多大数据。移出不必要在事务中的select操作。 相关文章 http://blog.itpub.net/31559358/viewspace-2221931https://www.runoob.com/mysql/mysql-transaction.htmlhttps://blog.csdn.net/w_linux/article/details/79666086https://blog.csdn.net/changudeng1992/article/details/81988927]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Explain详解]]></title>
    <url>%2F2016%2F03%2F29%2Fdocs%2F04-mysql%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[简介使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。 通过EXPLAIN，我们可以分析出以下结果 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 使用方式如下EXPLAIN +SQL语句 123456789mysql&gt; explain select * from test where name = 10000;+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | test | NULL | ALL | NULL | NULL | NULL | NULL | 378000 | 10.00 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+-------------+1 row in set, 1 warning (0.00 sec)mysql&gt; 执行计划各字段含义概要描述 字段 描述 id 选择标识符 select_type 表示查询的类型 table 输出结果集的表 partitions 匹配的分区 type 表示表的连接类型 possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 ref 列与索引的比较 rows 扫描出的行数(估算的行数) filtered 按表条件过滤的行百分比 Extra 执行情况的描述和说明 idselect查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序 id相同时，执行顺序由上至下 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 select_type表示查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询 select_type 描述 SIMPLE 简单SELECT，不使用UNION或子查询等 PRIMARY 子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY UNION UNION中的第二个或后面的SELECT语句 DEPENDENT UNION UNION中的第二个或后面的SELECT语句，取决于外面的查询 UNION RESULT UNION的结果，union语句中第二个select开始后面所有select SUBQUERY 子查询中的第一个SELECT，结果不依赖于外部查询 DEPENDENT SUBQUERY 子查询中的第一个SELECT，依赖于外部查询 DERIVED 派生表的SELECT, FROM子句的子查询 UNCACHEABLE SUBQUERY 一个子查询的结果不能被缓存，必须重新评估外链接的第一行 table指的就是当前执行的表 type对表访问方式，表示MySQL在表中找到所需行的方式，又称”访问类型”。常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从左到右，性能从差到好） type 描述 ALL Full Table Scan， MySQL将遍历全表以找到匹配的行 index Full Index Scan，index与ALL区别为index类型只遍历索引树 range 只检索给定范围的行，使用一个索引来选择行 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量，system是const类型的特例，当查询的表只有一行的情况下，使用system system 表只有一行记录（等于系统表），这是const类型的特列，平时不会出现，这个也可以忽略不计 NULL MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 possible_keys指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用（该查询可以利用的索引，如果没有任何索引显示 null） 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 Keykey列显示MySQL实际决定使用的键（索引），必然包含在possible_keys中 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 ref列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 Extra该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询，常见 group by ; order by Using filesort：当Query中包含 order by 操作，而且无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行（通过收集统计信息不可能存在结果）。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 No tables used：Query语句中使用from dual 或不含任何from子句 总结 EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 EXPLAIN不考虑各种Cache EXPLAIN不能显示MySQL在执行查询时所作的优化工作 部分统计信息是估算的，并非精确值 EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Explain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 索引详解]]></title>
    <url>%2F2016%2F03%2F28%2Fdocs%2F04-mysql%2Fmysql-index%2F</url>
    <content type="text"><![CDATA[介绍什么是索引？一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。 为什么要有索引呢？索引在MySQL中也叫做“键”，是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能非常关键，尤其是当表中的数据量越来越大时，索引对于性能的影响愈发重要。索引优化应该是对查询性能优化最有效的手段了。索引能够轻易将查询性能提高好几个数量级。索引相当于字典的音序表，如果要查某个字，如果不使用音序表，则需要从几百页中逐页去查。 索引的分类 普通索引 唯一索引 联合索引 全文索引 空间索引 索引的优缺点 优势： 可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序 劣势： 索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的1.5倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表 索引的实现原理哈希索引只有memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的hashCode，然后在hashCode相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个hashCode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能。 全文索引FULLTEXT（全文）索引，仅可用于MyISAM和InnoDB，针对较大的数据，生成全文索引非常的消耗时间和空间。对于文本的大对象，或者较大的CHAR类型的数据，如果使用普通索引，那么匹配文本前几个字符还是可行的，但是想要匹配文本中间的几个单词，那么就要使用LIKE %word%来匹配，这样需要很长的时间来处理，响应时间会大大增加，这种情况，就可使用时FULLTEXT索引了，在生成FULLTEXT索引时，会为文本生成一份单词的清单，在索引时及根据这个单词的清单来索引。FULLTEXT可以在创建表的时候创建，也可以在需要的时候用ALTER或者CREATE INDEX来添加。 BTree索引和B+Tree索引BTree索引BTree是平衡搜索多叉树，设树的度为2d（d&gt;1），高度为h，那么BTree要满足以一下条件： 每个叶子结点的高度一样，等于h； 每个非叶子结点由n-1个key和n个指针point组成，其中d&lt;=n&lt;=2d,key和point相互间隔，结点两端一定是key； 叶子结点指针都为null； 非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据； B+Tree索引B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于： B+Tree中的非叶子结点不存储数据，只存储键值； B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址； B+Tree的每个非叶子节点由n个键值key和n个指针point组成； 聚簇索引和非聚簇索引分析了MySQL的索引结构的实现原理，然后我们来看看具体的存储引擎怎么实现索引结构的，MySQL中最常见的两种存储引擎分别是MyISAM和InnoDB，分别实现了非聚簇索引和聚簇索引。 聚簇索引的解释是:聚簇索引的顺序就是数据的物理存储顺序 非聚簇索引的解释是:索引顺序与数据物理排列顺序无关 索引的使用策略什么时候要使用索引 主键自动建立唯一索引； 经常作为查询条件在WHERE或者ORDER BY 语句中出现的列要建立索引； 作为排序的列要建立索引； 查询中与其他表关联的字段，外键关系建立索引 高并发条件下倾向组合索引； 用于聚合函数的列可以建立索引，例如使用了max(column_1)或者count(column_1)时的column_1就需要建立索引 什么时候不要使用索引 经常增删改的列不要建立索引； 有大量重复的列不建立索引； 表记录太少不要建立索引。只有当数据库里已经有了足够多的测试数据时，它的性能测试结果才有实际参考价值。如果在测试数据库里只有几百条数据记录，它们往往在执行完第一条查询命令之后就被全部加载到内存里，这将使后续的查询命令都执行得非常快–不管有没有使用索引。只有当数据库里的记录超过了1000条、数据总量也超过了MySQL服务器上的内存总量时，数据库的性能测试结果才有意义。 索引失效的情况 在组合索引中不能有列的值为NULL，如果有，那么这一列对组合索引就是无效的。 在一个SELECT语句中，索引只能使用一次，如果在WHERE中使用了，那么在ORDER BY中就不要用了。 LIKE操作中，’%aaa%’不会使用索引，也就是索引会失效，但是‘aaa%’可以使用索引。 在索引的列上使用表达式或者函数会使索引失效，例如：select * from users where YEAR(adddate)&lt;2007，将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成：select * from users where adddate&lt;’2007-01-01′。其它通配符同样，也就是说，在查询条件中使用正则表达式时，只有在搜索模板的第一个字符不是通配符的情况下才能使用索引。 在查询条件中使用不等于，包括&lt;符号、&gt;符号和！=会导致索引失效。特别的是如果对主键索引使用！=则不会使索引失效，如果对主键索引或者整数类型的索引使用&lt;符号或者&gt;符号不会使索引失效。（经erwkjrfhjwkdb同学提醒，不等于，包括&lt;符号、&gt;符号和！，如果占总记录的比例很小的话，也不会失效） 在查询条件中使用IS NULL或者IS NOT NULL会导致索引失效。 字符串不加单引号会导致索引失效。更准确的说是类型不一致会导致失效，比如字段email是字符串类型的，使用WHERE email=99999 则会导致失败，应该改为WHERE email=’99999’。 在查询条件中使用OR连接多个条件会导致索引失效，除非OR链接的每个条件都加上索引，这时应该改为两次查询，然后用UNION ALL连接起来。 如果排序的字段使用了索引，那么select的字段也要是索引字段，否则索引失效。特别的是如果排序的是主键索引则select * 也不会导致索引失效。尽量不要包括多列排序，如果一定要，最好为这队列构建组合索引； 索引的语法查看一张表的索引123用法：show index from TABLE_NAME;mysql&gt; show index from users; 建立索引1234567891011121314151617181920用法# 普通索引 create index INDEX_NAME on TABLE_NAME(字段名);# 唯一索引 create unique INDEX_NAME on TABLE_NAME(字段名);# 全文索引 create fulltext INDEX_NAME on TABLE_NAME(字段名);# 多列索引 create index INDEX_NAME on TABLE_NAME(字段名,字段名);mysql&gt; create index index_name on user(name, age);创建表时直接创建索引CREATE TABLE school( NAME VARCHAR(8) NOT NULL , sid INT PRIMARY KEY auto_increment NOT NULL , age INT NOT NULL , sex ENUM(&apos;F&apos; , &apos;M&apos;) , INDEX(sid , NAME));在已存在的表上添加索引mysql&gt; alter table school add index index_name(name); 删除索引1mysql&gt; drop index INDEX_NAME on TABLE_NAME; 查看查询语句使用索引的情况1mysql&gt; explain SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = &apos;123&apos;; 索引测试准备 创建表 123456CREATE TABLE test( id INT , name VARCHAR(20) , gender CHAR(6) , email VARCHAR(50)); 创建存储过程，实现批量插入记录 123456789101112131415161718delimiter $$ #声明存储过程的结束符号为$$CREATE PROCEDURE auto_insert()BEGINDECLARE i INT DEFAULT 1 ;WHILE(i &lt; 1000000) DO INSERT INTO s1VALUES ( i , concat(&apos;egon&apos; , i) , &apos;male&apos; , concat(&apos;egon&apos; , i , &apos;@oldboy&apos;) ) ;SET i = i + 1 ;ENDWHILE ; END$$ #$$结束delimiter ; #重新声明分号为结束符号 调用存储过程 1call auto_insert(); 在没有索引的前提下测试查询速度123456789mysql&gt; select * from test where id = 10000;+-------+-----------+--------+------------------+| id | name | gender | email |+-------+-----------+--------+------------------+| 10000 | egon10000 | male | egon10000@oldboy |+-------+-----------+--------+------------------+1 row in set (0.03 sec)mysql&gt; 加上索引12345678910111213mysql&gt; create index idx on test(id);Query OK, 0 rows affected (0.60 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; select * from test where id = 10000;+-------+-----------+--------+------------------+| id | name | gender | email |+-------+-----------+--------+------------------+| 10000 | egon10000 | male | egon10000@oldboy |+-------+-----------+--------+------------------+1 row in set (0.00 sec)mysql&gt; 相关文章 https://www.runoob.com/mysql/mysql-index.htmlhttps://www.cnblogs.com/bypp/p/7755307.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP状态码]]></title>
    <url>%2F2016%2F03%2F01%2Fdocs%2F09-pc-base%2Fhttp-status-codes%2F</url>
    <content type="text"><![CDATA[简介当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 HTTP状态码的英文为HTTP Status Code。 分类HTTP状态码（HTTP Status Code）是用以表示网页服务器HTTP响应状态的3位数字代码。它由 RFC 2616 规范定义的，并得到RFC 2518、RFC 2817、RFC 2295、RFC 2774、RFC 4918等规范扩展。 所有状态码的第一个数字代表了响应的五种状态之一。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 HTTP状态码列表 状态码 含义 100 客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。 只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 200 请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 ‘202 Accepted’。 202 服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。 返回202状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回202状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超级。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。 如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。 由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 服务器成功处理了请求，且没有返回任何内容。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 与204响应一样，该响应也被禁止包含任何消息体，且以消息头后的第一个空行结束。 206 服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。 该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 响应必须包含如下的头部域： Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。 Date ETag 和/或 Content-Location，假如同样的请求本应该返回200响应。 Expires, Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回200响应中应当返回的所有实体头部域。 假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将206响应返回的内容与之前任何缓存过的内容组合在一起。 任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存206响应返回的内容。 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 300 被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 除非这是一个 HEAD 请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由 Content-Type 定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 如果服务器本身已经有了首选的回馈选择，那么在 Location 中应当指明这个回馈的 URI；浏览器可能会将这个 Location 值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。 301 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 新的永久性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用 HTTP/1.0 协议的浏览器，当它们发送的 POST 请求得到了一个301响应的话，接下来的重定向请求将会变成 GET 方式。 302 请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用 GET 方式访问在 Location 中规定的 URI，而无视原先请求的方法。状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 303 对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 新的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 注意：许多 HTTP/1.1 版以前的 浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。 304 如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 该响应必须包含以下的头信息： Date，除非这个服务器没有时钟。假如没有时钟的服务器也遵守这些规则，那么代理服务器以及客户端可以自行将 Date 字段添加到接收到的响应头中去（正如RFC 2068中规定的一样），缓存机制将会正常工作。 ETag 和/或 Content-Location，假如同样的请求本应返回200响应。 Expires, Cache-Control，和/或Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了强缓存验证，那么本次响应不应该包含其他实体头；否则（例如，某个带条件的 GET 请求使用了弱缓存验证），本次响应禁止包含其他实体头；这避免了缓存了的实体内容和更新了的实体头信息之间的不一致。 假如某个304响应指明了当前某个实体没有缓存，那么缓存系统必须忽视这个响应，并且重复发送不包含限制条件的请求。 假如接收到一个要求更新某个缓存条目的304响应，那么缓存系统必须更新整个条目以反映所有在响应中被更新的字段的值。 305 被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。 注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。 306 在最新版的规范中，306状态码已经不再被使用。 307 请求的资源现在临时从不同的URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的URI 应当在响应的 Location 域中返回。除非这是一个HEAD 请求，否则响应的实体中应当包含指向新的URI 的超链接及简短说明。因为部分浏览器不能识别307响应，因此需要添加上述必要信息以便用户能够理解并向新的 URI 发出访问请求。 如果这不是一个GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 400 1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 2、请求参数有误。 401 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。参见RFC 2617。 402 该状态码是为了将来可能的需求而预留的。 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个 HEAD 请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 405 请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 鉴于 PUT，DELETE 方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。 406 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 除非这是一个 HEAD 请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址列表的实体。实体的格式由 Content-Type 头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。 407 与401响应类似，只不过客户端必须在代理服务器上进行身份验证。代理服务器必须返回一个 Proxy-Authenticate 用以进行身份询问。客户端可以返回一个 Proxy-Authorization 信息头用以验证。参见RFC 2617。 408 请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。这个代码只允许用在这样的情况下才能被使用：用户被认为能够解决冲突，并且会重新提交新的请求。该响应应当包含足够的信息以便用户发现冲突的源头。 冲突通常发生于对 PUT 请求的处理中。例如，在采用版本检查的环境下，某次 PUT 提交的对特定资源的修改请求所附带的版本信息与之前的某个（第三方）请求向冲突，那么此时服务器就应该返回一个409错误，告知用户请求无法完成。此时，响应实体中很可能会包含两个冲突版本之间的差异比较，以便用户重新提交归并以后的新版本。 410 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。这样的状况应当被认为是永久性的。如果可能，拥有链接编辑功能的客户端应当在获得用户许可后删除所有指向这个地址的引用。如果服务器不知道或者无法确定这个状况是否是永久的，那么就应该使用404状态码。除非额外说明，否则这个响应是可缓存的。 410响应的目的主要是帮助网站管理员维护网站，通知用户该资源已经不再可用，并且服务器拥有者希望所有指向这个资源的远端连接也被删除。这类事件在限时、增值服务中很普遍。同样，410响应也被用于通知客户端在当前服务器站点上，原本属于某个个人的资源已经不再可用。当然，是否需要把所有永久不可用的资源标记为’410 Gone’，以及是否需要保持此标记多长时间，完全取决于服务器拥有者。 411 服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412 服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。 413 服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 如果这个状况是临时的，服务器应当返回一个 Retry-After 的响应头，以告知客户端可以在多少时间以后重新尝试。 414 请求的URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。这比较少见，通常的情况包括： 本应使用POST方法的表单提交变成了GET方法，导致查询字符串（Query String）过长。 重定向URI “黑洞”，例如每次重定向把旧的 URI 作为新的 URI 的一部分，导致在若干次重定向后 URI 超长。 客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的 URI，当 GET 后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[1]。没有此类漏洞的服务器，应当返回414状态码。 415 对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416 如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回416状态码。 假如 Range 使用的是字节范围，那么这种情况就是指请求指定的所有数据范围的首字节位置都超过了当前资源的长度。服务器也应当在返回416状态码的同时，包含一个 Content-Range 实体头，用以指明当前资源的长度。这个响应也被禁止使用 multipart/byteranges 作为其 Content-Type。 417 在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 请求格式正确，但是由于含有语义错误，无法响应。（RFC 4918 WebDAV）423 Locked 当前资源被锁定。（RFC 4918 WebDAV） 424 由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。（RFC 4918 WebDAV） 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426 客户端应当切换到TLS/1.0。（RFC 2817） 449 由微软扩展，代表请求应当在执行完适当的操作后进行重试。 500 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 注意：某些代理服务器在DNS查询超时时会返回400或者500错误 505 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510 获取资源所需要的策略并没有没满足。（RFC 2774） 相关文章 http://tool.oschina.net/commons?type=5http://www.httpstatus.cnhttps://www.runoob.com/http/http-status-codes.html]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 修改root密码的几种方法]]></title>
    <url>%2F2016%2F01%2F25%2Fdocs%2F04-mysql%2Fmysql-root-password-update%2F</url>
    <content type="text"><![CDATA[SET PASSWORD 命令123格式:mysql&gt; SET PASSWORD FOR 用户名@localhost = PASSWORD(&apos;新密码&apos;) mysql&gt; SET PASSWORD FOR root@localhost = PASSWORD(&apos;123456&apos;); 使用 mysqladmin 命令123格式:mysqladmin -u用户名 -p旧密码 password 新密码; $ mysqladmin -uroot -p123456 password 12345678; 使用 update 直接编辑 user 表123mysql&gt; USE mysql; mysql&gt; UPDATE user SET password = password(&apos;123456&apos;) WHERE user = &apos;root&apos; AND host = &apos;loclhoast&apos;; mysql&gt; flush privileges; 在忘记root密码的时候,可以这样Linux12345678910111213# Stop MySQLsudo service mysql stop# Make MySQL service directory.sudo mkdir /var/run/mysqld# Give MySQL user permission to write to the service directory.sudo chown mysql: /var/run/mysqld# Start MySQL manually, without permission checks or networking.sudo mysqld_safe --skip-grant-tables --skip-networking &amp;# Log in without a password.mysql -uroot mysqlUPDATE mysql.user SET authentication_string=PASSWORD('YOURNEWPASSWORD'), plugin='mysql_native_password' WHERE User='root' AND Host='%';EXIT; windows 关闭正在运行的MySQL服务 打开DOS窗口,转到mysql/bin目录 输入mysqld –skip-grant-tables回车。–skip-grant-tables的意思是启动MySQL服务的时候跳过权限表认证 再打开一个DOS窗口(因为刚才那个DOS窗口已经不能动啦),转到mysql/bin目录 输入mysql回车,如果成功,将出现MySQL提示符 &gt; 修改密码:UPDATE user SET password = password(‘123456’) WHERE user = ‘root’ 刷新权限(必须):flush priviliges; 退出:quit 注销系统,再进入使用户名root和刚才设置的新密码123456登录 相关文章 https://coderwall.com/p/j9btlg/reset-the-mysql-5-7-root-password-in-ubuntu-16-04-lts]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 数据库安装]]></title>
    <url>%2F2016%2F01%2F18%2Fdocs%2F04-mysql%2Fmysql-install%2F</url>
    <content type="text"><![CDATA[安装Oracle官方的yum源12$ wget http://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm$ rpm -ivh mysql-community-release-el6-5.noarch.rpm 安装MySQL服务器端和客户端1$ yum -y install mysql-server mysql 配置服务启动1$ chkconfig mysqld on 初始化安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768$ service mysqld start第一次启动时会自动创建初始化数据库，启动后执行 /usr/bin/mysql_secure_installation回车后显示如下信息： NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MySQL to secure it, we'll need the current password for the root user. If you've just installed MySQL, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none):（直接回车）执行后会提示输入当前密码，当前密码为空，直接回车 Set root password? [Y/n] （输入Y）回车后会提示是否设置root密码，输入“Y”回车，然后输入root的密码 New password: Re-enter new password:输入两次密码，回车后显示如下信息： By default, a MySQL installation has an anonymous user, allowing anyone to log into MySQL without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n]（输入Y）输入Y移除匿名用户 Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n]（输入n）输入n，不禁止root远程登录 By default, MySQL comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n]（输入Y）输入Y移除测试数据库，可能有错误信息，可以忽略 Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n]（输入Y）输入Y重载数据库权限 All done! If you've completed all of the above steps, your MySQL installation should now be secure. Thanks for using MySQL! Cleaning up...安装成功 配置文件1$ find / -name my.cnf 查找my.cnf的位置，确保只有/etc/my.cnf一个文件，如果在其他目录下还有该文件，将其他目录下的文件删除，只保留/etc/my.cnf 修改配置文件中主要是innodb_buffer_pool_size项的配置，如果该服务器为数据库专用服务器，则将该项配置为服务器内存的70%左右，例如服务器内存为16G的，可以将此项配置为12G，然后服务器内存为32G的，可以将此项配置为24G、26G或28G 删除/var/lib/mysql/下的ib_logfile0、ib_logfile1文件 123456$ service mysqld restart#进入mysql命令行$ mysql -uroot -p#查看InnoDB引擎是否启用mysql&gt; show engines; Engine Support MRG_MYISAM YES CSV YES MyISAM DEFAULT InnoDB YES MEMORY YES 如果看到InnoDB为yes即正常启用了，如果不为yes，则说明未启用，说明安装有问题。 完成安装 配置远程登录1234567$ service mysqld startmysql -uroot -p（不允许直接在-p后面直接带上密码，这样容易导致密码泄露，因为可以通过history看历史命令获取该密码）输入密码use mysql//将Host为localhost的记录改为允许任意主机访问%update user set Host='%' where Host='localhost';flush privileges; 注意：远程访问必须确认主机的防火墙中已开启3306端口的访问，否则将无法远程访问数据库]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在IDE上安装代码规范检查工具]]></title>
    <url>%2F2016%2F01%2F01%2Fdocs%2F02-tools%2Fphpcs-ide-setting%2F</url>
    <content type="text"><![CDATA[我相信每个公司都有一套完备的代码规范标准，但标准是标准，如何能有效的让所有人遵守，那就要工具的辅助和实时提醒了。 安装phpcs使用composer全局安装phpcs1$ composer global require "squizlabs/php_codesniffer=*" 具体可参考：https://github.com/squizlabs/PHP_CodeSniffer IDE集成PHPStorm 设置 (适用mac) 打开PHPStorm点击 PhpStorm -&gt; Preference; 点击 Languages &amp; Frameworks -&gt; PHP -&gt; Code Sniffer; 点击 Configuration 右侧的按钮; 选择 PHP Code Sniffer（phpcs）path：的路径，就是刚才composer之后生成的那个phpcs(/vendor/squizlabs/php_codesniffer/bin/phpcs)的路径; 选择之后点击 Validate 验证成功; 继续点击 Editor -&gt; Inspections 展开点击右侧的PHP; 勾选 PHP Code Sniffer Validation 选择右侧的PSR2; 勾选 PHP Mess Detector Validation 右侧 Options 全部勾选; 点击Code Style -&gt; PHP -&gt; Set from... -&gt; Predefiend Style 选择 PSR1/PSR2 现在笔者使用phpstorm的格式化，将会自动格式化成psr-2的风格。 Sublime Text (适用mac) 安装Package Control command + shift + p 调出 安装界面 install package Preferences-&gt;Package Settings-&gt;PHP Code Sniffer-&gt;Settings - User(Default) 配置phpcs 路径 “phpcs_executable_path”: “/usr/local/bin/phpcs” 配置phpcbf 路径 “phpcbf_executable_path”: “/usr/local/bin/phpcbf” VSCode TODO 如果写的代码不符合PSR-2编码风格规范的时候，该行代码会有波浪线，点击波浪线可以查看提示信息，根据信息我们修改就可以写出优雅的代码了。 参考文章：https://segmentfault.com/a/1190000015971297]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>phpcs</tag>
        <tag>代码检查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）开发效率低？造成代码难以维护的35个恶习]]></title>
    <url>%2F2016%2F01%2F01%2Fdocs%2F01-program-life%2F01001%2F</url>
    <content type="text"><![CDATA[代码组织 总是说”一会弄好”，但从来不兑现。(缺乏任务管理和时间管理能力) 坚持所谓的高效、优雅的”一行代码流”，事实上，可读性才是最重的，聪明是第二位的。 无意义的优化。(类似网页大小之类的优化最后在做) 不注重代码样式和风格的严谨。 使用无意义的命名。 忽略经过验证的最佳实践。(例如代码审核、TDD、QA、自动化部署等，推荐阅读软件开发必读经典著作：Making Software：What Really Works，and Why We Believe It) 给自己埋雷。(例如使用不会报错的类库或者忽略例外) 团队工作 过早放弃计划。 坚持一个无效的计划。 总是单打独斗。(必须强迫自己于团队分享进度和想法，避免错觉，提高效率) 拒绝写糟糕的代码。(日程紧迫的时候可以写一些”糟糕”的代码，这是程序员的能力而不是bug，当然，有时间的时候一定要回头偿还”技术债”) 抱怨他人。 不与团队分享所学。 向主管/客户反馈的速度过慢。 不会充分利用Google。 看重个人编码风格。 带着个人情绪看待他人对自己代码的评论和注释。 写代码 不懂优化策略。 使用错误的工具。 不追求对开发工具和IDE的精熟。 忽略报错信息 迷恋趁手的开发工具。(不同类型的开发任务需要匹配对应的最佳开发工具，例如Sublime适合动态语言，而Eclipse适合Java，如果你喜欢vim或emacs，并不意味着能用这些工具干所有事) 不注重代码中赋值的可配置型。(不养成把代码中的活动部件分离出来的习惯，会导致技术债暴增) 喜欢重新发明车轮。 盲目的剪切/粘贴代码。 应付差事，不求甚解，不花时间搞清楚项目运作的机理。 对自己写的代码过度的自信。 不去考虑每一个设计、方案或者代码库的”副作用”。(一个成功的用例并不意味着”万灵药”) 在一个地方卡住了但坚持不呼救。 测试与维护 只去写能通过的测试。 重要项目中忽略性能测试。 不去核实代码是否真的可用，没有养成开发中及时快速测试的习惯。 重大改进延迟推送。 抛弃和逃避自己的代码。 忽略其他非功能性需求。(例如安全和性能，准备一份这方面的清单，忽略这些会毁掉你的所有成果) 点击查看原文]]></content>
      <categories>
        <category>程序人生</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
        <tag>开发效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文章列表]]></title>
    <url>%2F2016%2F01%2F01%2Fdocs%2FREADME%2F</url>
    <content type="text"><![CDATA[目录 程序人生 开发效率低？造成代码难以维护的35个恶习 工具 在IDE上安装代码规范检查工具 PsySh PHP交互控制台 Git 客户端多账号管理 CentOS7.4搭建shadowsocks，以及配置BBR加速 PHP PHP 垃圾回收机制 MySQL MySQL 数据库安装 MySQL 修改root密码的几种方法 MySQL 事务详解 MySQL 索引详解 MySQL Explain详解 Linux Homestead 下安装Swoole扩展 TODO Nginx+php-fpm 运行原理 TODO AWK 命令详解 NoSQL MongoDB 基础教程 MongoDB 复制（副本集） MongoDB 分片 数据存储 计算机基础 HTTP状态码 HTTP和HTTPS的区别与联系 TODO 从输入 URL 到页面加载完成的过程中都发生了什么事情？ 数据结构预算法]]></content>
  </entry>
</search>
