<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Redis 为什么选择单线程模型（转）]]></title>
    <url>%2F2019%2F10%2F27%2Fdocs%2F07-redis%2Fredis-thread-model%2F</url>
    <content type="text"><![CDATA[Redis 作为广为人知的内存数据库，在玩具项目和复杂的工业级别项目中都看到它的身影，然而 Redis 却是使用单线程模型进行设计的，这与很多人固有的观念有所冲突，为什么单线程的程序能够抗住每秒几百万的请求量呢？这也是我们今天要讨论的问题之一。 除此之外，Redis 4.0 之后的版本却抛弃了单线程模型这一设计，原本使用单线程运行的 Redis 也开始选择性使用多线程模型，这一看似有些矛盾的设计决策是今天需要讨论的另一个问题。 概述就像在介绍中说的，这一篇文章想要讨论的两个与 Redis 有关的问题就是： 为什么 Redis 在最初的版本中选择单线程模型？ 为什么 Redis 在 4.0 之后的版本中加入了多线程的支持？ 这两个看起来有些矛盾的问题实际上并不冲突，我们会分别阐述对这个看起来完全相反的设计决策作出分析和解释，不过在具体分析它们的设计之前，我们先来看一下不同版本 Redis 顶层的设计： Redis 作为一个内存服务器，它需要处理很多来自外部的网络请求，它使用 I/O 多路复用机制同时监听多个文件描述符的可读和可写状态，一旦受到网络请求就会在内存中快速处理，由于绝大多数的操作都是纯内存的，所以处理的速度会非常地快。 在 Redis 4.0 之后的版本，情况就有了一些变动，新版的 Redis 服务在执行一些命令时就会使用『主处理线程』之外的其他线程，例如 UNLINK、FLUSHALL ASYNC、FLUSHDB ASYNC 等非阻塞的删除操作。 设计无论是使用单线程模型还是多线程模型，这两个设计上的决定都是为了更好地提升 Redis 的开发效率、运行性能，想要理解两个看起来矛盾的设计决策，我们首先需要重新梳理做出决定的上下文和大前提，从下面的角度来看，使用单线程模型和多线程模型其实也并不矛盾。 虽然 Redis 在较新的版本中引入了多线程，不过是在部分命令上引入的，其中包括非阻塞的删除操作，在整体的架构设计上，主处理程序还是单线程模型的；由此看来，我们今天想要分析的两个问题可以简化成 为什么 Redis 服务使用单线程模型处理绝大多数的网络请求？ 为什么 Redis 服务增加了多个非阻塞的删除操作，例如：UNLINK、FLUSHALL ASYNC 和 FLUSHDB ASYNC？ 接下来的两个小节将从多个角度分析这两个问题。 单线程模型Redis 从一开始就选择使用单线程模型处理来自客户端的绝大多数网络请求，这种考虑其实是多方面的，作者分析了相关的资料，发现其中最重要的几个原因如下： 使用单线程模型能带来更好的可维护性，方便开发和调试； 使用单线程模型也能并发的处理客户端的请求； Redis 服务中运行的绝大多数操作的性能瓶颈都不是 CPU； 上述三个原因中的最后一个是最终使用单线程模型的决定性因素，其他的两个原因都是使用单线程模型额外带来的好处，在这里我们会按顺序介绍上述的几个原因。 可维护性可维护性对于一个项目来说非常重要，如果代码难以调试和测试，问题也经常难以复现，这对于任何一个项目来说都会严重地影响项目的可维护性。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，代码的执行过程不再是串行的，多个线程同时访问的变量如果没有谨慎处理就会带来诡异的问题 在网络上有一个调侃多线程模型的段子，就很好地展示了多线程模型带来的潜在问题：竞争条件 (race condition) —— 如果计算机中的两个进程（线程同理）同时尝试修改一个共享内存的内容，在没有并发控制的情况下，最终的结果依赖于两个进程的执行顺序和时机，如果发生了并发访问冲突，最后的结果就会是不正确的。 引入了多线程，我们就必须要同时引入并发控制来保证在多个线程同时访问数据时程序行为的正确性，这就需要工程师额外维护并发控制的相关代码，例如，我们会需要在可能被并发读写的变量上增加互斥锁： 123456789101112131415161718var ( mu Mutex // cost data int)// thread 1func() &#123; mu.Lock() data += 1 mu.Unlock()&#125;// thread 2func() &#123; mu.Lock() data -= 1 mu.Unlock()&#125; 在访问这些变量或者内存之前也需要先对获取互斥锁，一旦忘记获取锁或者忘记释放锁就可能会导致各种诡异的问题，管理相关的并发控制机制也需要付出额外的研发成本和负担。 并发处理使用单线程模型也并不意味着程序不能并发的处理任务，Redis 虽然使用单线程模型处理用户的请求，但是它却使用 I/O 多路复用机制并发处理来自客户端的多个连接，同时等待多个连接发送的请求。 在 I/O 多路复用模型中，最重要的函数调用就是 select 以及类似函数，该方法的能够同时监控多个文件描述符（也就是客户端的连接）的可读可写情况，当其中的某些文件描述符可读或者可写时，select 方法就会返回可读以及可写的文件描述符个数。 使用 I/O 多路复用技术能够极大地减少系统的开销，系统不再需要额外创建和维护进程和线程来监听来自客户端的大量连接，减少了服务器的开发成本和维护成本。 性能瓶颈最后要介绍的其实就是 Redis 选择单线程模型的决定性原因 —— 多线程技术的能够帮助我们充分利用 CPU 的计算资源来并发的执行不同的任务，但是 CPU 资源往往都不是 Redis 服务器的性能瓶颈。哪怕我们在一个普通的 Linux 服务器上启动 Redis 服务，它也能在 1s 的时间内处理 1,000,000 个用户请求。 如果这种吞吐量不能满足我们的需求，更推荐的做法是使用分片的方式将不同的请求交给不同的 Redis 服务器来处理，而不是在同一个 Redis 服务中引入大量的多线程操作。 简单总结一下，Redis 并不是 CPU 密集型的服务，如果不开启 AOF 备份，所有 Redis 的操作都会在内存中完成不会涉及任何的 I/O 操作，这些数据的读写由于只发生在内存中，所以处理速度是非常快的；整个服务的瓶颈在于网络传输带来的延迟和等待客户端的数据传输，也就是网络 I/O，所以使用多线程模型处理全部的外部请求可能不是一个好的方案。 AOF 是 Redis 的一种持久化机制，它会在每次收到来自客户端的写请求时，将其记录到日志中，每次 Redis 服&gt; 务器启动时都会重放 AOF 日志构建原始的数据集，保证数据的持久性。 多线程虽然会帮助我们更充分地利用 CPU 资源，但是操作系统上线程的切换也不是免费的，线程切换其实会带来额外的开销，其中包括： 保存线程 1 的执行上下文； 加载线程 2 的执行上下文； 频繁的对线程的上下文进行切换可能还会导致性能的急剧下降，这可能会导致我们不仅没有提升请求处理的平均速度，反而进行了负优化，所以这也是为什么 Redis 对于使用多线程技术非常谨慎。 引入多线程Redis 在最新的几个版本中加入了一些可以被其他线程异步处理的删除操作，也就是我们在上面提到的 UNLINK、FLUSHALL ASYNC 和 FLUSHDB ASYNC，我们为什么会需要这些删除操作，而它们为什么需要通过多线程的方式异步处理？ 删除操作我们可以在 Redis 在中使用 DEL 命令来删除一个键对应的值，如果待删除的键值对占用了较小的内存空间，那么哪怕是同步地删除这些键值对也不会消耗太多的时间。 但是对于 Redis 中的一些超大键值对，几十 MB 或者几百 MB 的数据并不能在几毫秒的时间内处理完，Redis 可能会需要在释放内存空间上消耗较多的时间，这些操作就会阻塞待处理的任务，影响 Redis 服务处理请求的 PCT99 和可用性。 然而释放内存空间的工作其实可以由后台线程异步进行处理，这也就是 UNLINK 命令的实现原理，它只会将键从元数据中删除，真正的删除操作会在后台异步执行。 总结Redis 选择使用单线程模型处理客户端的请求主要还是因为 CPU 不是 Redis 服务器的瓶颈，所以使用多线程模型带来的性能提升并不能抵消它带来的开发成本和维护成本，系统的性能瓶颈也主要在网络 I/O 操作上；而 Redis 引入多线程操作也是出于性能上的考虑，对于一些大键值对的删除操作，通过多线程非阻塞地释放内存空间也能减少对 Redis 主线程阻塞的时间，提高执行的效率。 原文链接：https://mp.weixin.qq.com/s/3phhw0PTEXUmFkyqQ024kg]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式速查表]]></title>
    <url>%2F2019%2F07%2F17%2Fdocs%2F09-pc-base%2Fregexsc%2F</url>
    <content type="text"><![CDATA[正则表达式速查表 字符 描述 \ 将下一个字符标记为一个特殊字符、或一个原义字符、或一个向后引用、或一个八进制转义符。例如，“n”匹配字符”n”。”\n”匹配一个换行符。串行”\“匹配”&quot;而”(“则匹配”(“。 ^ 匹配输入字符串的开始位置。如果设置了RegExp对象的Multiline属性，^也匹配“\n”或”\r”之后的位置。 $ 匹配输入字符串的结束位置。如果设置了RegExp对象的Multiline属性，$也匹配“\n”或”\r”之前的位置。 * 匹配前面的子表达式零次或多次。例如，zo能匹配“z”以及”zoo”。等价于{0,}。 + 匹配前面的子表达式一次或多次。例如，“zo+”能匹配”zo”以及”zoo”，但不能匹配”z”。+等价于{1,}。 ? 匹配前面的子表达式零次或一次。例如，“do(es)?”可以匹配”does”或”does”中的”do”。?等价于{0,1}。 {n} n是一个非负整数。匹配确定的n次。例如，“o{2}”不能匹配”Bob”中的”o”，但是能匹配”food”中的两个o。 {n,} n是一个非负整数。至少匹配n次。例如，“o{2,}”不能匹配”Bob”中的”o”，但能匹配”foooood”中的所有o。”o{1,}”等价于”o+”。”o{0,}”则等价于”o*”。 {n,m} m和n均为非负整数，其中n&lt;=m。最少匹配n次且最多匹配m次。例如，“o{1,3}”将匹配”fooooood”中的前三个o。”o{0,1}”等价于”o?”。请注意在逗号和两个数之间不能有空格。 ? 当该字符紧跟在任何一个其他限制符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串。例如，对于字符串“oooo”，”o+?”将匹配单个”o”，而”o+”将匹配所有”o”。 . 匹配除“\n”之外的任何单个字符。要匹配包括”\n”在内的任何字符，请使用像”(.|\n)”的模式。 (pattern) 匹配pattern并获取这一匹配。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(“或”)“。 (?:pattern) 匹配pattern但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用或字符“( (?=pattern) 正向肯定预查，在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如，“Windows(?=95 (?!pattern) 正向否定预查，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用。例如“Windows(?!95 (?&lt;=pattern) 反向肯定预查，与正向肯定预查类拟，只是方向相反。例如，“(?&lt;=95 (?&lt;!pattern) 反向否定预查，与正向否定预查类拟，只是方向相反。例如“(?&lt;!95 x|y 匹配x或y。例如，“z [xyz] 字符集合。匹配所包含的任意一个字符。例如，“[abc]”可以匹配”plain”中的”a”。 [^xyz] 负值字符集合。匹配未包含的任意字符。例如，“[^abc]”可以匹配”plain”中的”p”。 [a-z] 字符范围。匹配指定范围内的任意字符。例如，“[a-z]”可以匹配”a”到”z”范围内的任意小写字母字符。 [^a-z] 负值字符范围。匹配任何不在指定范围内的任意字符。例如，“[^a-z]”可以匹配任何不在”a”到”z”范围内的任意字符。 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如，“er\b”可以匹配”never”中的”er”，但不能匹配”verb”中的”er”。 \B 匹配非单词边界。“er\B”能匹配”verb”中的”er”，但不能匹配”never”中的”er”。 \cx 匹配由x指明的控制字符。例如，\cM匹配一个Control-M或回车符。x的值必须为A-Z或a-z之一。否则，将c视为一个原义的“c”字符。 \d 匹配一个数字字符。等价于[0-9]。 \D 匹配一个非数字字符。等价于[^0-9]。 \f 匹配一个换页符。等价于\x0c和\cL。 \n 匹配一个换行符。等价于\x0a和\cJ。 \r 匹配一个回车符。等价于\x0d和\cM。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于[ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于[^ \f\n\r\t\v]。 \t 匹配一个制表符。等价于\x09和\cI。 \v 匹配一个垂直制表符。等价于\x0b和\cK。 \w 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。 \W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]”。 \xn 匹配n，其中n为十六进制转义值。十六进制转义值必须为确定的两个数字长。例如，“\x41”匹配”A”。”\x041”则等价于”\x04&amp;1”。正则表达式中可以使用ASCII编码。. \num 匹配num，其中num是一个正整数。对所获取的匹配的引用。例如，“(.)\1”匹配两个连续的相同字符。 \n 标识一个八进制转义值或一个向后引用。如果\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值。 \nm 标识一个八进制转义值或一个向后引用。如果\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\nm将匹配八进制转义值nm。 \nml 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml。 \un 匹配n，其中n是一个用四个十六进制数字表示的Unicode字符。例如，\u00A9匹配版权符号（©）。 常用正则表达式校验数字的表达式12345678910111213141516171819 1 数字：^[0-9]*$ 2 n位的数字：^\d&#123;n&#125;$ 3 至少n位的数字：^\d&#123;n,&#125;$ 4 m-n位的数字：^\d&#123;m,n&#125;$ 5 零和非零开头的数字：^(0|[1-9][0-9]*)$ 6 非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(.[0-9]&#123;1,2&#125;)?$ 7 带1-2位小数的正数或负数：^(\-)?\d+(\.\d&#123;1,2&#125;)?$ 8 正数、负数、和小数：^(\-|\+)?\d+(\.\d+)?$ 9 有两位小数的正实数：^[0-9]+(.[0-9]&#123;2&#125;)?$10 有1~3位小数的正实数：^[0-9]+(.[0-9]&#123;1,3&#125;)?$11 非零的正整数：^[1-9]\d*$ 或 ^([1-9][0-9]*)&#123;1,3&#125;$ 或 ^\+?[1-9][0-9]*$12 非零的负整数：^\-[1-9][]0-9&quot;*$ 或 ^-[1-9]\d*$13 非负整数：^\d+$ 或 ^[1-9]\d*|0$14 非正整数：^-[1-9]\d*|0$ 或 ^((-\d+)|(0+))$15 非负浮点数：^\d+(\.\d+)?$ 或 ^[1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0$16 非正浮点数：^((-\d+(\.\d+)?)|(0+(\.0+)?))$ 或 ^(-([1-9]\d*\.\d*|0\.\d*[1-9]\d*))|0?\.0+|0$17 正浮点数：^[1-9]\d*\.\d*|0\.\d*[1-9]\d*$ 或 ^(([0-9]+\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\.[0-9]+)|([0-9]*[1-9][0-9]*))$18 负浮点数：^-([1-9]\d*\.\d*|0\.\d*[1-9]\d*)$ 或 ^(-(([0-9]+\.[0-9]*[1-9][0-9]*)|([0-9]*[1-9][0-9]*\.[0-9]+)|([0-9]*[1-9][0-9]*)))$19 浮点数：^(-?\d+)(\.\d+)?$ 或 ^-?([1-9]\d*\.\d*|0\.\d*[1-9]\d*|0?\.0+|0)$ 校验字符的表达式123456789101112 1 汉字：^[\u4e00-\u9fa5]&#123;0,&#125;$ 2 英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]&#123;4,40&#125;$ 3 长度为3-20的所有字符：^.&#123;3,20&#125;$ 4 由26个英文字母组成的字符串：^[A-Za-z]+$ 5 由26个大写英文字母组成的字符串：^[A-Z]+$ 6 由26个小写英文字母组成的字符串：^[a-z]+$ 7 由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$ 8 由数字、26个英文字母或者下划线组成的字符串：^\w+$ 或 ^\w&#123;3,20&#125;$ 9 中文、英文、数字包括下划线：^[\u4E00-\u9FA5A-Za-z0-9_]+$10 中文、英文、数字但不包括下划线等符号：^[\u4E00-\u9FA5A-Za-z0-9]+$ 或 ^[\u4E00-\u9FA5A-Za-z0-9]&#123;2,20&#125;$11 可以输入含有^%&amp;&apos;,;=?$\&quot;等字符：[^%&amp;&apos;,;=?$\x22]+12 禁止输入含有~的字符：[^~\x22]+ 特殊需求表达式123456789101112131415161718192021222324252627282930313233 1 Email地址：^\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*$ 2 域名：[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;(/.[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;)+/.? 3 InternetURL：[a-zA-z]+://[^\s]* 或 ^http://([\w-]+\.)+[\w-]+(/[\w-./?%&amp;=]*)?$ 4 手机号码：^(13[0-9]|14[0-9]|15[0-9]|16[0-9]|17[0-9]|18[0-9]|19[0-9])\d&#123;8&#125;$ (由于工信部放号段不定时，所以建议使用泛解析 ^([1][3,4,5,6,7,8,9])\d&#123;9&#125;$) 5 电话号码(&quot;XXX-XXXXXXX&quot;、&quot;XXXX-XXXXXXXX&quot;、&quot;XXX-XXXXXXX&quot;、&quot;XXX-XXXXXXXX&quot;、&quot;XXXXXXX&quot;和&quot;XXXXXXXX)：^(\(\d&#123;3,4&#125;-)|\d&#123;3.4&#125;-)?\d&#123;7,8&#125;$ 6 国内电话号码(0511-4405222、021-87888822)：\d&#123;3&#125;-\d&#123;8&#125;|\d&#123;4&#125;-\d&#123;7&#125; 7 18位身份证号码(数字、字母x结尾)：^((\d&#123;18&#125;)|([0-9x]&#123;18&#125;)|([0-9X]&#123;18&#125;))$ 8 帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]&#123;4,15&#125;$ 9 密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\w&#123;5,17&#125;$10 强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间)：^(?=.*\d)(?=.*[a-z])(?=.*[A-Z]).&#123;8,10&#125;$ 11 日期格式：^\d&#123;4&#125;-\d&#123;1,2&#125;-\d&#123;1,2&#125;12 一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$13 一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$ 14 钱的输入格式：15 1.有四种钱的表示形式我们可以接受:&quot;10000.00&quot; 和 &quot;10,000.00&quot;, 和没有 &quot;分&quot; 的 &quot;10000&quot; 和 &quot;10,000&quot;：^[1-9][0-9]*$ 16 2.这表示任意一个不以0开头的数字,但是,这也意味着一个字符&quot;0&quot;不通过,所以我们采用下面的形式：^(0|[1-9][0-9]*)$ 17 3.一个0或者一个不以0开头的数字.我们还可以允许开头有一个负号：^(0|-?[1-9][0-9]*)$ 18 4.这表示一个0或者一个可能为负的开头不为0的数字.让用户以0开头好了.把负号的也去掉,因为钱总不能是负的吧.下面我们要加的是说明可能的小数部分：^[0-9]+(.[0-9]+)?$ 19 5.必须说明的是,小数点后面至少应该有1位数,所以&quot;10.&quot;是不通过的,但是 &quot;10&quot; 和 &quot;10.2&quot; 是通过的：^[0-9]+(.[0-9]&#123;2&#125;)?$ 20 6.这样我们规定小数点后面必须有两位,如果你认为太苛刻了,可以这样：^[0-9]+(.[0-9]&#123;1,2&#125;)?$ 21 7.这样就允许用户只写一位小数.下面我们该考虑数字中的逗号了,我们可以这样：^[0-9]&#123;1,3&#125;(,[0-9]&#123;3&#125;)*(.[0-9]&#123;1,2&#125;)?$ 22 8.1到3个数字,后面跟着任意个 逗号+3个数字,逗号成为可选,而不是必须：^([0-9]+|[0-9]&#123;1,3&#125;(,[0-9]&#123;3&#125;)*)(.[0-9]&#123;1,2&#125;)?$ 23 备注：这就是最终结果了,别忘了&quot;+&quot;可以用&quot;*&quot;替代如果你觉得空字符串也可以接受的话(奇怪,为什么?)最后,别忘了在用函数时去掉去掉那个反斜杠,一般的错误都在这里24 xml文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\.[x|X][m|M][l|L]$25 中文字符的正则表达式：[\u4e00-\u9fa5]26 双字节字符：[^\x00-\xff] (包括汉字在内，可以用来计算字符串的长度(一个双字节字符长度计2，ASCII字符计1))27 空白行的正则表达式：\n\s*\r (可以用来删除空白行)28 HTML标记的正则表达式：&lt;(\S*?)[^&gt;]*&gt;.*?&lt;/\1&gt;|&lt;.*? /&gt; (网上流传的版本太糟糕，上面这个也仅仅能部分，对于复杂的嵌套标记依旧无能为力)29 首尾空白字符的正则表达式：^\s*|\s*$或(^\s*)|(\s*$) (可以用来删除行首行尾的空白字符(包括空格、制表符、换页符等等)，非常有用的表达式)30 腾讯QQ号：[1-9][0-9]&#123;4,&#125; (腾讯QQ号从10000开始)31 中国邮政编码：[1-9]\d&#123;5&#125;(?!\d) (中国邮政编码为6位数字)32 IP地址：\d+\.\d+\.\d+\.\d+ (提取IP地址时有用)33 IP地址：((?:(?:25[0-5]|2[0-4]\\d|[01]?\\d?\\d)\\.)&#123;3&#125;(?:25[0-5]|2[0-4]\\d|[01]?\\d?\\d)) 在线工具http://tool.php.cn/regexhttps://c.runoob.com/front-end/854]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Laravel Facades 原理解析]]></title>
    <url>%2F2019%2F05%2F29%2Fdocs%2F03-php%2Flara-facades%2F</url>
    <content type="text"><![CDATA[简介Facades 为应用的 服务容器 提供了一个「静态」 接口。Laravel 自带了很多 Facades，可以访问绝大部分功能。Laravel Facades 实际是服务容器中底层类的 「静态代理」 ，相对于传统静态方法，在使用时能够提供更加灵活、更加易于测试、更加优雅的语法。 所有的 Laravel Facades 都定义在 Illuminate\Support\Facades 命名空间下。所以，我们可以轻松的使用 Facade ： 12345use Illuminate\Support\Facades\Cache;Route::get('/cache', function () &#123; return Cache::get('key');&#125;); 简化调用的列子我们使用一个 Laravel 中的例子，来说明一下 Facade 是如何简化调用的。我们需要调用设置数据缓存的方法，使用 Facade 的语法如下： 12345678910use Illuminate\Support\Facades\Cache;Route::get(&apos;/cache&apos;, function() &#123; // 将缓存存入 // Cache::put(&apos;key&apos;, &apos;test&apos;, 10); // 获取缓存项key的内容 return Cache::get(&apos;key&apos;);&#125; 如果不使用 Facade 来调用，那么调用的语法如下： 1234Route::get(&apos;/cache&apos;, function() &#123; // 获取缓存项key的内容 return app(&apos;cache&apos;)-&gt;get(&apos;key&apos;);&#125; 语法过程就是先从服务容器中解析出来缓存对象，再利用缓存对象将缓存项提取。对比这两种使用方式，第一种显而易见，更加直观，简单一些。这就是 Facade 的主要目的。 Facade 的启动与注册Facade 的启动引导是在Illuminate\Foundation\Bootstrap\RegisterFacades中注册的。 1234567891011121314151617181920212223242526272829&lt;?phpnamespace Illuminate\Foundation\Bootstrap;use Illuminate\Foundation\AliasLoader;use Illuminate\Support\Facades\Facade;use Illuminate\Foundation\PackageManifest;use Illuminate\Contracts\Foundation\Application;class RegisterFacades&#123; /** * Bootstrap the given application. * * @param \Illuminate\Contracts\Foundation\Application $app * @return void */ public function bootstrap(Application $app) &#123; Facade::clearResolvedInstances(); Facade::setFacadeApplication($app); AliasLoader::getInstance(array_merge( $app-&gt;make('config')-&gt;get('app.aliases', []), $app-&gt;make(PackageManifest::class)-&gt;aliases() ))-&gt;register(); &#125;&#125; AliasLoader 维护了一个 Facade 别名以及对应的 Facade 的数组，也就是 config\app.php 中 aliases 数组，类似于： 123456&apos;App&apos; =&gt; Illuminate\Support\Facades\App::class,&apos;Arr&apos; =&gt; Illuminate\Support\Arr::class,&apos;Artisan&apos; =&gt; Illuminate\Support\Facades\Artisan::class,&apos;Auth&apos; =&gt; Illuminate\Support\Facades\Auth::class,&apos;Blade&apos; =&gt; Illuminate\Support\Facades\Blade::class,... AliasLoader 的 register 方法注册了 AliasLoader 的 load 方法到 SPL __autoload 函数队列中。 1spl_autoload_register([$this, &apos;load&apos;], true, true); AliasLoader 的 load 方法通过 class_alias 函数，为实际的 Facade 创建一个 Facade 别名。 123456789101112131415161718/** * Load a class alias if it is registered. * * @param string $alias * @return bool|null */public function load($alias)&#123; if (static::$facadeNamespace &amp;&amp; strpos($alias, static::$facadeNamespace) === 0) &#123; $this-&gt;loadFacade($alias); return true; &#125; if (isset($this-&gt;aliases[$alias])) &#123; return class_alias($this-&gt;aliases[$alias], $alias); &#125;&#125; 所以，看到这里就很明白了： Facade 别名是通过 class_alias 创建，使用 spl_autoload_register 函数让别名实现自动加载 如何实现的呢？假如我们自定义了一个类并且放入容器中后，该如何以 Facade 的方式来调用呢？ 1234567891011121314use Illuminate\Support\Facades\Facade;class Cache extends Facade&#123; /** * Get the registered name of the component. * * @return string */ protected static function getFacadeAccessor() &#123; return 'cache'; &#125;&#125; 只要定义一个类让他继承自 Illuminate\Support\Facades\Facade，并且实现一个抽象方法 getFacadeAccessor Facade 类中包含__callStatic 这个方法： 12345678910111213141516171819/** * Handle dynamic, static calls to the object. * * @param string $method * @param array $args * @return mixed * * @throws \RuntimeException */public static function __callStatic($method, $args)&#123; $instance = static::getFacadeRoot(); if (! $instance) &#123; throw new RuntimeException('A facade root has not been set.'); &#125; return $instance-&gt;$method(...$args);&#125; 这是 PHP 中的一个魔术方法，当以静态的方式调用一个不存在的方法时，该方法会被调用。代码很简单，就是解析实例调用方法。不过这里要注意一个就是这里使用的是 static 关键字而不是 self 关键字，这涉及到一个后期的静态绑定， 再来看看 getFacadeRoot 的代码，这个方法就是从容器中解析出对象： 12345678910111213141516171819202122232425262728293031323334353637383940/** * Get the root object behind the facade. * * @return mixed */public static function getFacadeRoot()&#123; return static::resolveFacadeInstance(static::getFacadeAccessor());&#125;/** * Get the registered name of the component. * * @return string * * @throws \RuntimeException */protected static function getFacadeAccessor()&#123; throw new RuntimeException('Facade does not implement getFacadeAccessor method.');&#125;/** * Resolve the facade root instance from the container. * * @param object|string $name * @return mixed */protected static function resolveFacadeInstance($name)&#123; if (is_object($name)) &#123; return $name; &#125; if (isset(static::$resolvedInstance[$name])) &#123; return static::$resolvedInstance[$name]; &#125; return static::$resolvedInstance[$name] = static::$app[$name];&#125; 其中的 getFacadeAccessor 这个方法必须被重写，否者就会抛出异常。然后在 resolveFacadeInstance 这个方法中会先判断是否是一个对象，如果是的话就直接返回。所以上文说的 getFacadeAccessor 这个方法直接返回一个对象也是可以的，奥秘就在这。然后会去判断需要解析的对象是否已经解析过了，如果解析过了就直接返回，否则会从容器中去解析再返回，这样不仅仅实现了单例，而且还可以提升性能。得到对象后，就是直接通过对象来调用方法了： 1$instance-&gt;$method(...$args); // 调用方法 如何快速找到 Facades 中对应的类Facade 基类提供了一个getFacadeRoot的方法用来获取该facade对应在ioc里的实例，同时get_class又可获取对应的类名。即可。这个方法只要返回一个字符串，就是返回服务容器绑定类的别名。其实，通过源码可以知道，对象不一定要放到容器中，可以直接在这里返回也是可以的. 1234567891011121314use Illuminate\Support\Facades\Facade;use Cache;class Cache extends Facade&#123; /** * Get the registered name of the component. * * @return string */ protected static function getFacadeAccessor() &#123; return new Cache; &#125;&#125; 例： 12345&lt;?phpRoute::get('/test', function () &#123; dd(get_class(Route::getFacadeRoot()));&#125;); 输出： 1&quot;Illuminate\Routing\Router&quot; 如果想打印其它的类，把 Route 换成那个类即可。 参考文档 https://learnku.com/docs/laravel/5.8/facades/3888]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Laravel</tag>
        <tag>Facades</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 应用场景]]></title>
    <url>%2F2019%2F04%2F21%2Fdocs%2F07-redis%2Fredis-use-scenario%2F</url>
    <content type="text"><![CDATA[缓存及过期键处理及键淘汰策略缓存现在几乎是所有中大型网站都在用的必杀技，合理的利用缓存不仅能够提升网站访问速度，还能大大降低数据库的压力。Redis提供了键过期功能，也提供了灵活的键淘汰策略，所以，现在Redis用在缓存的场合非常多。 在redis中可以通过如下命令设置键的生存时间或生命周期： EXPIRE KEY TTL 命令用于将key的生存时间设置为ttl秒 PEXPIRE KEY TTL 命令用于将key的生存时间设置为ttl毫秒 EXPIREAT KEY TIMESTAMP 命令用于将key的过期时间设置为timestamp所指定的秒数时间戳 PEXPIREAT KEY TIMESTAMP 命令用于将key的过期时间设置为timestamp所指定的毫秒时间戳 PERSIST KEY 移除key的过期时间 Redis是如何判断一个键过期的呢？ 在redis中维护了一个expires字典，里面保存了数据库中所有设置了过期时间的键的过期时间，称为过期字典。我们可以用ttl（time to live）命令去查看key的剩余生存秒数，也可以用pttl查看key的剩余生存毫秒数，过程即是拿着key去expires字典中获取到key的过期毫秒时间戳，再减去当前时间戳，即可得到key的剩余生存时间。 而判断key是否过期，也是通过过期字典来完成的： 首先检查给定键是否存在于过期字典中，若存在则取得键的过期时间 检查当前UNIX时间戳是否大于键的过期时间，如果是的话则键已过期，若否则未过期 Redis的过期键删除策略： 如果一个键过期了那肯定是需要删除的，否则留在仍然留在内存中会导致取到过期数据，同时也浪费内存，那它什么时候会被删除呢？有两种策略： 惰性删除策略：程序在取出键时才对key进行过期检查，若过期则删除，否则照常执行，这个策略对cpu是友好的，因为不用额外的线程去自动清理过期key，但是是对内存不友好的，因为如果一直没有对这些过期键进行获取的话，这些键会一直留在内存中，造成垃圾数据内存泄漏 定期删除策略：每隔一段时间执行一次删除过期键的操作，并通过限制删除操作执行的时常和频率来减少删除操作对cpu时间的影响 Redis综合了这两种策略来实现过期键的删除： 首先所有读写数据库的redis命令在执行之前都会对输入键通过过期字典进行检查，如果已过期则将key删除，然后再执行请求命令，该返回空返回空，该set值set值； redis还实现了定期删除策略，在规定的时间内，分多次遍历服务器的各个数据库，从数据库的expires字典中随机抽查一部分键的过期时间，并删除其中过期键 通过两种策略的结合，redis实现了过期键删除的时间与空间的平衡。 内存不足时Redis的键淘汰策 redis在32位系统有最大内存限制3G，但是在64位系统并未有限制，因此我们需要在redis.conf文件中设置其最大占用内存值，否则将无限制使用内存甚至把服务器撑爆。设置方法如下： maxmemory 100mb 当内存达到设置的最大值之后，再次进行写操作，redi会首先根据配置的键淘汰策略尝试淘汰数据，释放空间；若是未设置淘汰策略或是根据淘汰策略仍不能清理出空间，则拒绝写操作，但是读操作并不影响。 因此，我们在设置了最大使用内存后还得配置合适的键淘汰策略，以使redis服务更高可用。根据redis.conf注释我们知道键淘汰策略有如下几种： volatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key allkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰 volatile-random：随机淘汰数据，只淘汰设定了有效期的key allkeys-random：随机淘汰数据，所有的key都可以被淘汰 volatile-ttl：淘汰剩余有效期最短的key redis默认的淘汰策略为noeviction，即不淘汰，显然是不行的，我们需要根据不同的应用场景配置不同的淘汰策略。推荐使用volatile-lru，对于很重要的、更新又不频繁的数据不应该设置过期时间，而对于更新频率较高，或是对一致性要求较高的数据可以设置过期时间，然后通过volatile-lru，对于这些设置了过期时间的键值，通过least-recently-used最近不常使用的规则进行删除，保留热点数据，提高缓存命中率。 需要注意的是，如果是redis集群配置了主从复制的话，maxmemory不能设置得跟服务器主机内存太接近，因为主从复制将占用一部分内存，最好保证一定的内存余量。 排行榜很多网站都有排行榜应用的，如京东的月度销量榜单、商品按时间的上新排行榜等。Redis提供的有序集合数据类构能实现各种复杂的排行榜应用。 Redis 有序集合和集合一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 有序集合的成员是唯一的,但分数(score)却可以重复。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员)。 示例 一个典型的游戏排行榜包括以下常见功能： 能够记录每个玩家的分数； 能够对玩家的分数进行更新； 能够查询每个玩家的分数和名次； 能够按名次查询排名前N名的玩家； 能够查询排在指定玩家前后M名的玩家。 更进一步，上面的操作都需要在短时间内实时完成，这样才能最大程度发挥排行榜的效用。 由于一个玩家名次上升x位将会引起x+1位玩家的名次发生变化（包括该玩家），如果采用传统数据库（比如MySQL）来实现排行榜，当玩家人数较多时，将会导致对数据库的频繁修改，性能得不到满足，所以我们只能另想它法。 假设lb为排行榜名称，user1、user2等为玩家唯一标识。 zadd —— 设置玩家分数命令格式：zadd 排行榜名称 分数 玩家标识时间复杂度：O(log(N)) 12345678127.0.0.1:6379&gt; zadd lb 79 user1(integer) 1127.0.0.1:6379&gt; zadd lb 83 user2(integer) 1127.0.0.1:6379&gt; zadd lb 97 user3(integer) 1127.0.0.1:6379&gt; zadd lb 90 user4(integer) 1 zscore —— 查看玩家分数命令格式：zscore 排行榜名称 玩家标识时间复杂度：O(1) 12127.0.0.1:6379&gt; zscore lb user1&quot;79&quot; zrevrange —— 按名次查看排行榜命令格式：zrevrange 排行榜名称 起始位置 结束位置 [withscores]时间复杂度：O(log(N)+M) 由于排行榜一般是按照分数由高到低排序的，所以我们使用zrevrange，而命令zrange是按照分数由低到高排序。 起始位置和结束位置都是以0开始的索引，且都包含在内。如果结束位置为-1则查看范围为整个排行榜。 带上withscores则会返回玩家分数。 123456789101112131415161718192021127.0.0.1:6379&gt; zrevrange lb 0 -1 withscores1) &quot;user3&quot;2) &quot;97&quot;3) &quot;user4&quot;4) &quot;90&quot;5) &quot;user2&quot;6) &quot;83&quot;7) &quot;user1&quot;8) &quot;79&quot;127.0.0.1:6379&gt; zrevrange lb 0 2 withscores1) &quot;user3&quot;2) &quot;97&quot;3) &quot;user4&quot;4) &quot;90&quot;5) &quot;user2&quot;6) &quot;83&quot;127.0.0.1:6379&gt; zrevrange lb 0 21) &quot;user3&quot;2) &quot;user4&quot;3) &quot;user2&quot;127.0.0.1:6379&gt; zrevrank —— 查看玩家排名命令格式：zrevrank 排行榜名称 玩家标识时间复杂度：O(log(N)) 与zrevrange类似，zrevrank是以分数由高到低的排序返回玩家排名（实际返回的是以0开始的索引），对应的zrank则是以分数由低到高的排序返回排名。 1234127.0.0.1:6379&gt; zrevrank lb user3(integer) 0127.0.0.1:6379&gt; zrevrank lb user1(integer) 3 zincrby —— 增减玩家分数命令格式：zincrby 排行榜名称 分数增量 玩家标识时间复杂度：O(log(N)) 有的排行榜是在变更时重新设置玩家的分数，而还有的排行榜则是以增量方式修改玩家分数，增量可正可负。如果执行zincrby时玩家尚不在排行榜中，则认为其原始分数为0，相当于执行zdd。 1234567891011127.0.0.1:6379&gt; zincrby lb 10 user4&quot;100&quot;127.0.0.1:6379&gt; zrevrange lb 0 -1 withscores1) &quot;user4&quot;2) &quot;100&quot;3) &quot;user3&quot;4) &quot;97&quot;5) &quot;user2&quot;6) &quot;83&quot;7) &quot;user1&quot;8) &quot;79&quot; zrem —— 移除某个玩家命令格式：zrem 排行榜名称 玩家标识时间复杂度：O(log(N)) 123456789127.0.0.1:6379&gt; zrem lb user4(integer) 1127.0.0.1:6379&gt; zrevrange lb 0 -1 withscores1) &quot;user3&quot;2) &quot;97&quot;3) &quot;user2&quot;4) &quot;83&quot;5) &quot;user1&quot;6) &quot;79&quot; del —— 删除排行榜名称排行榜对象在我们首次调用zadd或zincrby时被创建，当我们要删除它时，调用redis通用的命令del即可。 1234567127.0.0.1:6379&gt; del lb(integer) 1127.0.0.1:6379&gt; get lb(nil)127.0.0.1:6379&gt; zrevrange lb 0 -1 withscores(empty list or set)127.0.0.1:6379&gt; 相同分数问题免费的方案总有那么一些不完美。从前面的例子我们可以看到，user2和user3具有相同的分数，但在按分数逆序排序时，user3排在了user2前面。而在实际应用场景中，我们更希望看到user2排在user3前面，因为user2比user3先加入排行榜，也就是说user2先到达该分数。 但Redis在遇到分数相同时是按照集合成员自身的字典顺序来排序，这里即是按照”user2″和”user3″这两个字符串进行排序，以逆序排序的话user3自然排到了前面。 要解决这个问题，我们可以考虑在分数中加入时间戳，计算公式为： 带时间戳的分数 = 实际分数*10000000000 + (9999999999 – timestamp) timestamp我们采用系统提供的time()函数，也就是1970年1月1日以来的秒数，我们采用32位的时间戳（这能坚持到2038年），由于32位时间戳是10位十进制整数（最大值4294967295），所以我们让时间戳占据低10位（十进制整数），实际分数则扩大10^10倍，然后把两部分相加的结果作为zset的分数。考虑到要按时间倒序排列，所以时间戳这部分需要颠倒一下，这便是用9999999999减去时间戳的原因。当我们要读取玩家实际分数时，只需去掉后10位即可。 初步看起来这个方案还不错，但这里面有两个问题。 第一个问题是小问题，采用秒为时间戳可能区分度还不够，如果同一秒出现两个分数相同的仍然会出现前面的问题，当然我们可以选择精度更高的时间戳，但在实际场景中，同一秒谁排前面已经无关紧要。 第二个问题是大问题，因为Redis的分数类型采用的是double，64位双精度浮点数只有52位有效数字，它能精确表达的整数范围为-253到253，最高只能表示16位十进制整数（最大值为9007199254740992，其实连16位也不能完整表示）。这就是说，如果前面时间戳占了10位的话，分数就只剩下6位了，这对于某些排行榜分数来说是不够用的。我们可以考虑缩减时间戳位数，比如从2015年1月1日开始计时，但这仍然增加不了几位。或者减少区分度，以分钟、小时来作为时间戳单位。 如果Redis的分数类型为int64，我们就没有上面的烦恼。说到这里，其实Redis真应该再额外提供一个int64类型的ZSet，但目前只能是幻想，除非自己改其源码。 既然Redis也不能完美解决排行榜问题，那最终是不是有必要自己实现一个专门的排行榜数据结构呢？毕竟实际应用中的排行榜有很多可以优化的地方，比玩家呈金字塔分布，越是低分段玩家数量越多，同一分数拥有大量玩家，玩家增加一分都可能超越很多玩家，这就为优化提供了可能。 计数器什么是计数器，如电商网站商品的浏览量、视频网站视频的播放数等。为了保证数据实时效，每次浏览都得给+1，并发量高时如果每次都请求数据库操作无疑是种挑战和压力。Redis提供的incr命令来实现计数器功能，内存操作，性能非常好，非常适用于这些计数场景。 INCR key 将 key 中储存的数字值增一。 如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCR 操作。 如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。 这是一个针对字符串的操作，因为 Redis 没有专用的整数类型，所以 key 内储存的字符串被解释为十进制 64 位有符号整数来执行 INCR 操作。 12345127.0.0.1:6379&gt; incr test(integer) 11127.0.0.1:6379&gt; get test&quot;11&quot;127.0.0.1:6379&gt; 计数器的实现计数器是 Redis 的原子性自增操作可实现的最直观的模式了，它的想法相当简单：每当某个操作发生时，向 Redis 发送一个 INCR 命令。 比如在一个 web 应用程序中，如果想知道用户在一年中每天的点击量，那么只要将用户 ID 以及相关的日期信息作为键，并在每次用户点击页面时，执行一次自增操作即可。 比如用户名是 peter ，点击时间是 2012 年 3 月 22 日，那么执行命令 INCR peter::2012.3.22 。 以下是防止刷单的逻辑示例： 123456789101112$redisKey = “api_name_” + $api;$count = $this-&gt;redis-&gt;incr($redisKey);if ($count == 1) &#123; //设置有效期一s $this-&gt;redis-&gt;expire($redisKey,1);// 设置一s的过期时间&#125;if (count &gt; 200) &#123;// 防止刷单的安全拦截 return false;// 超过就返回false&#125; 聚合分类redis set是集合类型的数据结构，那么集合类型就比较适合用于聚合分类。 标签：比如我们博客网站常常使用到的兴趣标签，把一个个有着相同爱好，关注类似内容的用户利用一个标签把他们进行归并。 共同好友功能，共同喜好，或者可以引申到二度好友之类的扩展应用。 统计网站的独立IP。利用set集合当中元素不唯一性，可以快速实时统计访问网站的独立IP。 案例： 在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 交集，并集，差集 //tag表使用集合来存储数据，因为集合擅长求交集、并集 123456789101112131415127.0.0.1:6379&gt; sadd tag:ruby 1(integer) 1127.0.0.1:6379&gt; sadd tag:ruby 2(integer) 1127.0.0.1:6379&gt; sadd tag:web 2(integer) 1127.0.0.1:6379&gt; sadd tag:erlang 3(integer) 1127.0.0.1:6379&gt; SDIFF tag:ruby tag:web1) &quot;1&quot;127.0.0.1:6379&gt; sinter tag:ruby tag:web1) &quot;2&quot;127.0.0.1:6379&gt; sunion tag:ruby tag:web1) &quot;1&quot;2) &quot;2&quot; 秒杀Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边） 1234567891011121314127.0.0.1:6379&gt; LPUSH testkey redis 1(integer) 2127.0.0.1:6379&gt; LPUSH testkey mongodb 2(integer) 4127.0.0.1:6379&gt; LPUSH testkey mysql 3(integer) 6127.0.0.1:6379&gt; LRANGE testkey 0 101) &quot;3&quot;2) &quot;mysql&quot;3) &quot;2&quot;4) &quot;mongodb&quot;5) &quot;1&quot;6) &quot;redis&quot;127.0.0.1:6379&gt; 秒杀场景防止商品超卖： 数据库中设置商品数量为无符号型，即不允许负数。当更新商品数量到负数时，返回false。 商品数量存在Redis的list队列中，每次抢购就pop删除一个元素出队列。 12345678//存放商品数量的队列for($j =1; $j &lt;= 10; $j++)&#123; //设置商品数量为10 $re = Redis::lpush(gooods_count,1);&#125;// 判断商品数量逻辑$count = Redis::lpop(&apos;gooods_count&apos;);//$count = Redis::llen(&apos;gooods_count&apos;); //llen判断队列长度if(!$count)&#123;return&apos;已经抢光了哦&apos;;]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Docker 搭建 Redis 主从集群哨兵监控]]></title>
    <url>%2F2019%2F04%2F14%2Fdocs%2F07-redis%2Fredis-cluster%2F</url>
    <content type="text"><![CDATA[简介本文介绍如何使用Docker搭建Redis主从集群，并且通过哨兵监控集群状态，当主发生故障时，自动从备中选举出新的主。 环境准备 拉取redis镜像 12345$ docker pull redis:4.0.94.0.9: Pulling from library/redisDigest: sha256:87275ecd3017cdacd3e93eaf07e26f4a91d7f4d7c311b2305fccb50ec3a1a8cdStatus: Image is up to date for redis:4.0.9docker.io/library/redis:4.0.9 运行容器 1$ docker run -it redis:4.0.9 /bin/bash 进入容器安装依赖 12root@ef17fbe578b3:/data# apt-get updateroot@ef17fbe578b3:/data# apt-get install vim // 后面需要编辑配置文件所以需要安装vim 将容器新增的内容重新提交 1$ docker commit ef17fbe578b3 axkeson/redis:v1 用新提交的镜像，启动3个Redis容器服务，分别使用到6381、6382、6383端口 123$ docker run --name redis-6381 -p 6381:6379 -d axkeson/redis:v1 redis-server$ docker run --name redis-6382 -p 6382:6379 -d axkeson/redis:v1 redis-server$ docker run --name redis-6383 -p 6383:6379 -d axkeson/redis:v1 redis-server 查看容器 12345$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES632fb3a1b3bc axkeson/redis:v1 "docker-entrypoint.s…" 37 seconds ago Up 36 seconds 0.0.0.0:6383-&gt;6379/tcp redis-63831f79a2be5423 axkeson/redis:v1 "docker-entrypoint.s…" 45 seconds ago Up 44 seconds 0.0.0.0:6382-&gt;6379/tcp redis-638276aedad9e6f6 axkeson/redis:v1 "docker-entrypoint.s…" 54 seconds ago Up 54 seconds 0.0.0.0:6381-&gt;6379/tcp redis-6381 测试容器 123456$ docker exec -it redis-6381 redis-cli127.0.0.1:6379&gt; set test abcOK127.0.0.1:6379&gt; get test"abc"127.0.0.1:6379&gt; Redis集群配置 使用命令docker inspect redis-6381查看容器IP地址，分别为 123redis-6381：172.17.0.2:6379redis-6382：172.17.0.3:6379redis-6383：172.17.0.4:6379 进入docker容器内部，查看当前redis角色（主还是从） 123456789101112$ docker exec -it redis-6381 redis-cli# Replicationrole:masterconnected_slaves:0master_replid:f19667ee9ed54554c807a8b0e350b2025d6a6a63master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 123456789101112$ docker exec -it redis-6382 redis-cli# Replicationrole:masterconnected_slaves:0master_replid:f19667ee9ed54554c807a8b0e350b2025d6a6a63master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 123456789101112$ docker exec -it redis-6383 redis-cli# Replicationrole:masterconnected_slaves:0master_replid:f19667ee9ed54554c807a8b0e350b2025d6a6a63master_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 目前三个都是master状态 使用redis-cli命令修改redis-6382、redis-6383的主机为172.17.0.2:6379 12345678$ docker exec -it redis-6382 redis-cli127.0.0.1:6379&gt; SLAVEOF 172.17.0.2 6379OK127.0.0.1:6379&gt; quit$ docker exec -it redis-6383 redis-cli127.0.0.1:6379&gt; SLAVEOF 172.17.0.2 6379OK127.0.0.1:6379&gt; quit 查看redis-6381是否已经拥有2个从机，connected_slaves:2 1234567891011121314$ docker exec -it redis-6381 redis-cli# Replicationrole:masterconnected_slaves:2slave0:ip=172.17.0.3,port=6379,state=online,offset=126,lag=0slave1:ip=172.17.0.4,port=6379,state=online,offset=126,lag=0master_replid:1bfa59f9852a17012e9cee2db7311d603acac546master_replid2:0000000000000000000000000000000000000000master_repl_offset:126second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:126 配置Sentinel 进入3台redis容器内部进行配置，在容器根目录里面创建sentinel.conf文件文件内容为：sentinel monitor mymaster 172.17.0.2 6379 1 123$ docker exec -it redis-6381 /bin/bashroot@76aedad9e6f6:/data# cd / &amp;&amp; touch sentinel.confroot@76aedad9e6f6:/# vim /sentinel.conf 启动Redis哨兵 123456789101112131415161718192021222324252627root@76aedad9e6f6:/# redis-sentinel /sentinel.conf53:X 13 Nov 13:54:25.504 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo53:X 13 Nov 13:54:25.505 # Redis version=4.0.9, bits=64, commit=00000000, modified=0, pid=53, just started53:X 13 Nov 13:54:25.507 # Configuration loaded _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 4.0.9 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ ''-._ ( ' , .-` | `, ) Running in sentinel mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 26379 | `-._ `._ / _.-' | PID: 53 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-'53:X 13 Nov 13:54:25.516 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.53:X 13 Nov 13:54:25.521 # Sentinel ID is f9ab086ca7845053fa2a16817a65c70e15ced3c853:X 13 Nov 13:54:25.522 # +monitor master mymaster 172.17.0.2 6379 quorum 153:X 13 Nov 13:54:25.524 * +slave slave 172.17.0.3:6379 172.17.0.3 6379 @ mymaster 172.17.0.2 637953:X 13 Nov 13:54:25.529 * +slave slave 172.17.0.4:6379 172.17.0.4 6379 @ mymaster 172.17.0.2 6379 测试 关闭master（redis-6381）容器 1$ docker stop redis-6381 这时，剩余的2个从机，会自动选举产生新的主机，这里选举172.17.0.4为主机。 175:X 13 Nov 14:05:52.315 # +switch-master mymaster 172.17.0.2 6379 172.17.0.4 6379 查看redis-6383 编程了master 123456789101112# Replicationrole:masterconnected_slaves:1slave0:ip=172.17.0.3,port=6379,state=online,offset=72717,lag=1master_replid:0b2fa8cc7f5e5602c424c79da3a1c704ecd58289master_replid2:1bfa59f9852a17012e9cee2db7311d603acac546master_repl_offset:72852second_repl_offset:56611repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:43repl_backlog_histlen:72810]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>集群</tag>
        <tag>哨兵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文理解 Redis 持久化]]></title>
    <url>%2F2019%2F04%2F14%2Fdocs%2F07-redis%2Fredis-persistence%2F</url>
    <content type="text"><![CDATA[概述Redis 是内存数据库，数据都是存储在内存中的，为了避免进程退出导致数据的永久丢失，需要定期将Redis中的互数据以某形式（数据或命令）从内存保存到磁盘中；当下次Redis重启时，利用持久化文件实现数据恢复。除此之外，为了进行灾难备份，可以将持久化文件拷贝到一个远程位置。 Redis 持久化分为RDB持久化和AOF持久化：前者将当前数据保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的binlog）；由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。 本文一次介绍RDB持久化和AOF持久化。 RDB 持久化RDB持久化是将当前进程中的数据生成快照保存到硬盘(因此也称作快照持久化)，保存的文件后缀是rdb；当Redis重新启动时，可以读取快照文件恢复数据。 特点 RDB 是一种快照模式 RDB 有 2 种持久方式，同步 save 模式和异步 bgsave 模式。由于 save 是同步的，所以可以保证数据一致性，而 bgsave 则不能 save 可以在客户端显式触发，也可以在 shutdown 时自动触发；bgsave 可以在客户端显式触发，也可以通过配置由定时任务触发，也可以在 slave 节点触发 save 导致 redis 同步阻塞，基本已经废弃。bgsave 则不会导致阻塞，但也有缺点：在 fork 时，需要增加内存服务器开销，因为当内存不够时，将使用虚拟内存，导致阻塞 Redis 运行。所以，需要保证空闲内存足够 默认执行 shutdown 时，如果没有开启 AOF，则自动执行 bgsave 每次的 RDB 文件都是替换的 Redis 会压缩 RDB 文件，使用 LZF 算法，让最终的 RDB 文件远小于内存大小，默认开启。但会消耗 CPU。需要注意的是，RDB文件的压缩并不是针对整个文件进行的，而是对数据库中的字符串进行的，且只有在字符串达到一定长度(20字节)时才会进行 save m n的实现原理Redis的save m n，是通过serverCron函数、dirty计数器、和lastsave时间戳来实现的 serverCron是Redis服务器的周期性操作函数，默认每隔100ms执行一次；该函数对服务器的状态进行维护，其中一项工作就是检查 save m n 配置的条件是否满足，如果满足就执行bgsave dirty计数器是Redis服务器维持的一个状态，记录了上一次执行bgsave/save命令后，服务器状态进行了多少次修改(包括增删改)；而当save/bgsave执行完成后，会将dirty重新置为0 例如，如果Redis执行了set mykey helloworld，则dirty值会+1；如果执行了sadd myset v1 v2 v3，则dirty值会+3；注意dirty记录的是服务器进行了多少次修改，而不是客户端执行了多少修改数据的命令 lastsave时间戳也是Redis服务器维持的一个状态，记录的是上一次成功执行save/bgsave的时间 save m n的原理如下：每隔100ms，执行serverCron函数；在serverCron函数中，遍历save m n配置的保存条件，只要有一个条件满足，就进行bgsave。对于每一个save m n条件，只有下面两条同时满足时才算满足 当前时间-lastsave &gt; m dirty &gt;= n RDB文件格式RDB文件格式如下图所示 REDIS：常量，保存着”REDIS”5个字符 db_version：RDB文件的版本号，注意不是Redis的版本号 SELECTDB 0 pairs：表示一个完整的数据库(0号数据库)，同理SELECTDB 3 pairs表示完整的3号数据库；只有当数据库中有键值对时，RDB文件中才会有该数据库的信息(上图所示的Redis中只有0号和3号数据库有键值对)；如果Redis中所有的数据库都没有键值对，则这一部分直接省略。其中：SELECTDB是一个常量，代表后面跟着的是数据库号码；0和3是数据库号码；pairs则存储了具体的键值对信息，包括key、value值，及其数据类型、内部编码、过期时间、压缩信息等等 EOF：常量，标志RDB文件正文内容结束 check_sum：前面所有内容的校验和；Redis在载入RBD文件时，会计算前面的校验和并与check_sum值比较，判断文件是否损坏 启动时加载RDB文件的载入工作是在服务器启动时自动执行的，并没有专门的命令。但是由于AOF的优先级更高，因此当AOF开启时，Redis会优先载入AOF文件来恢复数据；只有当AOF关闭时，才会在Redis服务器启动时检测RDB文件，并自动载入。服务器载入RDB文件期间处于阻塞状态，直到载入完成为止。 Redis载入RDB文件时，会对RDB文件进行校验，如果文件损坏，则日志中会打印错误，Redis启动失败。 配置说明 save m n：bgsave自动触发的条件；如果没有save m n配置，相当于自动的RDB持久化关闭，不过此时仍可以通过其他方式触发 stop-writes-on-bgsave-error yes：当bgsave出现错误时，Redis是否停止执行写命令；设置为yes，则当硬盘出现问题时，可以及时发现，避免数据的大量丢失；设置为no，则Redis无视bgsave的错误继续执行写命令，当对Redis服务器的系统(尤其是硬盘)使用了监控时，该选项考虑设置为no rdbcompression yes：是否开启RDB文件压缩 rdbchecksum yes：是否开启RDB文件的校验，在写入文件和读取文件时都起作用；关闭checksum在写入文件和启动文件时大约能带来10%的性能提升，但是数据损坏时无法发现 dbfilename dump.rdb：RDB文件名dir /var/lib/redis：RDB文件和AOF文件所在目录 执行流程 Redis父进程首先判断，当前是否在执行save或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令 父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令 子进程创建RDB文件，根据父进程内存快照生成临时快照文件，完成后对原有文件进行原子替换 子进程发送信号给父进程表示完成，父进程更新统计信息 优缺点优点 文件紧凑，适合备份，全量复制场景。例如每 6 小时执行 bgsave，保存到文件系统之类的。 Redis 加载 RDB 恢复数据远远快于 AOF 缺点 无法秒级持久化 老版本 Redis 无法兼容新版本 RDB AOF 持久化RDB持久化是将进程数据写入文件，而AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中（有点像MySQL的binlog）；当Redis重启时再次执行AOF文件中的命令来恢复数据。 与RDB相比，AOF的实时性更好，因此已成为主流的持久化方案。Redis服务器默认开启RDB，关闭AOF；要开启AOF，需要在配置文件中配置appendonly yes 与RDB持久化相对应，AOF的优点在于支持秒级持久化、兼容性好，缺点是文件大、恢复速度慢、对性能影响大 执行流程AOF的执行流程包括： 命令追加(append)：将Redis的写命令追加到缓冲区aof_buf 文件写入(write)和文件同步(sync)：根据不同的同步策略将aof_buf中的内容同步到硬盘 文件重写(rewrite)：定期重写AOF文件，达到压缩的目的 1. 命令追加(append)Redis先将写命令追加到缓冲区，而不是直接写入文件，主要是为了避免每次有写命令都直接写入硬盘，导致硬盘IO成为Redis负载的瓶颈 命令追加的格式是Redis命令请求的协议格式，它是一种纯文本格式，具有兼容性好、可读性强、容易处理、操作简单避免二次开销等优点；具体格式略。在AOF文件中，除了用于指定数据库的select命令（如select 0 为选中0号数据库）是由Redis添加的，其他都是客户端发送来的写命令 2.文件写入(write)和文件同步(sync)Redis提供了多种AOF缓存区的同步文件策略，策略涉及到操作系统的write函数和fsync函数，说明如下： 为了提高文件写入效率，在现代操作系统中，当用户调用write函数将数据写入文件时，操作系统通常会将数据暂存到一个内存缓冲区里，当缓冲区被填满或超过了指定时限后，才真正将缓冲区的数据写入到硬盘里。这样的操作虽然提高了效率，但也带来了安全问题：如果计算机停机，内存缓冲区中的数据会丢失；因此系统同时提供了fsync、fdatasync等同步函数，可以强制操作系统立刻将缓冲区中的数据写入到硬盘里，从而确保数据的安全性 AOF缓存区的同步文件策略由参数appendfsync控制，各个值的含义如下： always：命令写入aof_buf后立即调用系统fsync操作同步到AOF文件，fsync完成后线程返回。这种情况下，每次有写命令都要同步到AOF文件，硬盘IO成为性能瓶颈，Redis只能支持大约几百TPS写入，严重降低了Redis的性能；即便是使用固态硬盘（SSD），每秒大约也只能处理几万个命令，而且会大大降低SSD的寿命 no：命令写入aof_buf后调用系统write操作，不对AOF文件做fsync同步；同步由操作系统负责，通常同步周期为30秒。这种情况下，文件同步的时间不可控，且缓冲区中堆积的数据会很多，数据安全性无法保证 everysec：命令写入aof_buf后调用系统write操作，write完成后线程返回；fsync同步文件操作由专门的线程每秒调用一次。everysec是前述两种策略的折中，是性能和数据安全性的平衡，因此是Redis的默认配置，也是我们推荐的配置 3. 文件重写(rewrite)随着时间流逝，Redis服务器执行的写命令越来越多，AOF文件也会越来越大；过大的AOF文件不仅会影响服务器的正常运行，也会导致数据恢复需要的时间过长 文件重写是指定期重写AOF文件，减小AOF文件的体积。需要注意的是，AOF重写是把Redis进程内的数据转化为写命令，同步到新的AOF文件；不会对旧的AOF文件进行任何读取、写入操作! 关于文件重写需要注意的另一点是：对于AOF持久化来说，文件重写虽然是强烈推荐的，但并不是必须的；即使没有文件重写，数据也可以被持久化并在Redis启动的时候导入；因此在一些实现中，会关闭自动的文件重写，然后通过定时任务在每天的某一时刻定时执行 文件重写之所以能够压缩AOF文件，原因在于： 过期的数据不再写入文件 无效的命令不再写入文件：如有些数据被重复设值(set mykey v1, set mykey v2)、有些数据被删除了(sadd myset v1, del myset)等等 多条命令可以合并为一个：如sadd myset v1, sadd myset v2, sadd myset v3可以合并为sadd myset v1 v2 v3。不过为了防止单条命令过大造成客户端缓冲区溢出，对于list、set、hash、zset类型的key，并不一定只使用一条命令；而是以某个常量为界将命令拆分为多条。这个常量在redis.h/REDIS_AOF_REWRITE_ITEMS_PER_CMD中定义，不可更改，3.0版本中值是64 通过上述内容可以看出，由于重写后AOF执行的命令减少了，文件重写既可以减少文件占用的空间，也可以加快恢复速度。 文件重写的触发文件重写的触发，分为手动触发和自动触发： 手动触发：直接调用bgrewriteaof命令，该命令的执行与bgsave有些类似：都是fork子进程进行具体的工作，且都只有在fork时阻塞 自动触发：根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数，以及aof_current_size和aof_base_size状态确定触发时机 auto-aof-rewrite-min-size：执行AOF重写时，文件的最小体积，默认值为64MB。 auto-aof-rewrite-percentage：执行AOF重写时，当前AOF大小(即aof_current_size)和上一次重写时AOF大小(aof_base_size)的比值 只有当auto-aof-rewrite-min-size和auto-aof-rewrite-percentage两个参数同时满足时，才会自动触发AOF重写，即bgrewriteaof操作 文件重写的流程关于文件重写的流程，有两点需要特别注意：(1)重写由父进程fork子进程进行；(2)重写期间Redis执行的写命令，需要追加到新的AOF文件中，为此Redis引入了aof_rewrite_buf缓存。 Redis父进程首先判断当前是否存在正在执行 bgsave/bgrewriteaof的子进程，如果存在则bgrewriteaof命令直接返回，如果存在bgsave命令则等bgsave执行完成后再执行。前面曾介绍过，这个主要是基于性能方面的考虑 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的 父进程fork后，bgrewriteaof命令返回”Background append only file rewrite started”信息并不再阻塞父进程，并可以响应其他命令。Redis的所有写命令依然写入AOF缓冲区，并根据appendfsync策略同步到硬盘，保证原有AOF机制的正确 由于fork操作使用写时复制技术，子进程只能共享fork操作时的内存数据。由于父进程依然在响应命令，因此Redis使用AOF重写缓冲区(图中的aof_rewrite_buf)保存这部分数据，防止新AOF文件生成期间丢失这部分数据。也就是说，bgrewriteaof执行期间，Redis的写命令同时追加到aof_buf和aof_rewirte_buf两个缓冲区 子进程根据内存快照，按照命令合并规则写入到新的AOF文件 子进程写完新的AOF文件后，向父进程发信号，父进程更新统计信息，具体可以通过info persistence查看 父进程把AOF重写缓冲区的数据写入到新的AOF文件，这样就保证了新AOF文件所保存的数据库状态和服务器当前状态一致 使用新的AOF文件替换老文件，完成AOF重写 启动时加载前面提到过，当AOF开启时，Redis启动时会优先载入AOF文件来恢复数据；只有当AOF关闭时，才会载入RDB文件恢复数据 当AOF开启，但AOF文件不存在时，即使RDB文件存在也不会加载(更早的一些版本可能会加载，但3.0不会) 与载入RDB文件类似，Redis载入AOF文件时，会对AOF文件进行校验，如果文件损坏，则日志中会打印错误，Redis启动失败。但如果是AOF文件结尾不完整(机器突然宕机等容易导致文件尾部不完整)，且aof-load-truncated参数开启，则日志中会输出警告，Redis忽略掉AOF文件的尾部，启动成功。aof-load-truncated参数默认是开启的： 配置说明 appendonly no：是否开启AOF appendfilename “appendonly.aof”：AOF文件名 dir /var/lib/redis：RDB文件和AOF文件所在目录 appendfsync everysec：fsync持久化策略 no-appendfsync-on-rewrite no：AOF重写期间是否禁止fsync；如果开启该选项，可以减轻文件重写时CPU和硬盘的负载（尤其是硬盘），但是可能会丢失AOF重写期间的数据；需要在负载和安全性之间进行平衡 auto-aof-rewrite-percentage 100：文件重写触发条件之一 auto-aof-rewrite-min-size 64mb：文件重写触发提交之一 aof-load-truncated yes：如果AOF文件结尾损坏，Redis启动时是否仍载入AOF文件 性能与实践持久化策略选择在介绍持久化策略之前，首先要明白无论是RDB还是AOF，持久化的开启都是要付出性能方面代价的：对于RDB持久化，一方面是bgsave在进行fork操作时Redis主进程会阻塞，另一方面，子进程向硬盘写数据也会带来IO压力；对于AOF持久化，向硬盘写数据的频率大大提高(everysec策略下为秒级)，IO压力更大，甚至可能造成AOF追加阻塞问题（后面会详细介绍这种阻塞），此外，AOF文件的重写与RDB的bgsave类似，会有fork时的阻塞和子进程的IO压力问题。相对来说，由于AOF向硬盘中写数据的频率更高，因此对Redis主进程性能的影响会更大。 在实际生产环境中，根据数据量、应用对数据的安全要求、预算限制等不同情况，会有各种各样的持久化策略；如完全不使用任何持久化、使用RDB或AOF的一种，或同时开启RDB和AOF持久化等。此外，持久化的选择必须与Redis的主从策略一起考虑，因为主从复制与持久化同样具有数据备份的功能，而且主机master和从机slave可以独立的选择持久化方案。 下面分场景来讨论持久化策略的选择，下面的讨论也只是作为参考，实际方案可能更复杂更具多样性 如果Redis中的数据完全丢弃也没有关系（如Redis完全用作DB层数据的cache），那么无论是单机，还是主从架构，都可以不进行任何持久化。 在单机环境下（对于个人开发者，这种情况可能比较常见），如果可以接受十几分钟或更多的数据丢失，选择RDB对Redis的性能更加有利；如果只能接受秒级别的数据丢失，应该选择AOF 但在多数情况下，我们都会配置主从环境，slave的存在既可以实现数据的热备，也可以进行读写分离分担Redis读请求，以及在master宕掉后继续提供服务 在这种情况下，一种可行的做法是 master：完全关闭持久化（包括RDB和AOF），这样可以让master的性能达到最好 slave：关闭RDB，开启AOF（如果对数据安全要求不高，开启RDB关闭AOF也可以），并定时对持久化文件进行备份（如备份到其他文件夹，并标记好备份的时间）；然后关闭AOF的自动重写，然后添加定时任务，在每天Redis闲时（如凌晨12点）调用bgrewriteaof。 这里需要解释一下，为什么开启了主从复制，可以实现数据的热备份，还需要设置持久化呢？因为在一些特殊情况下，主从复制仍然不足以保证数据的安全，例如： master和slave进程同时停止：考虑这样一种场景，如果master和slave在同一栋大楼或同一个机房，则一次停电事故就可能导致master和slave机器同时关机，Redis进程停止；如果没有持久化，则面临的是数据的完全丢失 master误重启：考虑这样一种场景，master服务因为故障宕掉了，如果系统中有自动拉起机制（即检测到服务停止后重启该服务）将master自动重启，由于没有持久化文件，那么master重启后数据是空的，slave同步数据也变成了空的；如果master和slave都没有持久化，同样会面临数据的完全丢失。需要注意的是，即便是使用了哨兵(关于哨兵后面会有文章介绍)进行自动的主从切换，也有可能在哨兵轮询到master之前，便被自动拉起机制重启了。因此，应尽量避免“自动拉起机制”和“不做持久化”同时出现 异地灾备：上述讨论的几种持久化策略，针对的都是一般的系统故障，如进程异常退出、宕机、断电等，这些故障不会损坏硬盘。但是对于一些可能导致硬盘损坏的灾难情况，如火灾地震，就需要进行异地灾备。例如对于单机的情形，可以定时将RDB文件或重写后的AOF文件，通过scp拷贝到远程机器，如阿里云、AWS等；对于主从的情形，可以定时在master上执行bgsave，然后将RDB文件拷贝到远程机器，或者在slave上执行bgrewriteaof重写AOF文件后，将AOF文件拷贝到远程机器上。一般来说，由于RDB文件文件小、恢复快，因此灾难恢复常用RDB文件；异地备份的频率根据数据安全性的需要及其他条件来确定，但最好不要低于一天一次 fork阻塞：CPU的阻塞在Redis的实践中，众多因素限制了Redis单机的内存不能过大，例如 当面对请求的暴增，需要从库扩容时，Redis内存过大会导致扩容时间太长 当主机宕机时，切换主机后需要挂载从库，Redis内存过大导致挂载速度过慢 以及持久化过程中的fork操作，下面详细说明 首先说明一下fork操作： 父进程通过fork操作可以创建子进程；子进程创建后，父子进程共享代码段，不共享进程的数据空间，但是子进程会获得父进程的数据空间的副本。在操作系统fork的实际实现中，基本都采用了写时复制技术，即在父/子进程试图修改数据空间之前，父子进程实际上共享数据空间；但是当父/子进程的任何一个试图修改数据空间时，操作系统会为修改的那一部分(内存的一页)制作一个副本 虽然fork时，子进程不会复制父进程的数据空间，但是会复制内存页表（页表相当于内存的索引、目录）；父进程的数据空间越大，内存页表越大，fork时复制耗时也会越多。 在Redis中，无论是RDB持久化的bgsave，还是AOF重写的bgrewriteaof，都需要fork出子进程来进行操作。如果Redis内存过大，会导致fork操作时复制内存页表耗时过多；而Redis主进程在进行fork时，是完全阻塞的，也就意味着无法响应客户端的请求，会造成请求延迟过大。 对于不同的硬件、不同的操作系统，fork操作的耗时会有所差别，一般来说，如果Redis单机内存达到了10GB，fork时耗时可能会达到百毫秒级别（如果使用Xen虚拟机，这个耗时可能达到秒级别）。因此，一般来说Redis单机内存一般要限制在10GB以内；不过这个数据并不是绝对的，可以通过观察线上环境fork的耗时来进行调整。观察的方法如下：执行命令info stats，查看latest_fork_usec的值，单位为微秒。 为了减轻fork操作带来的阻塞问题，除了控制Redis单机内存的大小以外，还可以适度放宽AOF重写的触发条件、选用物理机或高效支持fork操作的虚拟化技术等，例如使用Vmware或KVM虚拟机，不要使用Xen虚拟机 AOF追加阻塞：硬盘的阻塞前面提到过，在AOF中，如果AOF缓冲区的文件同步策略为everysec，则：在主线程中，命令写入aof_buf后调用系统write操作，write完成后主线程返回；fsync同步文件操作由专门的文件同步线程每秒调用一次。 这种做法的问题在于，如果硬盘负载过高，那么fsync操作可能会超过1s；如果Redis主线程持续高速向aof_buf写入命令，硬盘的负载可能会越来越大，IO资源消耗更快；如果此时Redis进程异常退出，丢失的数据也会越来越多，可能远超过1s。 为此，Redis的处理策略是这样的：主线程每次进行AOF会对比上次fsync成功的时间；如果距上次不到2s，主线程直接返回；如果超过2s，则主线程阻塞直到fsync同步完成。因此，如果系统硬盘负载过大导致fsync速度太慢，会导致Redis主线程的阻塞；此外，使用everysec配置，AOF最多可能丢失2s的数据，而不是1s。 AOF追加阻塞问题定位的方法： 监控info Persistence中的aof_delayed_fsync：当AOF追加阻塞发生时（即主线程等待fsync而阻塞），该指标累加。 AOF阻塞时的Redis日志： Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis. 如果AOF追加阻塞频繁发生，说明系统的硬盘负载太大；可以考虑更换IO速度更快的硬盘，或者通过IO监控分析工具对系统的IO负载进行分析，如iostat（系统级io）、iotop（io版的top）、pidstat等。 info命令与持久化前面提到了一些通过info命令查看持久化相关状态的方法，下面来总结一下 1. info Persistence db_last_bgsave_status:上次bgsave 执行结果，可以用于发现bgsave错误 rdb_last_bgsave_time_sec:上次bgsave执行时间（单位是s），可以用于发现bgsave是否耗时过长 aof_enabled:AOF是否开启 aof_last_rewrite_time_sec: 上次文件重写执行时间（单位是s），可以用于发现文件重写是否耗时过长 aof_last_bgrewrite_status: 上次bgrewrite执行结果，可以用于发现bgrewrite错误 aof_buffer_length和aof_rewrite_buffer_length:aof缓存区大小和aof重写缓冲区大小 aof_delayed_fsync:AOF追加阻塞情况的统计 2. info stats其中与持久化关系较大的是：latest_fork_usec，代表上次fork耗时 总结 持久化在Redis高可用中的作用：数据备份，与主从复制相比强调的是由内存到硬盘的备份。 RDB持久化：将数据快照备份到硬盘；介绍了其触发条件（包括手动出发和自动触发）、执行流程、RDB文件等，特别需要注意的是文件保存操作由fork出的子进程来进行。 AOF持久化：将执行的写命令备份到硬盘（类似于MySQL的binlog），介绍了其开启方法、执行流程等，特别需要注意的是文件同步策略的选择（everysec）、文件重写的流程。 一些现实的问题：包括如何选择持久化策略，以及需要注意的fork阻塞、AOF追加阻塞等。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>持久化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[COOKIE、SESSION、TOKEN各自的优缺点都有哪些？]]></title>
    <url>%2F2019%2F03%2F12%2Fdocs%2F09-pc-base%2Fsession-cookie-token%2F</url>
    <content type="text"><![CDATA[前言HTTP是一个无状态协议。什么是无状态呢？就是说这一次请求和上一次请求是没有关系的，互不认识的，没有关联的。这种无状态的好处是快速。坏处是假如我们想把对次请求关联起来，必须使用某些手段和工具。 COOKIECOOKIE 是什么 COOKIE 本身是由服务器产生的，生成之后发送给浏览器，并保存在浏览器 COOKIE 就是浏览器存储在本地目录的一小段文本 COOKIE 是以 key-value 形式存储的 COOKIE 有大小限制，为了保证COOKIE不占用太多的磁盘空间，每个COOKIE大小一般不超过4KB COOKIE 默认在会话结束后直接销毁，此种COOKIE称之为会话COOKIE COOKIE 可以设置过期时间，此种COOKIE 称之为持久COOKIE COOKIE 的生命周期 COOKIE 的不足 每个 COOKIE 的容量有限 因为cookie由浏览器存储在本地目录，所以不方便记录敏感信息，如密码等 cookie不支持跨域访问 cookie不支持手机端方案 SESSION什么是 SESSION SESSION 是由服务器产生的，存储在服务端 SESSION 的存储形式多种多样，可以是文件、数据库、缓存等，这需要靠程序如何设计 SESSION 也是以 key-value 形式存储的 SESSION 是没有大小限制的，这比cookie灵活很多，不过将过多的东西放在其中也并不是明智的做法 SESSION 也有过期时间的概念，默认为30分钟，可以通过tomcat、web.xml等方式进行配置 SESSION 可以主动通过invalidate()方法进行销毁 SESSION 通过session_id识别，如果请求持有正确的session_id，则服务器认为此请求处于session_id代表的会话中 SESSION 的生命周期 SESSION 的不足 SESSION 大小不限制，存储在服务端，本身是对资源的一种负担 如何保证session的高可用、准确性，优势对整体架构的一种负担 频繁的创建、查询、验证session，会对服务器造成很大的压力 SESSION 是有状态的 TOKEN什么是TOKEN TOKEN 是一种轻量级的用户验证方式 TOKEN 是无状态的 TOKEN 允许跨域访问 TOKEN 是服务端生成的一个字符串，保存在客户端（可以放在cookie中），作为请求服务的验证令牌 TOKEN 无需存放在服务端，这样服务端无需存放用户信息 TOKEN 对服务端压力极小`，因为服务端只需存储秘钥，并支持生成token的算法，无需存储token TOKEN 最简单的构造：用户唯一的身份标识(辨识用户) + 时间戳(用于过期校验) + 签名(防止第三方恶意冒充) TOKEN 无法主动过期，只能等待它达到过期时间后才会失效 TOKEN 的产生：首次请求时，服务器对请求参数（如账号、密码）验证通过，则根据用户标识，加上服务的密钥，通过生成算法，生成token TOKEN 的验证：再次请求时，携带此token，则服务端再次根据用户标识，生成token，根据两个token是否一致且未过期来判定用户是否已授权 TOKEN 的生命周期 TOKEN 的不足 TOKEN 无法主动过期，只能等待它达到过期时间后才会失效 TOKEN 本身比session_id要大，会消耗更多的流量与带宽]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>会话机制</tag>
        <tag>cookie</tag>
        <tag>session</tag>
        <tag>token</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）MySQL优化原理]]></title>
    <url>%2F2019%2F01%2F05%2Fdocs%2F04-mysql%2Fmysql-optimization-principle%2F</url>
    <content type="text"><![CDATA[说起MySQL的查询优化，相信大家收藏了一堆奇技淫巧：不能使用SELECT *、不使用NULL字段、合理创建索引、为字段选择合适的数据类型….. 你是否真的理解这些优化技巧？是否理解其背后的工作原理？在实际场景下性能真有提升吗？我想未必。因而理解这些优化建议背后的原理就尤为重要，希望本文能让你重新审视这些优化建议，并在实际业务场景下合理的运用。 MySQL逻辑架构如果能在头脑中构建一幅MySQL各组件之间如何协同工作的架构图，有助于深入理解MySQL服务器。下图展示了MySQL的逻辑架构图。 MySQL逻辑架构整体分为三层，最上层为客户端层，并非MySQL所独有，诸如：连接处理、授权认证、安全等功能均在这一层处理。 MySQL大多数核心服务均在中间这一层，包括查询解析、分析、优化、缓存、内置函数(比如：时间、数学、加密等函数)。所有的跨存储引擎的功能也在这一层实现：存储过程、触发器、视图等。 最下层为存储引擎，其负责MySQL中的数据存储和提取。和Linux下的文件系统类似，每种存储引擎都有其优势和劣势。中间的服务层通过API与存储引擎通信，这些API接口屏蔽了不同存储引擎间的差异。 MySQL查询过程我们总是希望MySQL能够获得更高的查询性能，最好的办法是弄清楚MySQL是如何优化和执行查询的。一旦理解了这一点，就会发现：很多的查询优化工作实际上就是遵循一些原则让MySQL的优化器能够按照预想的合理方式运行而已。 当向MySQL发送一个请求的时候，MySQL到底做了些什么呢？ 客户端/服务端通信协议MySQL客户端/服务端通信协议是“半双工”的：在任一时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，这两个动作不能同时发生。一旦一端开始发送消息，另一端要接收完整个消息才能响应它，所以我们无法也无须将一个消息切成小块独立发送，也没有办法进行流量控制。 客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 与之相反的是，服务器响应给用户的数据通常会很多，由多个数据包组成。但是当服务器响应客户端请求时，客户端必须完整的接收整个返回结果，而不能简单的只取前面几条结果，然后让服务器停止发送。因而在实际开发中，尽量保持查询简单且只返回必需的数据，减小通信间数据包的大小和数量是一个非常好的习惯，这也是查询中尽量避免使用SELECT *以及加上LIMIT限制的原因之一。 查询缓存在解析一个查询语句前，如果查询缓存是打开的，那么MySQL会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 既然是缓存，就会失效，那查询缓存何时失效呢？MySQL的查询缓存系统会跟踪查询中涉及的每个表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效。正因为如此，在任何的写操作时，MySQL必须将对应表的所有缓存都设置为失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗，甚至导致系统僵死一会儿。而且查询缓存对系统的额外消耗也不仅仅在写操作，读操作也不例外： 任何的查询语句在开始之前都必须经过检查，即使这条SQL语句永远不会命中缓存 如果查询结果可以被缓存，那么执行完成后，会将结果存入缓存，也会带来额外的系统消耗 基于此，我们要知道并不是什么情况下查询缓存都会提高系统性能，缓存和失效都会带来额外消耗，只有当缓存带来的资源节约大于其本身消耗的资源时，才会给系统带来性能提升。但要如何评估打开缓存是否能够带来性能提升是一件非常困难的事情，也不在本文讨论的范畴内。如果系统确实存在一些性能问题，可以尝试打开查询缓存，并在数据库设计上做一些优化，比如： 用多个小表代替一个大表，注意不要过度设计 批量插入代替循环单条插入 合理控制缓存空间大小，一般来说其大小设置为几十兆比较合适 可以通过SQL_CACHE和SQL_NO_CACHE来控制某个查询语句是否需要进行缓存 最后的忠告是不要轻易打开查询缓存，特别是写密集型应用。如果你实在是忍不住，可以将query_cache_type设置为DEMAND，这时只有加入SQL_CACHE的查询才会走缓存，其他查询则不会，这样可以非常自由地控制哪些查询需要被缓存。 当然查询缓存系统本身是非常复杂的，这里讨论的也只是很小的一部分，其他更深入的话题，比如：缓存是如何使用内存的？如何控制内存的碎片化？事务对查询缓存有何影响等等，读者可以自行阅读相关资料，这里权当抛砖引玉吧。 语法解析和预处理MySQL通过关键字将SQL语句进行解析，并生成一颗对应的解析树。这个过程解析器主要通过语法规则来验证和解析。比如SQL中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据MySQL规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。 查询优化经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成查询计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在MySQL可以通过查询当前会话的last_query_cost的值来得到其计算当前查询的成本。 示例中的结果表示优化器认为大概需要做6391个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致MySQL选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但MySQL值选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 重新定义表的关联顺序（多张表关联查询时，并不一定按照SQL中指定的顺序进行，但有一些技巧可以指定关联顺序） 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找B+Tree索引最左端，反之则可以找到最大值，具体原理见下文） 提前终止查询（比如：使用Limit时，查找到满足数量的结果集后会立即终止查询） 优化排序（在老版本MySQL会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于I/O密集型应用，效率会高很多） 随着MySQL的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 查询执行引擎在完成解析和优化阶段以后，MySQL会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 返回结果给客户端查询执行的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。 如果查询缓存被打开且这个查询可以被缓存，MySQL也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能MySQL在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足①中所描述的通信协议的数据包发送，再通过TCP协议进行传输，在传输过程中，可能对MySQL的数据包进行缓存然后批量发送。 回头总结一下MySQL整个查询执行过程，总的来说分为5个步骤： 客户端向MySQL服务器发送一条查询请求 服务器首先检查查询缓存，如果命中缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段 服务器进行SQL解析、预处理、再由优化器生成对应的执行计划 MySQL根据执行计划，调用存储引擎的API来执行查询 将结果返回给客户端，同时缓存查询结果 性能优化建议看了这么多，你可能会期待给出一些优化手段，是的，下面会从3个不同方面给出一些优化建议。但请等等，还有一句忠告要先送给你：不要听信你看到的关于优化的“绝对真理”，包括本文所讨论的内容，而应该是在实际的业务场景下通过测试来验证你关于执行计划以及响应时间的假设。 Scheme设计与数据类型优化 选择数据类型只要遵循小而简单的原则就好，越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的CPU周期也更少。越简单的数据类型在计算时需要更少的CPU周期，比如，整型就比字符操作代价低，因而会使用整型来存储ip地址，使用DATETIME来存储时间，而不是使用字符串。 这里总结几个可能容易理解错误的技巧： 通常来说把可为NULL的列改为NOT NULL不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为NOT NULL。 对整数类型指定宽度，比如INT(11)，没有任何卵用。INT使用32位（4个字节）存储空间，那么它的表示范围已经确定，所以INT(1)和INT(20)对于存储和计算是相同的。 UNSIGNED表示不允许负值，大致可以使正数的上限提高一倍。比如TINYINT存储范围是-128 ~ 127，而UNSIGNED TINYINT存储的范围却是0 - 255。 通常来讲，没有太大的必要使用DECIMAL数据类型。即使是在需要存储财务数据时，仍然可以使用BIGINT。比如需要精确到万分之一，那么可以将数据乘以一百万然后使用BIGINT存储。这样可以避免浮点数计算不准确和DECIMAL精确计算代价高的问题。 TIMESTAMP使用4个字节存储空间，DATETIME使用8个字节存储空间。因而，TIMESTAMP只能表示1970 - 2038年，比DATETIME表示的范围小得多，而且TIMESTAMP的值因时区不同而不同。 大多数情况下没有使用枚举类型的必要，其中一个缺点是枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。 schema的列不要太多。原因是存储引擎的API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列，这个转换过程的代价是非常高的。如果列太多而实际使用的列又很少的话，有可能会导致CPU占用过高。 大表ALTER TABLE非常耗时，MySQL执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。尤其当内存不足而表又很大，而且还有很大索引的情况下，耗时更久。当然有一些奇技淫巧可以解决这个问题，有兴趣可自行查阅。 创建高性能索引 索引是提高MySQL查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的SQL才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。 索引相关的数据结构和算法 通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为MySQL在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如InnoDB就是使用的B+Tree。 B+Tree中的B是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 在介绍B+Tree前，先了解一下二叉查找树，它是一种经典的数据结构，其左子树的值总是小于根的值，右子树的值总是大于根的值，如下图①。如果要在这课树中查找值为5的记录，其大致流程：先找到根，其值为6，大于5，所以查找左子树，找到3，而5大于3，接着找3的右子树，总共找了3次。同样的方法，如果查找值为8的记录，也需要查找3次。所以二叉查找树的平均查找次数为(3 + 3 + 3 + 2 + 2 + 1) / 6 = 2.3次，而顺序查找的话，查找值为2的记录，仅需要1次，但查找值为8的记录则需要6次，所以顺序查找的平均查找次数为：(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.3次，因此大多数情况下二叉查找树的平均查找速度比顺序查找要快。 由于二叉查找树可以任意构造，同样的值，可以构造出如图②的二叉查找树，显然这棵二叉树的查询效率和顺序查找差不多。若想二叉查找数的查询性能最高，需要这棵二叉查找树是平衡的，也即平衡二叉树（AVL树）。 平衡二叉树首先需要符合二叉查找树的定义，其次必须满足任何节点的两个子树的高度差不能大于1。显然图②不满足平衡二叉树的定义，而图①是一课平衡二叉树。平衡二叉树的查找性能是比较高的（性能最好的是最优二叉树），查询性能越好，维护的成本就越大。比如图①的平衡二叉树，当用户需要插入一个新的值9的节点时，就需要做出如下变动。 一种行之有效的解决方法是减少树的深度，将二叉树变为m叉树（多路搜索树），而B+Tree就是一种多路搜索树。理解B+Tree时，只需要理解其最重要的两个特征即可：第一，所有的关键字（可以理解为数据）都存储在叶子节点（Leaf Page），非叶子节点（Index Page）并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。其次，所有的叶子节点由指针连接。如下图为高度为2的简化了的B+Tree 怎么理解这两个特征？MySQL将每个节点的大小设置为一个页的整数倍（原因下文会介绍），也就是在节点空间大小一定的情况下，每个节点可以存储更多的内结点，这样每个结点能索引的范围更大更精确。所有的叶子节点使用指针链接的好处是可以进行区间访问，比如上图中，如果查找大于20而小于30的记录，只需要找到节点20，就可以遍历指针依次找到25、30。如果没有链接指针的话，就无法进行区间查找。这也是MySQL使用B+Tree作为索引存储结构的重要原因。 MySQL为何将节点大小设置为页的整数倍，这就需要理解磁盘的存储原理。磁盘本身存取就比主存慢很多，在加上机械运动损耗（特别是普通的机械硬盘），磁盘的存取速度往往是主存的几百万分之一，为了尽量减少磁盘I/O，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存，预读的长度一般为页的整数倍。 页是计算机管理存储器的逻辑块，硬件及OS往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（许多OS中，页的大小通常为4K）。主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后一起返回，程序继续运行。 MySQL巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了读取一个节点只需一次I/O。假设B+Tree的高度为h，一次检索最多需要h-1次I/O（根节点常驻内存），复杂度O(h) = O(logmN)。实际应用场景中，M通常较大，常常超过100，因此树的高度一般都比较小，通常不超过3。 最后简单了解下B+Tree节点的操作，在整体上对索引的维护有一个大概的了解，虽然索引可以大大提高查询效率，但维护索引仍要花费很大的代价，因此合理的创建索引也就尤为重要。 仍以上面的树为例，我们假设每个节点只能存储4个内节点。首先要插入第一个节点28，如下图所示。 接着插入下一个节点70，在Index Page中查询后得知应该插入到50 - 70之间的叶子节点，但叶子节点已满，这时候就需要进行也分裂的操作，当前的叶子节点起点为50，所以根据中间值来拆分叶子节点，如下图所示。 最后插入一个节点95，这时候Index Page和Leaf Page都满了，就需要做两次拆分，如下图所示。 拆分后最终形成了这样一颗树。 B+Tree为了保持平衡，对于新插入的值需要做大量的拆分页操作，而页的拆分需要I/O操作，为了尽可能的减少页的拆分操作，B+Tree也提供了类似于平衡二叉树的旋转功能。当Leaf Page已满但其左右兄弟节点没有满的情况下，B+Tree并不急于去做拆分操作，而是将记录移到当前所在页的兄弟节点上。通常情况下，左兄弟会被先检查用来做旋转操作。就比如上面第二个示例，当插入70的时候，并不会去做页拆分，而是左旋操作。 通过旋转操作可以最大限度的减少页分裂，从而减少索引维护过程中的磁盘的I/O操作，也提高索引维护效率。需要注意的是，删除节点跟插入节点类似，仍然需要旋转和拆分操作，这里就不再说明。 高性能策略通过上文，相信你对B+Tree的数据结构已经有了大致的了解，但MySQL中索引是如何组织数据的存储呢？以一个简单的示例来说明，假如有如下数据表： 对于表中每一行数据，索引中包含了last_name、first_name、dob列的值，下图展示了索引是如何组织数据存储的。 可以看到，索引首先根据第一个字段来排列顺序，当名字相同时，则根据第三个字段，即出生日期来排序，正是因为这个原因，才有了索引的“最左原则”。 MySQL不会使用索引的情况：非独立的列 “独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如： 1select * from where id + 1 = 5 我们很容易看出其等价于 id = 4，但是MySQL无法自动解析这个表达式，使用函数是同样的道理。 前缀索引 如果列很长，通常可以索引开始的部分字符，这样可以有效节约索引空间，从而提高索引效率。 多列索引和索引顺序 在多数情况下，在多个列上建立独立的索引并不能提高查询性能。理由非常简单，MySQL不知道选择哪个索引的查询效率更好，所以在老版本，比如MySQL5.0之前就会随便选择一个列的索引，而新的版本会采用合并索引的策略。举个简单的例子，在一张电影演员表中，在actor_id和film_id两个列上都建立了独立的索引，然后有如下查询： 1select film_id,actor_id from film_actor where actor_id = 1 or film_id = 1 老版本的MySQL会随机选择一个索引，但新版本做如下的优化： 123select film_id,actor_id from film_actor where actor_id = 1 union all select film_id,actor_id from film_actor where film_id = 1 and actor_id &lt;&gt; 1 当出现多个索引做相交操作时（多个AND条件），通常来说一个包含所有相关列的索引要优于多个独立索引。 当出现多个索引做联合操作时（多个OR条件），对结果集的合并、排序等操作需要耗费大量的CPU和内存资源，特别是当其中的某些索引的选择性不高，需要返回合并大量数据时，查询成本更高。所以这种情况下还不如走全表扫描。 因此explain时如果发现有索引合并（Extra字段出现Using union），应该好好检查一下查询和表结构是不是已经是最优的，如果查询和表都没有问题，那只能说明索引建的非常糟糕，应当慎重考虑索引是否合适，有可能一个包含所有相关列的多列索引更适合。 前面我们提到过索引如何组织数据存储的，从图中可以看到多列索引时，索引的顺序对于查询是至关重要的，很明显应该把选择性更高的字段放到索引的前面，这样通过第一个字段就可以过滤掉大多数不符合条件的数据。 索引选择性是指不重复的索引值和数据表的总记录数的比值，选择性越高查询效率越高，因为选择性越高的索引可以让MySQL在查询时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。 理解索引选择性的概念后，就不难确定哪个字段的选择性较高了，查一下就知道了，比如： 1SELECT * FROM payment where staff_id = 2 and customer_id = 584 是应该创建(staff_id,customer_id)的索引还是应该颠倒一下顺序？执行下面的查询，哪个字段的选择性更接近1就把哪个字段索引前面就好。 123select count(distinct staff_id)/count(*) as staff_id_selectivity, count(distinct customer_id)/count(*) as customer_id_selectivity, count(*) from payment 多数情况下使用这个原则没有任何问题，但仍然注意你的数据中是否存在一些特殊情况。举个简单的例子，比如要查询某个用户组下有过交易的用户信息： 1select user_id from trade where user_group_id = 1 and trade_amount &gt; 0 MySQL为这个查询选择了索引(user_group_id,trade_amount)，如果不考虑特殊情况，这看起来没有任何问题，但实际情况是这张表的大多数数据都是从老系统中迁移过来的，由于新老系统的数据不兼容，所以就给老系统迁移过来的数据赋予了一个默认的用户组。这种情况下，通过索引扫描的行数跟全表扫描基本没什么区别，索引也就起不到任何作用。 推广开来说，经验法则和推论在多数情况下是有用的，可以指导我们开发和设计，但实际情况往往会更复杂，实际业务场景下的某些特殊情况可能会摧毁你的整个设计。 避免多个范围条件 实际开发中，我们会经常使用多个范围条件，比如想查询某个时间段内登录过的用户： 1select user.* from user where login_time &gt; &apos;2017-04-01&apos; and age between 18 and 30; 这个查询有一个问题：它有两个范围条件，login_time列和age列，MySQL可以使用login_time列的索引或者age列的索引，但无法同时使用它们。 覆盖索引 如果一个索引包含或者说覆盖所有需要查询的字段的值，那么就没有必要再回表查询，这就称为覆盖索引。覆盖索引是非常有用的工具，可以极大的提高性能，因为查询只需要扫描索引会带来许多好处： 索引条目远小于数据行大小，如果只读取索引，极大减少数据访问量索引是有按照列值顺序存储的，对于I/O密集型的范围查询要比随机从磁盘读取每一行数据的IO要少的多 使用索引扫描来排序MySQL有两种方式可以生产有序的结果集，其一是对结果集进行排序的操作，其二是按照索引顺序扫描得出的结果自然是有序的。如果explain的结果中type列的值为index表示使用了索引扫描来做排序。 扫描索引本身很快，因为只需要从一条索引记录移动到相邻的下一条记录。但如果索引本身不能覆盖所有需要查询的列，那么就不得不每扫描一条索引记录就回表查询一次对应的行。这个读取操作基本上是随机I/O，因此按照索引顺序读取数据的速度通常要比顺序地全表扫描要慢。 在设计索引时，如果一个索引既能够满足排序，又满足查询，是最好的。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向也一样时，才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有ORDER BY子句引用的字段全部为第一张表时，才能使用索引做排序。ORDER BY子句和查询的限制是一样的，都要满足最左前缀的要求（有一种情况例外，就是最左的列被指定为常数，下面是一个简单的示例），其他情况下都需要执行排序操作，而无法利用索引排序。 12// 最左列为常数，索引：(date,staff_id,customer_id)select staff_id,customer_id from demo where date = &apos;2015-06-01&apos; order by staff_id,customer_id 冗余和重复索引 冗余索引是指在相同的列上按照相同的顺序创建的相同类型的索引，应当尽量避免这种索引，发现后立即删除。比如有一个索引(A,B)，再创建索引(A)就是冗余索引。冗余索引经常发生在为表添加新索引时，比如有人新建了索引(A,B)，但这个索引不是扩展已有的索引(A)。 大多数情况下都应该尽量扩展已有的索引而不是创建新索引。但有极少情况下出现性能方面的考虑需要冗余索引，比如扩展已有索引而导致其变得过大，从而影响到其他使用该索引的查询。 删除长期未使用的索引 定期删除一些长时间未使用过的索引是一个非常好的习惯。 关于索引这个话题打算就此打住，最后要说一句，索引并不总是最好的工具，只有当索引帮助提高查询速度带来的好处大于其带来的额外工作时，索引才是有效的。对于非常小的表，简单的全表扫描更高效。对于中到大型的表，索引就非常有效。对于超大型的表，建立和维护索引的代价随之增长，这时候其他技术也许更有效，比如分区表。最后的最后，explain后再提测是一种美德。 特定类型查询优化优化COUNT()查询 COUNT()可能是被大家误解最多的函数了，它有两种不同的作用，其一是统计某个列值的数量，其二是统计行数。统计列值时，要求列值是非空的，它不会统计NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用COUNT(*)时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。 我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用COUNT(*)，意义清晰，且性能更好。 有时候某些业务场景并不需要完全精确的COUNT值，可以用近似值来代替，EXPLAIN出来的行数就是一个不错的近似值，而且执行EXPLAIN并不需要真正地去执行查询，所以成本非常低。通常来说，执行COUNT()都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用redis这样的外部缓存系统。 优化关联查询 在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用JOIN有更好的性能。如果确实需要使用关联查询的情况下，需要特别注意的是： 确保ON和USING字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器关联的顺序是A、B，那么就不需要在A表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。 确保任何的GROUP BY和ORDER BY中的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化。 要理解优化关联查询的第一个技巧，就需要理解MySQL是如何执行关联查询的。当前MySQL关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。 太抽象了？以上面的示例来说明，比如有这样的一个查询： 123SELECT A.xx,B.yy FROM A INNER JOIN B USING(c)WHERE A.xx IN (5,6) 假设MySQL按照查询中的关联顺序A、B来进行关联操作，那么可以用下面的伪代码表示MySQL如何完成这个查询： 可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。 优化LIMIT分页 当需要分页操作时，通常会使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY字句。如果有对应的索引，通常效率会不错，否则，MySQL需要做大量的文件排序操作。 一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL需要查询10020条记录然后只返回20条记录，前面的10000条都将被抛弃，这样的代价非常高。 优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询： 1SELECT film_id,description FROM film ORDER BY title LIMIT 50,5; 如果这张表非常大，那么这个查询最好改成下面的样子： 1234SELECT film.film_id,film.descriptionFROM film INNER JOIN ( SELECT film_id FROM film ORDER BY title LIMIT 50,5) AS tmp USING(film_id); 这里的延迟关联将大大提升查询效率，让MySQL扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。 有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询： SELECT id FROM t LIMIT 10000, 10; 改为： SELECT id FROM t WHERE id &gt; 10000 LIMIT 10; 其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。 优化UNION MySQL处理UNION的策略是先创建临时表，然后再把各个查询结果插入到临时表中，最后再来做查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。 除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用ALL关键字，MySQL总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。 结语理解查询是如何执行以及时间都消耗在哪些地方，再加上一些优化过程的知识，可以帮助大家更好的理解MySQL，理解常见优化技巧背后的原理。希望本文中的原理、示例能够帮助大家更好的将理论和实践联系起来，更多的将理论知识运用到实践中。 其他也没啥说的了，给大家留两个思考题吧，可以在脑袋里想想答案，这也是大家经常挂在嘴边的，但很少有人会思考为什么？ 有非常多的程序员在分享时都会抛出这样一个观点：尽可能不要使用存储过程，存储过程非常不容易维护，也会增加使用成本，应该把业务逻辑放到客户端。既然客户端都能干这些事，那为什么还要存储过程？ JOIN本身也挺方便的，直接查询就好了，为什么还需要视图呢？ 原文链接：https://www.toutiao.com/i6639660378177405453]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 数据库字符集 utf8 和 utf8mb4 的区别]]></title>
    <url>%2F2018%2F12%2F07%2Fdocs%2F04-mysql%2Fmysql-utf8-utf8mb4-diff%2F</url>
    <content type="text"><![CDATA[概述MySQL在5.5.3之后增加了这个utf8mb4的编码，mb4就是most bytes 4的意思，专门用来兼容四字节的unicode。好在utf8mb4是utf8的超集，除了将编码改为utf8mb4外不需要做其他转换。当然，为了节省空间，一般情况下使用utf8也就够了。 那上面说了既然utf8能够存下大部分中文汉字,那为什么还要使用utf8mb4呢? 原来mysql支持的 utf8 编码最大字符长度为 3 字节，如果遇到 4 字节的宽字符就会插入异常了。三个字节的 UTF-8 最大能编码的 Unicode 字符是 0xffff，也就是 Unicode 中的基本多文种平面(BMP)。也就是说，任何不在基本多文本平面的 Unicode字符，都无法使用 Mysql 的 utf8 字符集存储。包括 Emoji 表情(Emoji 是一种特殊的 Unicode 编码，常见于 ios 和 android 手机上)，和很多不常用的汉字，以及任何新增的 Unicode 字符等等。 问题来源最初的 UTF-8 格式使用一至六个字节，最大能编码 31 位字符。最新的 UTF-8 规范只使用一到四个字节，最大能编码21位，正好能够表示所有的 17个 Unicode 平面。 utf8 是 Mysql 中的一种字符集，只支持最长三个字节的 UTF-8字符，也就是 Unicode 中的基本多文本平面。 Mysql 中的 utf8 为什么只支持持最长三个字节的 UTF-8字符呢？我想了一下，可能是因为 Mysql 刚开始开发那会，Unicode 还没有辅助平面这一说呢。那时候，Unicode 委员会还做着 “65535 个字符足够全世界用了”的美梦。Mysql 中的字符串长度算的是字符数而非字节数，对于 CHAR 数据类型来说，需要为字符串保留足够的长。当使用 utf8 字符集时，需要保留的长度就是 utf8 最长字符长度乘以字符串长度，所以这里理所当然的限制了 utf8 最大长度为 3，比如 CHAR(100) Mysql 会保留 300字节长度。至于后续的版本为什么不对 4 字节长度的 UTF-8 字符提供支持，我想一个是为了向后兼容性的考虑，还有就是基本多文种平面之外的字符确实很少用到。 要在 Mysql 中保存 4 字节长度的 UTF-8 字符，需要使用 utf8mb4 字符集，但只有 5.5.3 版本以后的才支持(查看版本： select version()。我觉得，为了获取更好的兼容性，应该总是使用 utf8mb4 而非 utf8. 对于 CHAR 类型数据，utf8mb4 会多消耗一些空间，根据 Mysql 官方建议，使用 VARCHAR 替代 CHAR。 urf8mb4比utf8多了什么呢？ 多了emoji编码支持 如果实际用途上来看,可以给要用到emoji的库或者说表,设置utf8mb4比如评论要支持emoji可以用到建议普通表使用utf8 如果这个表需要支持emoji就使用utf8mb4 新建mysql库或者表的时候还有一个排序规则 utf8_unicode_ci比较准确，utf8_general_ci速度比较快。通常情况下 utf8_general_ci的准确性就够我们用的了，所以新建数据 库时一般选用utf8_general_ci就可以了 如果是utf8mb4那么对应的就是 utf8mb4_general_ci utf8mb4_unicode_ci 查看数据库及服务器的字符集/编码集情况可以使用如下sql查看数据库及服务器的字符集/编码集情况： 123456789101112131415161718192021222324mysql&gt; show variables like &apos;%character%&apos;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | latin1 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | latin1 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec)mysql&gt; show variables like &apos;%collation%&apos;;+----------------------+-------------------+| Variable_name | Value |+----------------------+-------------------+| collation_connection | utf8_general_ci || collation_database | latin1_swedish_ci || collation_server | latin1_swedish_ci |+----------------------+-------------------+3 rows in set (0.00 sec) 将数据库和已经建好的表也转换成utf8mb4更改数据库编码（字符集） 1ALTER DATABASE DATABASE_NAME DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 更改表编码(字符集)（注意：这里修改的是表的字符集，表里面字段的字符集并没有被修改） 1ALTER TABLE TABLE_NAME DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 更改表编码(字符集)和表中所有字段的编码(字符集) 1ALTER TABLE TABLE_NAME CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; 修改字符集为utf8mb4的sql语句session级别12345678910/*来自客户端的语句的字符集*/set character_set_client = utf8mb4;/*建立连接使用的字符集*/set character_set_connection = utf8mb4;/*默认数据库使用的字符集。当默认数据库更改时，服务器则设置该变量。如果没有默认数据库，变量的值同character_set_server*/set character_set_database = utf8mb4;/*用于向客户端返回查询结果的字符集*/set character_set_results = utf8mb4;/*服务器的默认字符集*/set character_set_server=utf8mb4; 修改配置文件/etc/my.cnf在以下三部分里添加如下内容： 123456789[client]default-character-set = utf8mb4[mysql]default-character-set = utf8mb4[mysqld]character-set-client-handshake = FALSEcharacter-set-server = utf8mb4collation-server = utf8mb4_unicode_ciinit_connect=&apos;SET NAMES utf8mb4&apos; 总之，utf8mb4 是目前最大的一个字符编码,支持任意文字，不过具体还是要根据项目情况来评估。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>utf8</tag>
        <tag>utf8mb4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vagrant+phpstorm+xdebug断点调试]]></title>
    <url>%2F2018%2F12%2F02%2Fdocs%2F02-tools%2Fxdebug-phpstorm-vagrant%2F</url>
    <content type="text"><![CDATA[安装 xdebug 扩展xdebug官网 安装1$ sudo pecl install xdebug xdebug 配置(remote为vagrant与宿主机默认网关)123456789101112zend_extension=xdebug.so[Xdebug]xdebug.remote_enable=1xdebug.remote_connect_back = 1xdebug.remote_port = 9000xdebug.max_nesting_level = 512xdebug.remote_autostart=1xdebug.idekey=phpstormxdebug.remote_host=10.0.2.2xdebug.remote_port=9000xdebug.remote_handler=dbgpxdebug.auto_trace = On PHPStorm 配置PHP 配置PHP CLI interptrter Debug 配置 DGBp proxy 配置 Servers 配置 配置完成，测试配置RUN -&gt; Web server debug vaildation -&gt;vaildate,有错误会提示，根据提示修改 配置完成，测试debugRUN-&gt;Start Listening for PHP debug connections-&gt;打断点-&gt;debug]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>xdebug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）单点登录（SSO）]]></title>
    <url>%2F2018%2F11%2F29%2Fdocs%2F09-pc-base%2Fsingle-sign-on%2F</url>
    <content type="text"><![CDATA[背景在企业发展初期，企业使用的系统很少，通常一个或者两个，每个系统都有自己的登录模块，运营人员每天用自己的账号登录，很方便。但随着企业的发展，用到的系统随之增多，运营人员在操作不同的系统时，需要多次登录，而且每个系统的账号都不一样，这对于运营人员来说，很不方便。于是，就想到是不是可以在一个系统登录，其他系统就不用登录了呢？这就是单点登录要解决的问题。 单点登录英文全称Single Sign On，简称就是SSO。它的解释是：在多个应用系统中，只需要登录一次，就可以访问其他相互信任的应用系统。 如图所示，图中有4个系统，分别是Application1、Application2、Application3、和SSO。Application1、Application2、Application3没有登录模块，而SSO只有登录模块，没有其他的业务模块，当Application1、Application2、Application3需要登录时，将跳到SSO系统，SSO系统完成登录，其他的应用系统也就随之登录了。这完全符合我们对单点登录（SSO）的定义。 技术实现在说单点登录（SSO）的技术实现之前，我们先说一说普通的登录认证机制。 如上图所示，我们在浏览器（Browser）中访问一个应用，这个应用需要登录，我们填写完用户名和密码后，完成登录认证。这时，我们在这个用户的session中标记登录状态为yes（已登录），同时在浏览器（Browser）中写入Cookie，这个Cookie是这个用户的唯一标识。下次我们再访问这个应用的时候，请求中会带上这个Cookie，服务端会根据这个Cookie找到对应的session，通过session来判断这个用户是否登录。如果不做特殊配置，这个Cookie的名字叫做jsessionid，值在服务端（server）是唯一的。 同域下的单点登录一个企业一般情况下只有一个域名，通过二级域名区分不同的系统。比如我们有个域名叫做：a.com，同时有两个业务系统分别为：app1.a.com和app2.a.com。我们要做单点登录（SSO），需要一个登录系统，叫做：sso.a.com 我们只要在sso.a.com登录，app1.a.com和app2.a.com就也登录了。通过上面的登陆认证机制，我们可以知道，在sso.a.com中登录了，其实是在sso.a.com的服务端的session中记录了登录状态，同时在浏览器端（Browser）的sso.a.com下写入了Cookie。那么我们怎么才能让app1.a.com和app2.a.com登录呢？这里有两个问题： Cookie是不能跨域的，我们Cookie的domain属性是sso.a.com，在给app1.a.com和app2.a.com发送请求是带不上的 sso、app1和app2是不同的应用，它们的session存在自己的应用内，是不共享的 那么我们如何解决这两个问题呢？针对第一个问题，sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。比如：我们不能在自己的系统中给baidu.com的域设置Cookie。 Cookie的问题解决了，我们再来看看session的问题。我们在sso系统登录了，这时再访问app1，Cookie也带到了app1的服务端（Server），app1的服务端怎么找到这个Cookie对应的Session呢？这里就要把3个系统的Session共享，如图所示。共享Session的解决方案有很多，例如：Spring-Session。这样第2个问题也解决了。 同域下的单点登录就实现了，但这还不是真正的单点登录。 不同域下的单点登录同域下的单点登录是巧用了Cookie顶域的特性。如果是不同域呢？不同域之间Cookie是不共享的，怎么办？ 这里我们就要说一说CAS流程了，这个流程是单点登录的标准流程。 上图是CAS官网上的标准流程，具体流程如下： 用户访问app系统，app系统是需要登录的，但用户现在没有登录。 跳转到CAS server，即SSO登录系统，以后图中的CAS Server我们统一叫做SSO系统。 SSO系统也没有登录，弹出用户登录页。 用户填写用户名、密码，SSO系统进行认证后，将登录状态写入SSO的session，浏览器（Browser）中写入SSO域下的Cookie。 SSO系统登录完成后会生成一个ST（Service Ticket），然后跳转到app系统，同时将ST作为参数传递给app系统。 app系统拿到ST后，从后台向SSO发送请求，验证ST是否有效。 验证通过后，app系统将登录状态写入session并设置app域下的Cookie。 至此，跨域单点登录就完成了。以后我们再访问app系统时，app就是登录的。接下来，我们再看看访问app2系统时的流程。 用户访问app2系统，app2系统没有登录，跳转到SSO。 由于SSO已经登录了，不需要重新登录认证。 SSO生成ST，浏览器跳转到app2系统，并将ST作为参数传递给app2。 app2拿到ST，后台访问SSO，验证ST是否有效。 验证成功后，app2将登录状态写入session，并在app2域下写入Cookie。 这样，app2系统不需要走登录流程，就已经是登录了。SSO，app和app2在不同的域，它们之间的session不共享也是没问题的。 有的同学问我，SSO系统登录后，跳回原业务系统时，带了个参数ST，业务系统还要拿ST再次访问SSO进行验证，觉得这个步骤有点多余。他想SSO登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？ 其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。 总结单点登录（SSO）的所有流程都介绍完了，原理大家都清楚了。总结一下单点登录要做的事情： 单点登录（SSO系统）是保障各业务系统的用户资源的安全 。 各个业务系统获得的信息是，这个用户能不能访问我的资源。 单点登录，资源都在各个业务系统这边，不在SSO那一方。 用户在给SSO服务器提供了用户名密码后，作为业务系统并不知道这件事。 SSO随便给业务系统一个ST，那么业务系统是不能确定这个ST是用户伪造的，还是真的有效，所以要拿着这个ST去SSO服务器再问一下，这个用户给我的ST是否有效，是有效的我才能让这个用户访问。 本文转载自：https://yq.aliyun.com/articles/636281]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>单点登录</tag>
        <tag>SSO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker+Nginx+Keepalived 实现高可用架构]]></title>
    <url>%2F2018%2F11%2F11%2Fdocs%2F06-Linux%2Flinux-keepalived-nginx-docker%2F</url>
    <content type="text"><![CDATA[概述这篇文章通过Docker+Keepalived模拟搭建一套web server层面的高可用系统。 Keepalived是基于vrrp协议的一款高可用软件。Keepailived有一台主服务器和多台备份服务器，在主服务器和备份服务器上面部署相同的服务配置，使用一个虚拟IP地址对外提供服务，当主服务器出现故障时，虚拟IP地址会自动漂移到备份服务器。 原理 在两台服务器上分别部署主备keepalived，主keepalived会在当前服务器配置漂移IP用于nginx对外提供服务 在两台服务器分别部署主备Nginx用于故障时切换 当nginx服务器挂掉后，主keepalived会降低当前机器权重，备keepalived服务器会把漂移IP抢过来配置在备服务器上，使备服务器上的nginx能接替工作继续对外提供服务 由于keepalived只能检测服务器是否宕机来实现故障自动切换，不能针对应用级别（nginx）的检测，因此，需要编写脚本实时监测nginx服务是否运行正常，当检测nginx运行不正常时就降低权重来实现故障自动切换 安装说明 安装基础镜像Centos 1$ sudo docker pull centos 说明：通过用centos镜像来安装高可用所需要的所有环境，再启两个容器，再真实模拟跨主机的场景 在centos安装所需环境（nginx和其它工具) 运行Centos容器，注意：启动容易的时候需要使用 --privileged 1$ sudo docker run --privileged -it centos 安装依赖和所需的包 1234567891011# 使用yum安装nginx需要包括Nginx的库，安装Nginx的库$ rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm# 使用下面命令安装nginx$ yum install nginx#安装网络包(需要使用ifconfig和ping命令)$ yum install net-tools#安装vim$ yum install vim 安装Keepalived 12345#安装keepalived环境依赖$ yum install -y gcc openssl-devel popt-devel#安装keepalived$ yum install keepalived 修改配置 修改 /etc/keepalived/keepalived.conf 文件 123456789101112131415161718192021222324252627282930313233343536373839! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; example@example.com &#125; notification_email_from itsection@example.com smtp_server mail.example.com smtp_connect_timeout 30 router_id LVS_DEVEL &#125; vrrp_script nginx_check &#123; script &quot;/etc/keepalived/nginx_check.sh&quot; interval 2 weight -5 fall 3 rise 2&#125; vrrp_instance VI_1 &#123; state MASTER interface eth0 virtual_router_id 2 priority 101 advert_int 2 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.17.0.100 &#125; track_script &#123; nginx_check &#125; &#125; 新建 /etc/keepalived/check_nginx.sh 文件 12345678910111213141516A=`ps -ef | grep nginx | grep -v grep | wc -l`if [ $A -eq 0 ];then nginx sleep 2 if [ `ps -ef | grep nginx | grep -v grep | wc -l` -eq 0 ];then #killall keepalived ps -ef|grep keepalived|grep -v grep|awk &apos;&#123;print $2&#125;&apos;|xargs kill -9 fi fi ``` 对 `check_nginx.sh` 赋于执行权限：```bash$ chmod +x /etc/keepalived/check_nginx.sh 注：keepalived是通过检测keepalived进程是否存在判断服务器是否宕机，如果keepalived进程在但是nginx进程不在了那么keepalived是不会做主备切换，所以我们需要写个脚本来监控nginx进程是否存在，如果nginx不存在就将keepalived进程杀掉。 在主nginx上需要编写nginx进程检测脚本（check_nginx.sh），判断nginx进程是否存在，如果nginx不存在就将keepalived进程杀掉，并将vip漂移到备份机器上 设置开机启动 1systemctl enable keepalived.service 安装好所有需要的依赖和环境后，将容器新增的内容重新提交 1$ sudo docker commit 704e0a8b19dc centos-keepalived-nginx:v1 704e0a8b19dc 为容器ID 可通过 docker ps -a 查看 启动keepalived_master容器 123$ sudo docker run --privileged -tid --name keepalived-master centos-keepalived-nginx:v1 /usr/sbin/init$sudo docker exec -it keepalived-master bash 进入/usr/share/nginx/html，修改index.html文件,修改标题为:Welcome to nginx Master! 启动keepalived-salve容器 123$ sudo docker run --privileged -tid --name keepalived-salve centos-keepalived-nginx:v1 /usr/sbin/init$ sudo docker exec -it keepalived-salve bash 进入/usr/share/nginx/html，修改index.html文件,修改标题为:Welcome to nginx Slave! 修改keepalived-salve容器中keepalived.conf文件 123456789101112131415161718192021222324252627282930313233343536373839! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; example@example.com &#125; notification_email_from itsection@example.com smtp_server mail.example.com smtp_connect_timeout 30 router_id LVS_DEVEL &#125; vrrp_script nginx_check &#123; script &quot;/etc/keepalived/nginx_check.sh&quot; interval 2 weight -5 fall 3 rise 2&#125; vrrp_instance VI_1 &#123; state BACKUP interface eth0 virtual_router_id 2 priority 100 advert_int 2 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.17.0.100 &#125; track_script &#123; nginx_check &#125; &#125; 其实，由配置中可以看出，主要是state和priority两个参数的调整，其中master节点的priority值一定要比backup大才行！ 原理说明： 通过vrrp协议广播，每个keepalived vrrp都去争取master 以virtual_router_id为组队标识。 同为一个vip服务的keepalived的virtual_router_id要保持相同 以priority 为权值，同一个virtual_router_id下那个priority大那个就是master,其它为backup 改完之后，重新加载 12$ systemctl daemon-reload $ systemctl restart keepalived.service 测试 启动nginx 1$ nginx 分别在Master和Salve容器中使用curl测试 1234567[root@d8aafedca447 /]# curl 172.17.0.100&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx Master!&lt;/title&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /&gt; 1234567[root@45bb50395bb0 /]# curl 172.17.0.100&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx Master!&lt;/title&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /&gt; 可以看现，此时master和slave容器两边通过虚拟vip : 172.17.0.100 访问nginx数据，请求返回的数据都是master容器中nginx配置的数据： welcome to nginx master 继续验证，关掉master容器的keepalived服务： 12345678[root@d8aafedca447 /]# systemctl stop keepalived.service[root@d8aafedca447 /]# curl 172.17.0.100&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx Slave!&lt;/title&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /&gt; 验证得到的结果是当master容器中的keepalived服务关掉后，curl 172.17.0.210请求返回的数据来自slave，welcome to nginx slave 再继续验证，把关掉master容器的keepalived服务再开启： 1234567[root@d8aafedca447 /]# curl 172.17.0.100&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx Master!&lt;/title&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /&gt; 可以看到，当master容器中的keepalived服务开启后，请求返回的数据会再次转到master中。 到此，所有的验证和预期的一致，也达到我们借助docker为基础来实现了整套基于Nginx+Keepalived高可用的方案了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 入门]]></title>
    <url>%2F2018%2F11%2F04%2Fdocs%2F02-tools%2Fdocker-get-started%2F</url>
    <content type="text"><![CDATA[概述Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 优点 简化程序： Docker 让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，便可以实现虚拟化。Docker改变了虚拟化的方式，使开发者可以直接将自己的成果放入Docker中进行管理。方便快捷已经是 Docker的最大优势，过去需要用数天乃至数周的 任务，在Docker容器的处理下，只需要数秒就能完成。 避免选择恐惧症： 如果你有选择恐惧症，还是资深患者。Docker 帮你 打包你的纠结！比如 Docker 镜像；Docker 镜像中包含了运行环境和配置，所以 Docker 可以简化部署多种应用实例工作。比如 Web 应用、后台应用、数据库应用、大数据应用比如 Hadoop 集群、消息队列等等都可以打包成一个镜像部署。 节省开支： 一方面，云计算时代到来，使开发者不必为了追求效果而配置高额的硬件，Docker 改变了高性能必然高价格的思维定势。Docker 与云的结合，让云空间得到更充分的利用。不仅解决了硬件管理的问题，也改变了虚拟化的方式。 应用场景 Web 应用的自动化打包和发布。 自动化测试和持续集成、发布。 在服务型环境中部署和调整数据库或其他的后台应用。 从头编译或者扩展现有的OpenShift或Cloud Foundry平台来搭建自己的PaaS环境。 架构Docker 使用客户端-服务器 (C/S) 架构模式，使用远程API来管理和创建Docker容器。Docker 容器通过 Docker 镜像来创建。容器与镜像的关系类似于面向对象编程中的对象与类。 Docker 面向 容器 对象 镜像 类 相关链接 http://www.docker.com https://github.com/docker/docker http://www.dockerinfo.net/document https://wiki.imooc.com/docker/introduce.html https://hub.docker.com Title Des Docker 镜像(Images) Docker 镜像是用于创建 Docker 容器的模板 Docker 容器(Container) 容器是独立运行的一个或一组应用 Docker 客户端(Client) Docker 客户端通过命令行或者其他工具使用 Docker API (https://docs.docker.com/reference/api/docker_remote_api) 与 Docker 的守护进程通信 Docker 主机(Host) 一个物理或者虚拟的机器用于执行 Docker 守护进程和容器 Docker 仓库(Registry) Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。Docker Hub(https://hub.docker.com) 提供了庞大的镜像集合供使用 Docker Machine Docker Machine是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、 Digital Ocean、Microsoft Azure 安装Ubuntu 安装首先要确认你的 Ubuntu 版本是否符合安装 Docker 的前提条件。如果没有问题，你可以通过下边的方式来安装 Docker ： 使用具有sudo权限的用户来登录你的Ubuntu。 查看你是否安装了wget 123456$ which wget如果`wget`没有安装，先升级包管理器，然后再安装它。$ sudo apt-get update $ sudo apt-get install wget 获取最新版本的 Docker 安装包 1$ wget -qO- https://get.docker.com/ | sh 验证 Docker 是否被正确的安装 1$ sudo docker run hello-world 上边的命令会下载一个测试镜像，并在容器内运行这个镜像。 CentOS 安装移除旧的版本 12345678910$ sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 安装一些必要的系统工具： 1$ sudo yum install -y yum-utils device-mapper-persistent-data lvm2 添加软件源信息： 1$ sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新 yum 缓存： 1$ sudo yum makecache fast 安装 Docker-ce： 1$ sudo yum -y install docker-ce 启动 Docker 后台服务 1$ sudo systemctl start docker 验证 Docker 是否被正确的安装 1$ sudo docker run hello-world 上边的命令会下载一个测试镜像，并在容器内运行这个镜像。 镜像加速鉴于国内网络问题，后续拉取 Docker 镜像十分缓慢，我们可以需要配置加速器来解决.通过修改daemon配置文件/etc/docker/daemon.json来使用加速器 12345678$ sudo mkdir -p /etc/docker$ sudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;https://tnxkcso1.mirror.aliyuncs.com&quot;]&#125;EOF$ sudo systemctl daemon-reload$ sudo systemctl restart docker 使用镜像在 Docker 的术语里，一个只读层被称为镜像，一个镜像是永久不会变的。 由于 Docker 使用一个统一文件系统，Docker 进程认为整个文件系统是以读写方式挂载的。 但是所有的变更都发生顶层的可写层，而下层的原始的只读镜像文件并未变化。由于镜像不 可写，所以镜像是无状态的。 每一个镜像都可能依赖于由一个或多个下层的组成的另一个镜像。我们有时说，下层那个 镜像是上层镜像的父镜像。 一个没有任何父镜像的镜像，谓之基础镜像。 列出本地镜像使用 docker images 可以列出本地主机上的镜像 1234567$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEphp 7.2 7cae463c729f 2 weeks ago 398MBnginx latest 540a289bab6c 2 weeks ago 126MBmysql 5.7 cd3ed0dfff7e 3 weeks ago 437MBubuntu 14.04 2c5e00d77a67 5 months ago 188MBhello-world latest fce289e99eb9 10 months ago 1.84kB 各个选项说明 REPOSTITORY：表示镜像的仓库源 TAG：镜像的标签 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 查找镜像我们可以从 Docker Hub 网站来搜索镜像，Docker Hub 网址为：https://hub.docker.com/我们也可以使用 docker search 命令来搜索镜像 1$ docker search ubuntu 获取镜像当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用 docker pull 命令来下载它。 123456789$ docker pull ubuntu:14.0414.04: Pulling from library/ubuntua7344f52cb74: Pull complete515c9bb51536: Pull completee1eabe0537eb: Pull complete4701f1215c13: Pull completeDigest: sha256:2f7c79927b346e436cc14c92bd4e5bd778c3bd7037f35bc639ac1589a7acfa90Status: Downloaded newer image for ubuntu:14.04docker.io/library/ubuntu:14.04 下载完成后，我们可以直接使用这个镜像来运行容器。 1$ docker run -t -i ubuntu:14.04 /bin/bash 创建镜像当我们从docker镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改。 从已经创建的容器中更新镜像，并且提交这个镜像 使用 Dockerfile 指令来创建一个新的镜像 更新镜像更新镜像之前，先使用下载的镜像启动容器。 12$ docker run -t -i ubuntu:14.04 /bin/bashroot@0bbebbd3e195:/# 在运行的容器内使用 apt-get update 命令进行更新。在完成操作之后，输入 exit 命令来退出这个容器。 此时ID为0bbebbd3e195的容器，是按我们的需求更改的容器。我们可以通过命令 docker commit 来提交容器副本。 12$ docker commit -m &quot;apt-get update&quot; -a &quot;Axkeson&quot; 0bbebbd3e195 ubuntu:devsha256:f7ce3235203891aec7a2c130a87844c0473e2d8af08750b3883284ae2aa51b53 各个参数说明： -m: 提交的描述信息 -a: 指定镜像作者 0bbebbd3e195: 容器ID ubuntu:dev: 指定要创建的目标镜像名 使用 docker images 来查看新创建的镜像 12345678$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEubuntu dev f7ce32352038 About a minute ago 188MBphp 7.2 7cae463c729f 2 weeks ago 398MBnginx latest 540a289bab6c 2 weeks ago 126MBmysql 5.7 cd3ed0dfff7e 3 weeks ago 437MBubuntu 14.04 2c5e00d77a67 5 months ago 188MBhello-world latest fce289e99eb9 10 months ago 1.84kB 之后，可以使用新的镜像来启动容器 12$ docker run -t -i ubuntu:dev /bin/bashroot@5c4cfc9e69fb:/# 使用 Dockerfile 构建镜像使用 docker commit 来扩展一个镜像比较简单，但是不方便在一个团队中分享。我们可以使用 docker build 来创建一个新的镜像。为此，首先需要创建一个 Dockerfile，包含一些如何创建镜像的指令 新建一个目录和一个 Dockerfile 123$ mkdir docker$ cd docker/$ touch Dockerfile Dockerfile 中每一条指令都创建镜像的一层,每一个指令的前缀都必须是大写的 12345# This is a commentFROM ubuntu:14.04MAINTAINER Docker Axkeson &lt;axkeson@docker.com&gt;RUN apt-get -qq updateEXPOSE 80 然后，我们使用 Dockerfile 文件，通过 docker build 命令来构建一个镜像 123456789101112131415161718$ docker build -t axkeson/ubuntu:dev .Sending build context to Docker daemon 2.048kBStep 1/4 : FROM ubuntu:14.04 ---&gt; 2c5e00d77a67Step 2/4 : MAINTAINER Docker Axkeson &lt;axkeson@docker.com&gt; ---&gt; Running in e59c86a66883Removing intermediate container e59c86a66883 ---&gt; a1ae3d914b38Step 3/4 : RUN apt-get -qq update ---&gt; Running in 0f613f00c78bRemoving intermediate container 0f613f00c78b ---&gt; 8ad734c56442Step 4/4 : EXPOSE 80 ---&gt; Running in 262038d95663Removing intermediate container 262038d95663 ---&gt; a3d12b32a13dSuccessfully built a3d12b32a13dSuccessfully tagged axkeson/ubuntu:dev 参数说明： -t ：指定要创建的目标镜像名 . ：Dockerfile 文件所在目录，可以指定Dockerfile 的绝对路径 使用 docker images 查看创建的镜像已经在列表中存在,镜像ID为a3d12b32a13d,我们可以使用新的镜像来创建容器 1234567891011$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEaxkeson/ubuntu dev a3d12b32a13d 3 minutes ago 202MBubuntu dev f7ce32352038 22 minutes ago 188MBphp 7.2 7cae463c729f 2 weeks ago 398MBnginx latest 540a289bab6c 2 weeks ago 126MBmysql 5.7 cd3ed0dfff7e 3 weeks ago 437MBubuntu 14.04 2c5e00d77a67 5 months ago 188MBhello-world latest fce289e99eb9 10 months ago 1.84kB$ docker run -t -i axkeson/ubuntu:dev /bin/bashroot@56352e5c35bc:/# Dockerfile 基本的语法是 使用#来注释 FROM 指令告诉 Docker 使用哪个镜像作为基础 MAINTAINER 是维护者的信息 RUN开头的指令会在创建中运行，比如安装一个软件包，在这里使用 apt-get 来安装了一些软件 EXPOSE 指令来向外部开放端口 CMD 指令来描述容器启动后运行的程序等 上传镜像可以通过 docker push 命令，把自己创建的镜像上传到仓库中来共享。例如，用户在 Docker Hub 上完成注册后，可以推送自己的镜像到仓库中 1234$ sudo docker push axkeson/ubuntuThe push refers to a repository [axkeson/ubuntu] (len: 1)Sending image listPushing repository axkeson/ubuntu (3 tags) 移除本地镜像如果要移除本地的镜像，可以使用 docker rmi 命令。注意 docker rm 命令是移除容器 1234$ docker rmi ubuntu:devUntagged: ubuntu:devDeleted: sha256:f7ce3235203891aec7a2c130a87844c0473e2d8af08750b3883284ae2aa51b53Deleted: sha256:fc8ca62746e4e5f5e4eebed68ef9b89fa4648f8194b03cc81fa6d19ff06d43de 导出镜像如果要导出镜像到本地文件，可以使用 docker save 命令 123456789$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEaxkeson/ubuntu dev a3d12b32a13d 28 minutes ago 202MBphp 7.2 7cae463c729f 2 weeks ago 398MBnginx latest 540a289bab6c 2 weeks ago 126MBmysql 5.7 cd3ed0dfff7e 3 weeks ago 437MBubuntu 14.04 2c5e00d77a67 5 months ago 188MBhello-world latest fce289e99eb9 10 months ago 1.84kB$ docker save -o ubuntu_14.04.tar axkeson/ubuntu:dev 载入镜像可以使用 docker load 从导出的本地文件中再导入到本地镜像库 12345$ docker load --input ubuntu_14.04.tarLoaded image: axkeson/ubuntu:dev或者$ docker load &lt; ubuntu_14.04.tarLoaded image: axkeson/ubuntu:dev 容器启动容器基于镜像ubuntu:14.04新建一个容器并启动 下面的命令输出一个 “Hello World”，之后终止容器。 12$ docker run ubuntu:14.04 /bin/echo &quot;Hello world&quot;Hello world 下面的命令则启动一个 bash 终端，允许用户进行交互。 123456$ docker run -t -i ubuntu:14.04 /bin/bashroot@c487045a4dcc:/#root@c487045a4dcc:/# pwd/root@c487045a4dcc:/# lltotal 72 参数说明: -t:让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上 -i:让容器的标准输入保持打开 -d:容器启动后会进入后台。 某些时候需要进入容器进行操作，有很多种方法，包括使用docker attach 命令或 nsenter 工具等 查看容器使用 docker ps 可以查看我们正在运行的容器 12345$ sudo docker run -d ubuntu:14.04 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;ba4bed03628aa8092feb0378345050f401089b8b9e04327c95012fdff48dd166$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba4bed03628a ubuntu:14.04 &quot;/bin/sh -c &apos;while t…&quot; 3 seconds ago Up 2 seconds eager_solomon 进入容器在使用 -d 参数时，容器启动后会进入后台。 某些时候需要进入容器进行操作，有很多种方法，包括使用docker attach 命令或 nsenter 工具等 attach 命令docker attach 是Docker自带的命令。下面示例如何使用该命令 1234567$ docker run -idt ubuntu15e22b1c1e5345aa848a599f3ff12a9abdcaa8f294bff77ddef816ac2d398772$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES15e22b1c1e53 ubuntu &quot;/bin/bash&quot; 6 seconds ago Up 5 seconds thirsty_almeida$ docker attach 15e22b1c1e53root@15e22b1c1e53:/# 但是使用 attach 命令有时候并不方便。当多个窗口同时 attach 到同一个容器的时候，所有窗口都会同步显示。当某个窗口因命令阻塞时,其他窗口也无法执行操作了 nsenter 命令TODO 终止容器可以使用 docker stop 来终止一个运行中的容器,此外，当Docker容器中指定的应用终结时，容器也自动终止。用户通过 exit 命令或 Ctrl+d 来退出终端时，所创建的容器立刻终止。终止状态的容器可以用 docker ps -a 命令看到。 处于终止状态的容器，可以通过 docker start 命令来重新启动。 此外，docker restart 命令会将一个运行态的容器终止，然后再重新启动它。 123$ docker stop 206d3189e0b5$ docker start 206d3189e0b5$ docker restart 206d3189e0b5 移除容器可以使用 docker rm 来删除一个处于终止状态的容器,删除容器时，容器必须是停止状态 1$ docker rm ba4bed03628a Volumen数据卷是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 数据卷可以在容器之间共享和重用 对数据卷的修改会立马生效 对数据卷的更新，不会影响镜像 卷会一直存在，直到没有容器使用 创建一个数据卷在用 docker run 命令的时候，使用 -v 标记来创建一个数据卷并挂载到容器里。在一次 run 中多次使用可以挂载多个数据卷。 下面创建一个 web 容器，并加载一个数据卷到容器的 /webapp 目录。 1$ docker run -d -P --name web -v /webapp training/webapp python app.py 注意：也可以在 Dockerfile 中使用 VOLUME 来添加一个或者多个新的卷到由该镜像创建的任意容器 挂载一个主机目录作为数据卷使用 -v 标记也可以指定挂载一个本地主机的目录到容器中去。 1$ docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py 上面的命令加载主机的 /src/webapp 目录到容器的 /opt/webapp 目录。这个功能在进行测试的时候十分方便，比如用户可以放置一些程序到本地目录中，来查看容器是否正常工作。本地目录的路径必须是绝对路径，如果目录不存在 Docker 会自动为你创建它。 *注意：Dockerfile 中不支持这种用法，这是因为 Dockerfile 是为了移植和分享用的。然而，不同操作系统的路径格式不一样，所以目前还不能支持。 Docker 挂载数据卷的默认权限是读写，用户也可以通过 :ro 指定为只读。 1$ docker run -d -P --name web -v /src/webapp:/opt/webapp:ro training/webapp python app.py 挂载一个本地主机文件作为数据卷-v 标记也可以从主机挂载单个文件到容器中 1$ docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash 这样就可以记录在容器输入过的命令了。 *注意：如果直接挂载一个文件，很多文件编辑工具，包括 vi 或者 sed –in-place，可能会造成文件 inode 的改变，从 Docker 1.1 .0起，这会导致报错误信息。所以最简单的办法就直接挂载文件的父目录]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Supervisord 管理服务进程]]></title>
    <url>%2F2018%2F07%2F31%2Fdocs%2F06-Linux%2Flinux-supervisord%2F</url>
    <content type="text"><![CDATA[简介在开发中我们通过端口号启动多个网站服务和接口服务，网关服务，如果不使用工具管理这些服务进程，每次开机时都重新启动一次，将浪费我们大量时间。 安装1$ sudo apt install supervisor 配置默认supervisor配置目录为/etc/supervisor/conf.d。为方便操作，这里我们使用自己的配置目录。 1$ mkdir -p supervisor/&#123;conf,logs,run&#125; 添加配置文件supervisor.conf: 12345678910111213141516171819202122[unix_http_server]file=/path/to/your/supervisor/run/supervisor.sock[supervisord]logfile=/path/to/your/supervisor/logs/supervisor.loglogfile_maxbytes=50MBlogfile_backups=10loglevel=infopidfile=/path/to/your/supervisor/run/supervisor.pidnodaemon=falseminfds=1024minprocs=200childlogdir=/path/to/your/supervisor/logs[rpcinterface:supervisor]supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface[supervisorctl]serverurl=unix:///path/to/your/supervisor/run/supervisor.sock[include]files = conf/*.conf 修改配置文件中 /path/to/your/supervisor 为当前目录绝对路径。 启动 supervisor 服务： 1$ supervisord -n -c supervisor.conf 配置与管理进程再启动一个终端，运行命令： 1$ supervisorctl -c supervisor.conf 输入 help 命令查看所有命令： 1234567supervisor&gt; helpdefault commands (type help &lt;topic&gt;):=====================================add exit open reload restart start tail avail fg pid remove shutdown status update clear maintail quit reread signal stop version 运行 status 查看所有管理进程，目前没有配置，所以没有输出。 添加配置 conf/web-test.conf : 123[program:web-test]directory=/path/to/your/web-test/publiccommand=php -S 0:8099 index.php 修改 /path/to/your/web-test 为实际项目地址。 执行 update 命令可以看到配置已经加载： 1234supervisor&gt; updatepet-clinic-web: added process groupsupervisor&gt; statuspet-clinic-web RUNNING pid 5958, uptime 0:00:01 打开网站看看服务是否正确。 开机启动在 /etc/systemd/system 目录中添加我们自己的 supervisor 开机启动配置 my-supervisord.service : 12345678910[Unit]Description=My Supervisord Daemon[Service]ExecStart=/usr/bin/supervisord -n -c /path/to/your/supervisor.confUser=your-usernameGroup=your-username[Install]WantedBy=multi-user.target 替换上面配置文件中路径和用户名。启用开机启动： 123systemctl daemon-reloadsystemctl start my-supervisordsystemctl enable my-supervisord 别名supervisorctl 管理命令需要指定文件名启动，可以通过在 ~/.bashrc 中配置别名： 123function sv &#123; supervisorctl -c /path/to/your/supervisor.conf $@&#125; 现在在终端使用 sv 命令就可以进入管理命令行。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Supervisord</tag>
        <tag>服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWK 简明教程]]></title>
    <url>%2F2018%2F07%2F23%2Fdocs%2F06-Linux%2Flinux-awk%2F</url>
    <content type="text"><![CDATA[简介awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。 awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 使用方法1234$ awk '&#123;pattern + action&#125;' &#123;filenames&#125;# 示例$ awk '&#123;print $0&#125;' demo.txt 尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。 awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。 通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 内键变量 变量 描述 $n 当前记录的第n个字段，字段间由FS分隔 $0 完整的输入记录 ARGC 命令行参数的数目 ARGIND 命令行中当前文件的位置(从0开始算) ARGV 包含命令行参数的数组 CONVFMT 数字转换格式(默认值为%.6g)ENVIRON环境变量关联数组 ERRNO 最后一个系统错误的描述 FIELDWIDTHS 字段宽度列表(用空格键分隔) FILENAME 当前文件名 FNR 各文件分别计数的行号 FS 字段分隔符(默认是任何空格) IGNORECASE 如果为真，则进行忽略大小写的匹配 NF 一条记录的字段的数目 NR 已经读出的记录数，就是行号，从1开始 OFMT 数字的输出格式(默认值是%.6g) OFS 输出记录分隔符（输出换行符），输出时用指定的符号代替换行符 ORS 输出记录分隔符(默认值是一个换行符) RLENGTH 由match函数所匹配的字符串的长度 RS 记录分隔符(默认是一个换行符) RSTART 由match函数所匹配的字符串的第一个位置 SUBSEP 数组下标分隔符(默认值是/034) 运算符 运算符 描述 = += -= = /= %= ^= *= 赋值 ?: C条件表达式 &amp;&amp; 逻辑与 ~ ~! 匹配正则表达式和不匹配正则表达式 &lt; &lt;= &gt; &gt;= != == 关系运算符 空格 连接 + - 加，减 * / % 乘，除与求余 + - ! 一元加，减和逻辑非 ^ *** 求幂 ++ – 增加或减少，作为前缀或后缀 $ 字段引用 in 数组成员 入门实例Linux下查看使用频率最高的十个命令123456789101112$ history | awk &apos;&#123;CMD[$2]++;count++;&#125; END &#123; for (a in CMD )print CMD[ a ]&quot; &quot; CMD[ a ]/count*100 &quot;% &quot; a &#125;&apos; | grep -v &quot;./&quot; | column -c3 -s &quot; &quot; -t |sort -nr | nl | head -n10 1 43 22.0513% ll 2 41 21.0256% cd 3 24 12.3077% php 4 15 7.69231% sudo 5 8 4.10256% nginx 6 6 3.07692% vim 7 6 3.07692% find 8 5 2.5641% logout 9 4 2.05128% pwd 10 4 2.05128% ngixn 查看有多少个IP访问1$ awk &apos;&#123;print $1&#125;&apos; log_file|sort|uniq|wc -l 将每个IP访问的页面数进行从小到大排序1$ awk &apos;&#123;++S[$1]&#125; END &#123;for (a in S) print S[a],a&#125;&apos; log_file | sort -n 查看某一个IP访问了哪些页面1$ grep ^111.111.111.111 log_file| awk &apos;&#123;print $1,$7&#125;&apos; 相关文章 https://segmentfault.com/a/1190000009745139http://wiki.jikexueyuan.com/project/awk]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>awk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TODO Nginx+php-fpm 运行原理]]></title>
    <url>%2F2018%2F07%2F20%2Fdocs%2F06-Linux%2Fphp-nginx-run-way%2F</url>
    <content type="text"><![CDATA[前言随着互联网的发展，用户对此接受面广，数据流的增大使得Web端的运行承载压力日益增大,野蛮生长在大数据时代里的WEB语言PHP也找到了比老搭档更优越的活力搭档Nginx. Nginx 是什么Nginx (“engine x”) 是一个高性能的HTTP和反向代理服务器，也是一个IMAP/POP3/SMTP服务器。 php-fpm 是什么 php-fpm即php-Fastcgi Process Manager. php-fpm是 FastCGI 的实现，并提供了进程管理的功能 进程包含 master 进程和 worker 进程两种进程。master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个(具体数量根据实际需要配置)，每个进程内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方。 要明白nginx 与 php-fpm是如何协同工作的,那么首先要明白CGI (Common Gateway Interface) 和 FastCGI 这两个协议 CGI与fastcgiCGI 是 Web Server 与后台语言交互的协议，有了这个协议，开发者可以使用任何语言处理 Web Server 发来的请求，动态的生成内容。但 CGI 有一个致命的缺点，那就是每处理一个请求都需要 fork 一个全新的进程，随着 Web 的兴起，高并发越来越成为常态，这样低效的方式明显不能满足需求。就这样，FastCGI 诞生了，CGI 很快就退出了历史的舞台。 FastCGI，顾名思义为更快的CGI, 它允许在一个进程内处理多个请求，而不是一个请求处理完毕就直接结束进程，性能上有了很大的提高。 FastCGI是一个可伸缩地、高速地在HTTP server和动态脚本语言间通信的接口。多数流行的HTTP server都支持FastCGI，包括Apache、Nginx和lighttpd等。同时，FastCGI也被许多脚本语言支持，其中就有PHP FastCGI接口方式采用C/S结构，可以将HTTP服务器和脚本解析服务器分开，同时在脚本解析服务器上启动一个或者多个脚本解析守护进程。当HTTP服务器每次遇到动态程序时，可以将其直接交付给FastCGI进程来执行，然后将得到的结果返回给浏览器。这种方式可以让HTTP服务器[如nginx]专一地处理静态请求或者将动态脚本服务器的结果返回给客户端，这在很大程度上提高了整个应用系统的性能。 至于 FPM (FastCGI Process Manager)，它是 FastCGI 的实现，任何实现了 FastCGI 协议的 Web Server 都能够与之通信。FPM 之于标准的 FastCGI，也提供了一些增强功能. FPM 是一个 PHP 进程管理器，包含 master 进程和 worker 进程两种进程：master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个 (具体数量根据实际需要配置)，每个进程内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>nginx</tag>
        <tag>PHP-FPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PsySh PHP交互控制台]]></title>
    <url>%2F2018%2F07%2F17%2Fdocs%2F02-tools%2Fpsysh%2F</url>
    <content type="text"><![CDATA[简介psysh是一个PHP的运行时开发平台，交互式调试器和Read-Eval-Print Loop (REPL)。说的简单点,就跟你用Chrome的时候firebug的console调试你的JavaScript代码一样。 官网 GitHub Packagist 安装直接下载123$ wget https://git.io/psysh$ chmod +x psysh$ ./psysh 使用Composer安装12$ composer g require psy/psysh:@stable$ psysh 以下教程以OS X和Windows为例，在这之前您已经将安装了php和composer，并且把加入了环境变量 OS x1. 下载12$ composer global require psy/psysh` 2. 安装完毕后，PsySH已经安装到/Users/{用户名}/.composer/vendor/psy/psysh目录下,这个时候你可以这样来直接运行1$ /Users/&#123;用户名&#125;/.composer/vendor/psy/psysh/bin/psysh 3. 为了使用方便，建议将它加入到环境变量：12$ echo &apos;export PATH=&quot;/Users/&#123;用户名&#125;/.composer/vendor/psy/psysh/bin:$PATH&quot;&apos; &gt;&gt; ~/.bashrc$ source ~/.bashrc Windows1. 我们还是用的composer来安装，win+r召唤控制台，然后1composer global require psy/psysh 2. 安装完成后，PsySH被安装到C:Users{用户名}AppDataRoamingComposervendorpsypsysh因为bin/psysh文件并不是windows的可执行文件，所以需要使用以下命令运行PsySH 1php C:\Users\&#123;用户名&#125;\AppData\Roaming\Composer\vendor\psy\psysh\bin\psysh 3. 为了使用方便，在C:Users{用户名}AppDataRoamingComposervendorpsypsyshbin目录下新建一个名为psysh.bat的文件，其内容如下12@ECHO OFFphp &quot;%~dp0psysh&quot; %* 4. 此时，把C:Users{用户名}A ppDataRoamingComposervendorpsypsyshbin 加入到系统的环境变量PATH，以后可以直接在cmd下运行psysh了123C:\Users\Vergil&gt;psyshPsy Shell v0.6.1 (PHP 5.6.8 — cli) by Justin Hileman&gt;&gt;&gt; 神器特性psysh是一个交互式的PHP运行控制台，在这里，你可以写php代码运行，并且可以清楚看到每次的返回值： 能够很智能的知道你的代码是否已经结束 自动完成，psysh可以像控制台那样，按下两次[tab]键自动补全，帮你自动完成变量名，函数，类，方法，属性，甚至是文件 文档在运行时忘记参数怎么办？psysh的文档功能可以上你及时查看文档。 PsySH的文档存放在~/.local/share/psysh/。（windows系统存放在C:\Users\{用户名}\AppData\Roaming\PsySH\） 下载中文文档： 1234$ cd ~/.local/share $ mkdir psysh$ cd psydh$ wget http://psysh.org/manual/zh/php_manual.sqlite OK，完成后重新打开PsySH 查看源代码轻松展现任何用户级的对象，类，接口，特质，常数，方法或属性的源代码： 反射列表list命令知道所有关于你的代码 - 和其他人的。轻松地列出并搜索所有的变量，常量，类，接口，特点，功能，方法和属性。 获取最后的异常信息如果忘记catch异常，可以使用wtf命令（wtf是what the fuck的意思么？）查看异常的信息： 历史记录可以像类Unix系统的history命令一样，在PsySH可以查看你运行过的PHP代码或命令。详情运行help history命令查看。 退出使用exit命令退出你的PsySH]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>psysh</tag>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 关于 self 和 static]]></title>
    <url>%2F2018%2F06%2F22%2Fdocs%2F03-php%2Fphp-late-static-bindings%2F</url>
    <content type="text"><![CDATA[实例self:: 的限制使用 self:: 或者 __CLASS__ 对当前类的静态引用，取决于定义当前方法所在的类： self:: 用法 123456789101112131415161718192021&lt;?phpclass A &#123; public static function who() &#123; echo __CLASS__; &#125; public static function test() &#123; self::who(); &#125;&#125;class B extends A &#123; public static function who() &#123; echo __CLASS__; &#125;&#125;B::test();// 以上例程会输出：A 后期静态绑定的用法后期静态绑定本想通过引入一个新的关键字表示运行时最初调用的类来绕过限制。简单地说，这个关键字能够让你在上述例子中调用 test() 时引用的类是 B 而不是 A。最终决定不引入新的关键字，而是使用已经预留的 static 关键字。 static:: 简单用法 123456789101112131415161718192021&lt;?phpclass A &#123; public static function who() &#123; echo __CLASS__; &#125; public static function test() &#123; static::who(); // 后期静态绑定从这里开始 &#125;&#125;class B extends A &#123; public static function who() &#123; echo __CLASS__; &#125;&#125;B::test();// 以上例程会输出：B 在非静态环境下，所调用的类即为该对象实例所属的类。由于 $this-&gt; 会在同一作用范围内尝试调用私有方法，而 static:: 则可能给出不同结果。另一个区别是 static:: 只能用于静态属性。 非静态环境下使用 static:: 12345678910111213141516171819202122232425262728293031323334&lt;?phpclass A &#123; private function foo() &#123; echo "success!\n"; &#125; public function test() &#123; $this-&gt;foo(); static::foo(); &#125;&#125;class B extends A &#123; /* foo() will be copied to B, hence its scope will still be A and * the call be successful */&#125;class C extends A &#123; private function foo() &#123; /* original method is replaced; the scope of the new one is C */ &#125;&#125;$b = new B();$b-&gt;test();$c = new C();$c-&gt;test(); //fails// 以上例程会输出：success!success!success!Fatal error: Call to private method C::foo() from context 'A' in /tmp/test.php on line 9 后期静态绑定的解析会一直到取得一个完全解析了的静态调用为止。另一方面，如果静态调用使用 parent:: 或者 self:: 将转发调用信息。 转发和非转发调用 12345678910111213141516171819202122232425262728293031323334&lt;?phpclass A &#123; public static function foo() &#123; static::who(); &#125; public static function who() &#123; echo __CLASS__."\n"; &#125;&#125;class B extends A &#123; public static function test() &#123; A::foo(); parent::foo(); self::foo(); &#125; public static function who() &#123; echo __CLASS__."\n"; &#125;&#125;class C extends B &#123; public static function who() &#123; echo __CLASS__."\n"; &#125;&#125;C::test();// 以上例程会输出：ACC 总结 self 和 __CLASS__，都是对当前类的静态引用，取决于定义当前方法所在的类。也就是说，self 写在哪个类里面， 它引用的就是谁。 $this 指向的是实际调用时的对象，也就是说，实际运行过程中，谁调用了类的属性或方法，$this 指向的就是哪个对象。但 $this 不能访问类的静态属性和常量，且 $this 不能存在于静态方法中。 static 关键字除了可以声明类的静态成员（属性和方法）外，还有一个非常重要的作用就是后期静态绑定。 self 可以用于访问类的静态属性、静态方法和常量，但 self 指向的是当前定义所在的类，这是 self 的限制。 $this 指向的对象所属的类和 static 指向的类相同。 static 可以用于静态或非静态方法中，也可以访问类的静态属性、静态方法、常量和非静态方法，但不能访问非静态属性。 静态调用时，static 指向的是实际调用时的类；非静态调用时，static 指向的是实际调用时的对象所属的类 相关文章 https://www.php.net/manual/zh/language.oop5.late-static-bindings.phphttps://blog.csdn.net/lamp_yang_3533/article/details/79912453]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>self</tag>
        <tag>static</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）HTTP和HTTPS的区别与联系]]></title>
    <url>%2F2018%2F05%2F13%2Fdocs%2F09-pc-base%2Fnetwork-http-https%2F</url>
    <content type="text"><![CDATA[超文本传输协议HTTP协议被用于在Web浏览器和网站服务器之间传递信息，HTTP协议以明文方式发送内容，不提供任何方式的数据加密，如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息，因此，HTTP协议不适合传输一些敏感信息，比如：信用卡号、密码等支付信息。 为了解决HTTP协议的这一缺陷，需要使用另一种协议：安全套接字层超文本传输协议HTTPS，为了数据传输的安全，HTTPS在HTTP的基础上加入了SSL协议，SSL依靠证书来验证服务器的身份，并为浏览器和服务器之间的通信加密。 HTTP和HTTPS的基本概念HTTP：是互联网上应用最为广泛的一种网络协议，是一个客户端和服务器端请求和应答的标准（TCP），用于从WWW服务器传输超文本到本地浏览器的传输协议，它可以使浏览器更加高效，使网络传输减少。 HTTPS：是以安全为目标的HTTP通道，简单讲是HTTP的安全版，即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL。 HTTPS协议的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 HTTP与HTTPS有什么区别HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，为了保证这些隐私数据能加密传输，于是网景公司设计了SSL（Secure Sockets Layer）协议用于对HTTP协议传输的数据进行加密，从而就诞生了HTTPS。 简单来说，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。 HTTPS和HTTP的区别主要如下： https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。 http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 HTTPS的工作原理客户端发起HTTPS请求这个没什么好说的，就是用户在浏览器里输入一个https网址，然后连接到server的443端口。 服务端的配置采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请，区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面(startssl就是个不错的选择，有1年的免费服务)。 这套证书其实就是一对公钥和私钥，如果对公钥和私钥不太理解，可以想象成一把钥匙和一个锁头，只是全世界只有你一个人有这把钥匙，你可以把锁头给别人，别人可以用这个锁把重要的东西锁起来，然后发给你，因为只有你一个人有这把钥匙，所以只有你才能看到被这把锁锁起来的东西。 传送证书这个证书其实就是公钥，只是包含了很多信息，如证书的颁发机构，过期时间等等。 客户端解析证书这部分工作是有客户端的TLS来完成的，首先会验证公钥是否有效，比如颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。 如果证书没有问题，那么就生成一个随机值，然后用证书对该随机值进行加密，就好像上面说的，把随机值用锁头锁起来，这样除非有钥匙，不然看不到被锁住的内容。 传送加密信息这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了。 服务段解密信息服务端用私钥解密后，得到了客户端传过来的随机值(私钥)，然后把内容通过该值进行对称加密，所谓对称加密就是，将信息和私钥通过某种算法混合在一起，这样除非知道私钥，不然无法获取内容，而正好客户端和服务端都知道这个私钥，所以只要加密算法够彪悍，私钥够复杂，数据就够安全。 传输加密后的信息这部分信息是服务段用私钥加密后的信息，可以在客户端被还原。 客户端解密信息客户端用之前生成的私钥解密服务段传过来的信息，于是获取了解密后的内容，整个过程第三方即使监听到了数据，也束手无策。 HTTPS的优点正是由于HTTPS非常的安全，攻击者无法从中找到下手的地方，从站长的角度来说，HTTPS的优点有以下2点： SEO方面谷歌曾在2014年8月份调整搜索引擎算法，并称“比起同等HTTP网站，采用HTTPS加密的网站在搜索结果中的排名将会更高”。 安全性尽管HTTPS并非绝对安全，掌握根证书的机构、掌握加密算法的组织同样可以进行中间人形式的攻击，但HTTPS仍是现行架构下最安全的解决方案，主要有以下几个好处： 使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器； HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。 HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。 HTTPS的缺点虽然说HTTPS有很大的优势，但其相对来说，还是有些不足之处的，具体来说，有以下2点： SEO方面据ACM CoNEXT数据显示，使用HTTPS协议会使页面的加载时间延长近50%，增加10%到20%的耗电，此外，HTTPS协议还会影响缓存，增加数据开销和功耗，甚至已有安全措施也会受到影响也会因此而受到影响。 而且HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。 最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。 经济方面 SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。 SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗（SSL有扩展可以部分解决这个问题，但是比较麻烦，而且要求浏览器、操作系统支持，Windows XP就不支持这个扩展，考虑到XP的装机量，这个特性几乎没用）。 HTTPS连接缓存不如HTTP高效，大流量网站如非必要也不会采用，流量成本太高。 HTTPS连接服务器端资源占用高很多，支持访客稍多的网站需要投入更大的成本，如果全部采用HTTPS，基于大部分计算资源闲置的假设的VPS的平均成本会上去。 HTTPS协议握手阶段比较费时，对网站的相应速度有负面影响，如非必要，没有理由牺牲用户体验。 本文转载自 https://blog.csdn.net/xionghuixionghui/article/details/68569282 相关文章 https://www.runoob.com/w3cnote/http-vs-https.htmlhttps://www.jianshu.com/p/37654eb66b58]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TODO 从输入 URL 到页面加载完成的过程中都发生了什么事情？]]></title>
    <url>%2F2018%2F05%2F12%2Fdocs%2F09-pc-base%2Fwhenyouenteraurl%2F</url>
    <content type="text"><![CDATA[背景具体过程DNS 解析DNS（Domain Name System，域名系统）解析DNS解析的过程就是寻找哪台机器上有你需要资源的过程。当你在浏览器中输入一个地址时，例如www.baidu.com，其实不是百度网站真正意义上的地址。互联网上每一台计算机的唯一标识是它的IP地址，但是IP地址并不方便记忆。用户更喜欢用方便记忆的网址去寻找互联网上的其它计算机，也就是上面提到的百度的网址。所以互联网设计者需要在用户的方便性与可用性方面做一个权衡，这个权衡就是一个网址到IP地址的转换，这个过程就是DNS解析。它实际上充当了一个翻译的角色，实现了网址到IP地址的转换。网址到IP地址转换的过程是如何进行的? TCP 连接发送HTTP请求服务器处理请求并返回HTTP报文浏览器解析渲染页面连接结束相关文档 https://dailc.github.io/2018/03/12/whenyouenteraurl.htmlhttp://fex.baidu.com/blog/2014/05/what-happen]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）CentOS7.4搭建shadowsocks，以及配置BBR加速]]></title>
    <url>%2F2018%2F04%2F13%2Fdocs%2F02-tools%2Fshadowsocks-setting%2F</url>
    <content type="text"><![CDATA[前言作为一个新世纪的码农，我们经常需要使用百度以及Google等搜索引擎搜索资料或搜索一些错误的解决方案，如果English好的还可能需要到stackoverflow里查看或提问一些开发中遇到的问题，再者可能还需要到youtube上查找一下教学、科普视频等等。还好的是stackoverflow部分不牵扯Google的内容在国内还是能够正常访问的，但是Google和youtube嘛大家都懂，所以本文就介绍一下如何在vps上搭建shadowsocks，让我们能够访问这些网站，以便于我门查阅资料，切勿用做其他不法用途。 常见VPS的购买地址活跃于大街小巷的搬瓦工，也是适合新手使用的： https://bwh1.net/ vultr https://www.vultr.com/?ref=7315390 SugarHosts https://www.sugarhosts.com/zh-cn/ Linode https://www.linode.com/ Virmach https://billing.virmach.com/cart.php?gid=1 RAKSmart https://billing.raksmart.com/ Bluehost https://cn.bluehost.com/ DigitalOcean https://www.digitalocean.com 以上这些都是国外的vps，国内的可以购买阿里云或者腾讯云等，国内没有遇到优惠的话就比较贵。 安装 pippip是python的包管理工具。在本文中将使用python版本的shadowsocks，此版本的shadowsocks已发不到pip上，因此我们需要通过pip命令来安装。 在控制台执行以下命令安装 pip： 12$ curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"$ python get-pip.py 安装配置 shadowsocks在控制台执行以下命令安装shadowsocks: 12$ pip install --upgrade pip$ pip install shadowsocks 安装完成后，需要创建shadowsocks的配置文件/etc/shadowsocks.json，编辑内容如下： 12345678910111213$ vim /etc/shadowsocks.json&#123; "server": "0.0.0.0", "local_address": "127.0.0.1", "local_port": 1080, "port_password": &#123; "8080": "填写密码", "8081": "填写密码" &#125;, "timeout": 600, "method": "aes-256-cfb"&#125; 说明 method为加密方法，可选aes-128-cfb,aes-192-cfb,aes-256-cfb,bf-cfb,cast5-cfb,des-cfb,rc4-md5,chacha20,rc4,table port_password为端口对应的密码，可使用密码生成工具生成一个随机密码 以上两项信息在配置shadowsocks客户端时需要配置一致，具体说明可查看shadowsocks的帮助文档。 如果你不需要配置多个端口的话，仅配置单个端口，则可以使用以下配置： 123456&#123; &quot;server&quot;: &quot;0.0.0.0&quot;, &quot;server_port&quot;: 8080, &quot;password&quot;: &quot;填写密码&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;&#125; 说明： server_port为服务监听端口 password为密码 同样的以上两项信息在配置 shadowsocks 客户端时需要配置一致。 配置自启动编辑shadowsocks 服务的启动脚本文件，内容如下： 12345678910$ vim /etc/systemd/system/shadowsocks.service[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c /etc/shadowsocks.json[Install]WantedBy=multi-user.target 执行以下命令启动 shadowsocks 服务： 12$ systemctl enable shadowsocks$ systemctl start shadowsocks 检查 shadowsocks 服务是否已成功启动，可以执行以下命令查看服务的状态： 1$ systemctl status shadowsocks -l 确认服务启动成功后，配置防火墙规则，开放你配置的端口，不然客户端是无法连接的： 123456$ firewall-cmd --zone=public --add-port=8080/tcp --permanentsuccess$ firewall-cmd --zone=public --add-port=8081/tcp --permanentsuccess$ firewall-cmd --reloadsuccess 一键安装脚本代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/bin/bash# Install Shadowsocks on CentOS 7echo "Installing Shadowsocks..."random-string()&#123; cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w $&#123;1:-32&#125; | head -n 1&#125;CONFIG_FILE=/etc/shadowsocks.jsonSERVICE_FILE=/etc/systemd/system/shadowsocks.serviceSS_PASSWORD=$(random-string 32)SS_PORT=8388SS_METHOD=aes-256-cfbSS_IP=`ip route get 1 | awk '&#123;print $NF;exit&#125;'`GET_PIP_FILE=/tmp/get-pip.py# install pipcurl "https://bootstrap.pypa.io/get-pip.py" -o "$&#123;GET_PIP_FILE&#125;"python $&#123;GET_PIP_FILE&#125;# install shadowsockspip install --upgrade pippip install shadowsocks# create shadowsocls configcat &lt;&lt;EOF | sudo tee $&#123;CONFIG_FILE&#125;&#123; "server": "0.0.0.0", "server_port": $&#123;SS_PORT&#125;, "password": "$&#123;SS_PASSWORD&#125;", "method": "$&#123;SS_METHOD&#125;"&#125;EOF# create servicecat &lt;&lt;EOF | sudo tee $&#123;SERVICE_FILE&#125;[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/ssserver -c $&#123;CONFIG_FILE&#125;[Install]WantedBy=multi-user.targetEOF# start servicesystemctl enable shadowsockssystemctl start shadowsocks# view service statussleep 5systemctl status shadowsocks -lecho "================================"echo ""echo "Congratulations! Shadowsocks has been installed on your system."echo "You shadowsocks connection info:"echo "--------------------------------"echo "server: $&#123;SS_IP&#125;"echo "server_port: $&#123;SS_PORT&#125;"echo "password: $&#123;SS_PASSWORD&#125;"echo "method: $&#123;SS_METHOD&#125;"echo "--------------------------------" 配置客户端我这里配置的是windows的客户端，挺方便的，点击即用，不需要安装。 Windows客户端下载地址： https://github.com/shadowsocks/shadowsocks-windows/releases Mac客户端下载地址： https://github.com/shadowsocks/ShadowsocksX-NG/releases Android客户端下载地址： https://github.com/shadowsocks/shadowsocks-android/releases 接着测试能否上Google搜索即可，以下的配置BBR加速则是选看，不配置也是可以正常使用shadowsocks的。 配置BBR加速什么是BBR： TCP BBR是谷歌出品的TCP拥塞控制算法。BBR目的是要尽量跑满带宽，并且尽量不要有排队的情况。BBR可以起到单边加速TCP连接的效果。 Google提交到Linux主线并发表在ACM queue期刊上的TCP-BBR拥塞控制算法。继承了Google“先在生产环境上部署，再开源和发论文”的研究传统。TCP-BBR已经再YouTube服务器和Google跨数据中心的内部广域网(B4)上部署。由此可见出该算法的前途。 TCP-BBR的目标就是最大化利用网络上瓶颈链路的带宽。一条网络链路就像一条水管，要想最大化利用这条水管，最好的办法就是给这跟水管灌满水。 BBR解决了两个问题： 在有一定丢包率的网络链路上充分利用带宽。非常适合高延迟，高带宽的网络链路。 降低网络链路上的buffer占用率，从而降低延迟。非常适合慢速接入网络的用户。Google 在 2016年9月份开源了他们的优化网络拥堵算法BBR，最新版本的 Linux内核(4.9-rc8)中已经集成了该算法。 对于TCP单边加速，并非所有人都很熟悉，不过有另外一个大名鼎鼎的商业软件“锐速”，相信很多人都清楚。特别是对于使用国外服务器或者VPS的人来说，效果更佳。 BBR项目地址： https://github.com/google/bbr 升级内核，第一步首先是升级内核到支持BBR的版本：1.yum更新系统版本： 1$ yum update 2.查看系统版本： 12$ cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) 3.安装elrepo并升级内核： 123$ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org$ rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm$ yum --enablerepo=elrepo-kernel install kernel-ml -y 4.更新grub文件并重启系统： 12345678$ egrep ^menuentry /etc/grub2.cfg | cut -f 2 -d \'CentOS Linux 7 Rescue 8619ff5e1306499eac41c02d3b23868e (4.14.14-1.el7.elrepo.x86_64)CentOS Linux (4.14.14-1.el7.elrepo.x86_64) 7 (Core)CentOS Linux (3.10.0-693.11.6.el7.x86_64) 7 (Core)CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)CentOS Linux (0-rescue-c73a5ccf3b8145c3a675b64c4c3ab1d4) 7 (Core)$ grub2-set-default 0$ reboot 5.重启完成后查看内核是否已更换为4.14版本： 12$ uname -r4.14.14-1.el7.elrepo.x86_64 6.开启bbr： 123$ vim /etc/sysctl.conf # 在文件末尾添加如下内容net.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbr 7.加载系统参数： 12345$ sysctl -pnet.ipv6.conf.all.accept_ra = 2net.ipv6.conf.eth0.accept_ra = 2net.core.default_qdisc = fqnet.ipv4.tcp_congestion_control = bbr 如上，输出了我们添加的那两行配置代表正常。 8.确定bbr已经成功开启： 1234$ sysctl net.ipv4.tcp_available_congestion_controlnet.ipv4.tcp_available_congestion_control = bbr cubic reno$ lsmod | grep bbrtcp_bbr 20480 2 输出内容如上，则表示bbr已经成功开启。 相关文章 http://blog.51cto.com/zero01/2064660 https://github.com/shadowsocks/shadowsocks-libev https://www.jianshu.com/p/4984f324010f 原文链接（如需转载，请注明出处）http://blog.51cto.com/zero01/2064660]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>shadowsocks</tag>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL InnoDB行级锁实现]]></title>
    <url>%2F2018%2F03%2F28%2Fdocs%2F04-mysql%2Fmysql-innodb-lock%2F</url>
    <content type="text"><![CDATA[InnoDB行锁是通过给索引上的索引项加锁来实现的，这一点MySQL与Oracle不同，后者是通过在数据块中对相应数据行加锁来实现的。InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁！ 在实际应用中，要特别注意InnoDB行锁的这一特性，不然的话，可能导致大量的锁冲突，从而影响并发性能。下面通过一些实际例子来加以说明 在不通过索引条件查询的时候，InnoDB使用的是表锁，而不是行锁123456mysql&gt; create table test (id int, name varchar(10)) engine=innodb;Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into test values (1, &apos;1&apos;),(2, &apos;2&apos;),(3, &apos;3&apos;),(4, &apos;4&apos;);Query OK, 4 rows affected (0.02 sec)Records: 4 Duplicates: 0 Warnings: 0 InnoDB存储引擎的表在不使用索引时使用表锁例子session_1 12345678910111213141516171819mysql&gt; set autocommit=0; | Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from test where id = 1; +------+------+ | id | name | +------+------+ | 1 | 1 | +------+------+ 1 row in set (0.00 sec) mysql&gt; select * from test where id = 1 for update;+------+------+| id | name |+------+------+| 1 | 1 |+------+------+1 row in set (0.00 sec) session_2 12345678910111213mysql&gt; set autocommit=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test where id = 2;+------+------+| id | name |+------+------+| 2 | 2 |+------+------+1 row in set (0.01 sec)mysql&gt; select * from test where id = 2 for update;waiting 看起来session_1只给一行加了排他锁，但session_2在请求其他行的排他锁时，却出现了锁等待！原因就是在没有索引的情况下，InnoDB只能使用表锁。当我们给其增加一个索引后，InnoDB就只锁定了符合条件的行 InnoDB存储引擎的表在使用索引时使用行锁例子id字段增加普通索引： 123mysql&gt; alter table test add index id(id);Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0 session_1 123456789101112131415161718mysql&gt; set autocommit=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test where id = 1 ;+------+------+| id | name |+------+------+| 1 | 1 |+------+------+1 row in set (0.00 sec)mysql&gt; select * from test where id = 1 for update;+------+------+| id | name |+------+------+| 1 | 1 |+------+------+1 row in set (0.01 sec) session_2 123456789101112131415161718mysql&gt; set autocommit=0;Query OK, 0 rows affected (0.01 sec)mysql&gt; select * from test where id = 2 ;+------+------+| id | name |+------+------+| 2 | 2 |+------+------+1 row in set (0.01 sec)mysql&gt; select * from test where id = 2 for update;+------+------+| id | name |+------+------+| 2 | 2 |+------+------+1 row in set (0.00 sec) 由于MySQL的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果是使用相同的索引键，是会出现锁冲突的。应用设计的时候要注意这一点。表test的id字段有索引，name字段没有索引： session_1 12345678910mysql&gt; set autocommit=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test where id = 1 and name = &apos;1&apos; for update;+------+------+| id | name |+------+------+| 1 | 1 |+------+------+1 row in set (0.00 sec) session_2 1234567set autocommit=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test where id = 1 and name = &apos;4&apos; for update;waiting虽然session_2访问的是和session_1不同的记录，但是因为使用了相同的索引，所以需要等待锁： 当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引、唯一索引或普通索引，InnoDB都会使用行锁来对数据加锁表test的id字段有主键索引，name字段有普通索引： session_1 12345678910111213141516171819mysql&gt; set autocommit=0;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from test where id = 1 for update;+------+------+| id | name |+------+------+| 1 | 1 || 1 | 4 |+------+------+2 rows in set (0.00 sec) session_2 1234567891011121314151617181920212223mysql&gt; set autocommit=0;Query OK, 0 rows affected (0.00 sec)Session_2使用name的索引访问记录，因为记录没有被索引，所以可以获得锁：mysql&gt; select * from test where name = &apos;2&apos; for update;+------+------+| id | name |+------+------+| 2 | 2 |+------+------+1 row in set (0.00 sec)由于访问的记录已经被session_1锁定，所以等待获得锁。：mysql&gt; select * from test where name = &apos;4&apos; for update; 即便在条件中使用了索引字段，但是否使用索引来检索数据是由MySQL通过判断不同执行计划的代价来决定的，如果MySQL认为全表扫描效率更高，比如对一些很小的表，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。 在下面的例子中，检索值的数据类型与索引字段不同，虽然MySQL能够进行数据类型转换，但却不会使用索引，从而导致InnoDB使用表锁。通过用explain检查两条SQL的执行计划，我们可以清楚地看到了这一点。 例子中test表的name字段有索引，但是name字段是varchar类型的，如果where条件中不是和varchar类型进行比较，则会对name进行类型转换，而执行的全表扫描。 1234567891011121314151617181920212223242526272829mysql&gt; alter table test add index name(name);Query OK, 4 rows affected (8.06 sec)Records: 4 Duplicates: 0 Warnings: 0mysql&gt; explain select * from test where name = 1 \G*************************** 1. row ***************************id: 1select_type: SIMPLEtable: testtype: ALLpossible_keys: namekey: NULLkey_len: NULLref: NULLrows: 4Extra: Using where1 row in set (0.00 sec)mysql&gt; explain select * from test where name = &apos;1&apos; \G*************************** 1. row ***************************id: 1select_type: SIMPLEtable: testtype: refpossible_keys: namekey: namekey_len: 23ref: constrows: 1Extra: Using where1 row in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>InnoDB</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）MySQL 锁机制]]></title>
    <url>%2F2018%2F03%2F27%2Fdocs%2F04-mysql%2Fmysql-lock%2F</url>
    <content type="text"><![CDATA[锁是计算机协调多个进程或线程并发访问某一资源的机制。在数据库中，除传统的计算资源（CPU、RAM、I/O）的争用以外，数据也是一种供许多用户共享的资源。如何保证数据并发访问的一致性、有效性是所在有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决。 Mysql用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。这些锁统称为悲观锁(Pessimistic Lock)。 MySQL锁概述相对其他数据库而言，MySQL的锁机制比较简单，其最 显著的特点是不同的存储引擎支持不同的锁机制。比如，MyISAM和MEMORY存储引擎采用的是表级锁（table-level locking）；BDB存储引擎采用的是页面锁（page-level locking），但也支持表级锁；InnoDB存储引擎既支持行级锁（row-level locking），也支持表级锁，但默认情况下是采用行级锁。表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般从上述特点可见，很难笼统地说哪种锁更好，只能就具体应用的特点来说哪种锁更合适！仅从锁的角度 来说：表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有 并发查询的应用，如一些在线事务处理（OLTP）系统。 MyISAM表锁MySQL的表级锁有两种模式：表共享读锁（Table Read Lock）和表独占写锁（Table Write Lock）。对MyISAM表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求；对 MyISAM表的写操作，则会阻塞其他用户对同一表的读和写操作；MyISAM表的读操作与写操作之间，以及写操作之间是串行的！当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读、写操作都会等待，直到锁被释放为止。 MyISAM存储引擎的写锁阻塞读例子： 当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读、写操作都会等待，直到锁被释放为止。 MyISAM存储引擎的读锁阻塞写例子: 一个session使用LOCK TABLE命令给表film_text加了读锁，这个session可以查询锁定表中的记录，但更新或访问其他表都会提示错误；同时，另外一个session可以查询表中的记录，但更新就会出现锁等待。 如何加表锁MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作 （UPDATE、DELETE、INSERT等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用LOCK TABLE命令给MyISAM表显式加锁。在示例中，显式加锁基本上都是为了演示而已，并非必须如此。给MyISAM表显示加锁，一般是为了在一定程度模拟事务操作，实现对某一时间点多个表的一致性读取。例如， 有一个订单表orders，其中记录有各订单的总金额total，同时还有一个订单明细表order_detail，其中记录有各订单每一产品的金额小计 subtotal，假设我们需要检查这两个表的金额合计是否相符，可能就需要执行如下两条SQL： 12Select sum(total) from orders;Select sum(subtotal) from order_detail; 这时，如果不先给两个表加锁，就可能产生错误的结果，因为第一条语句执行过程中，order_detail表可能已经发生了改变。因此，正确的方法应该是： 1234Lock tables orders read local, order_detail read local;Select sum(total) from orders;Select sum(subtotal) from order_detail;Unlock tables; 要特别说明以下两点内容： 上面的例子在LOCK TABLES时加了“local”选项，其作用就是在满足MyISAM表并发插入条件的情况下，允许其他用户在表尾并发插入记录，有关MyISAM表的并发插入问题，在后面还会进一步介绍。 在用LOCK TABLES给表显式加表锁时，必须同时取得所有涉及到表的锁，并且MySQL不支持锁升级。也就是说，在执行LOCK TABLES后，只能访问显式加锁的这些表，不能访问未加锁的表；同时，如果加的是读锁，那么只能执行查询操作，而不能执行更新操作。其实，在自动加锁的 情况下也基本如此，MyISAM总是一次获得SQL语句所需要的全部锁。这也正是MyISAM表不会出现死锁（Deadlock Free）的原因。 当使用LOCK TABLES时，不仅需要一次锁定用到的所有表，而且，同一个表在SQL语句中出现多少次，就要通过与SQL语句中相同的别名锁定多少次，否则也会出错！举例说明如下。 （1）对actor表获得读锁： 12mysql&gt; lock table actor read;Query OK, 0 rows affected (0.00 sec) （2）但是通过别名访问会提示错误： 123456mysql&gt; select a.first_name,a.last_name,b.first_name,b.last_name from actor a,actor b where a.first_name = b.first_name and a.first_name = &apos;Lisa&apos; and a.last_name = &apos;Tom&apos; and a.last_name &lt;&gt; b.last_name;ERROR 1100 (HY000): Table ‘a’ was not locked with LOCK TABLES （3）需要对别名分别锁定： 123mysql&gt; lock table actor as a read,actor as b read;Query OK, 0 rows affected (0.00 sec) （4）按照别名的查询可以正确执行： 123456789101112mysql&gt; select a.first_name,a.last_name,b.first_name,b.last_name from actor a,actor b where a.first_name = b.first_name and a.first_name = &apos;Lisa&apos; and a.last_name = &apos;Tom&apos; and a.last_name &lt;&gt; b.last_name;+————+———–+————+———–+| first_name | last_name | first_name | last_name |+————+———–+————+———–+| Lisa | Tom | LISA | MONROE |+————+———–+————+———–+1 row in set (0.00 sec)———————————————— 查询表级锁争用情况可以通过检查table_locks_waited和table_locks_immediate状态变量来分析系统上的表锁定争夺： 12345678mysql&gt; show status like &apos;table_locks%&apos;;+-----------------------+-------+| Variable_name | Value |+-----------------------+-------+| Table_locks_immediate | 99 || Table_locks_waited | 0 |+-----------------------+-------+2 rows in set (0.00 sec) 如果Table_locks_waited的值比较高，则说明存在着较严重的表级锁争用情况。 并发插入（Concurrent Inserts）上文提到过MyISAM表的读和写是串行的，但这是就总体而言的。在一定条件下，MyISAM表也支持查询和插入操作的并发进行。MyISAM存储引擎有一个系统变量concurrent_insert，专门用以控制其并发插入的行为，其值分别可以为0、1或2。 当concurrent_insert设置为0时，不允许并发插入 当concurrent_insert设置为1时，如果MyISAM表中没有空洞（即表的中间没有被删除的行），MyISAM允许在一个进程读表的同时，另一个进程从表尾插入记录。这也是MySQL的默认设置 当concurrent_insert设置为2时，无论MyISAM表中有没有空洞，都允许在表尾并发插入记录 在下面的例子中，session_1获得了一个表的READ LOCAL锁，该线程可以对表进行查询操作，但不能对表进行更新操作；其他的线程（session_2），虽然不能对表进行删除和更新操作，但却可以对该表进行并发插入操作，这里假设该表中间不存在空洞。 可以利用MyISAM存储引擎的并发插入特性，来解决应 用中对同一表查询和插入的锁争用。例如，将concurrent_insert系统变量设为2，总是允许并发插入；同时，通过定期在系统空闲时段执行 OPTIMIZE TABLE语句来整理空间碎片，收回因删除记录而产生的中间空洞。 MyISAM的锁调度前面讲过，MyISAM存储引擎的读锁和写锁是互斥的，读写操作是串行的。那么，一个进程请求某个 MyISAM表的读锁，同时另一个进程也请求同一表的写锁，MySQL如何处理呢？答案是写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后 到，写锁也会插到读锁请求之前！这是因为MySQL认为写请求一般比读请求要重要。这也正是MyISAM表不太适合于有大量更新操作和查询操作应用的原 因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况有时可能会变得非常糟糕！幸好我们可以通过一些设置来调节MyISAM 的调度行为。 通过指定启动参数low-priority-updates，使MyISAM引擎默认给予读请求以优先的权利 通过执行命令SET LOW_PRIORITY_UPDATES=1，使该连接发出的更新请求优先级降低 通过指定INSERT、UPDATE、DELETE语句的LOW_PRIORITY属性，降低该语句的优先级 虽然上面3种方法都是要么更新优先，要么查询优先的方法，但还是可以用其来解决查询相对重要的应用（如用户登录系统）中，读锁等待严重的问题。另外，MySQL也提供了一种折中的办法来调节读写冲突，即给系统参数max_write_lock_count设置一个合适的值，当一个表的读锁达到这个值后，MySQL就暂时将写请求的优先级降低，给读进程一定获得锁的机会。 上面已经讨论了写优先调度机制带来的问题和解决办法。这 里还要强调一点：一些需要长时间运行的查询操作，也会使写进程“饿死”！因此，应用中应尽量避免出现长时间运行的查询操作，不要总想用一条SELECT语 句来解决问题，因为这种看似巧妙的SQL语句，往往比较复杂，执行时间较长，在可能的情况下可以通过使用中间表等措施对SQL语句做一定的“分解”，使每 一步查询都能在较短时间完成，从而减少锁冲突。如果复杂查询不可避免，应尽量安排在数据库空闲时段执行，比如一些定期统计可以安排在夜间执行。 InnoDB锁InnoDB与MyISAM的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。行级锁与表级锁本来就有许多不同之处，另外，事务的引入也带来了一些新问题。 1. 事务（Transaction）及其ACID属性 事务是由一组SQL语句组成的逻辑处理单元，事务具有4属性，通常称为事务的ACID属性。 原子性（Actomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。 一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以操持完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。 持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 2. 并发事务带来的问题 相对于串行处理来说，并发事务处理能大大增加数据库资源的利用率，提高数据库系统的事务吞吐量，从而可以支持可以支持更多的用户。但并发事务处理也会带来一些问题，主要包括以下几种情况。 更新丢失（Lost Update）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题——最后的更新覆盖了其他事务所做的更新。例如，两个编辑人员制作了同一文档的电子副本。每个编辑人员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改保存其更改副本的编辑人员覆盖另一个编辑人员所做的修改。如果在一个编辑人员完成并提交事务之前，另一个编辑人员不能访问同一文件，则可避免此问题。 脏读（Dirty Reads）：一个事务正在对一条记录做修改，在这个事务并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”的数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做“脏读”。 不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据已经发生了改变、或某些记录已经被删除了！这种现象叫做“不可重复读”。 幻读（Phantom Reads）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 3. 事务隔离级别 在并发事务处理带来的问题中，“更新丢失”通常应该是完全避免的。但防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决，因此，防止更新丢失应该是应用的责任。 “脏读”、“不可重复读”和“幻读”，其实都是数据库读一致性问题，必须由数据库提供一定的事务隔离机制来解决。数据库实现事务隔离的方式，基本可以分为以下两种。 一种是在读取数据前，对其加锁，阻止其他事务对数据进行修改。 另一种是不用加任何锁，通过一定机制生成一个数据请求时间点的一致性数据快照（Snapshot），并用这个快照来提供一定级别（语句级或事务级）的一致性读取。从用户的角度，好像是数据库可以提供同一数据的多个版本，因此，这种技术叫做数据多版本并发控制（ＭultiVersion Concurrency Control，简称MVCC或MCC），也经常称为多版本数据库。 在MVCC并发控制中，读操作可以分成两类：快照读 (snapshot read)与当前读 (current read)。快照读，读取的是记录的可见版本 (有可能是历史版本)，不用加锁。当前读，读取的是记录的最新版本，并且，当前读返回的记录，都会加上锁，保证其他事务不会再并发修改这条记录。在一个支持MVCC并发控制的系统中，哪些读操作是快照读？哪些操作又是当前读呢？以MySQL InnoDB为例： 快照读：简单的select操作，属于快照读，不加锁。(当然，也有例外) 1select * from table where ?; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。下面语句都属于当前读，读取记录的最新版本。并且，读取之后，还需要保证其他并发事务不能修改当前记录，对读取记录加锁。其中，除了第一条语句，对读取记录加S锁 (共享锁)外，其他的操作，都加的是X锁 (排它锁)。 12345select * from table where ? lock in share mode;select * from table where ? for update;insert into table values (…);update table set ? where ?;delete from table where ?; 数据库的事务隔离越严格，并发副作用越小，但付出的代价也就越大，因为事务隔离实质上就是使事务在一定程度上 “串行化”进行，这显然与“并发”是矛盾的。同时，不同的应用对读一致性和事务隔离程度的要求也是不同的，比如许多应用对“不可重复读”和“幻读”并不敏 感，可能更关心数据并发访问的能力。 为了解决“隔离”与“并发”的矛盾，ISO/ANSI SQL92定义了4个事务隔离级别，每个级别的隔离程度不同，允许出现的副作用也不同，应用可以根据自己的业务逻辑要求，通过选择不同的隔离级别来平衡 “隔离”与“并发”的矛盾。下表很好地概括了这4个隔离级别的特性。 获取InonoD行锁争用情况可以通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况： 1234567891011mysql&gt; show status like &apos;innodb_row_lock%&apos;;+-------------------------------+--------+| Variable_name | Value |+-------------------------------+--------+| Innodb_row_lock_current_waits | 0 || Innodb_row_lock_time | 326100 || Innodb_row_lock_time_avg | 19182 || Innodb_row_lock_time_max | 51029 || Innodb_row_lock_waits | 17 |+-------------------------------+--------+5 rows in set (0.00 sec) 如果发现锁争用比较严重，如InnoDB_row_lock_waits和InnoDB_row_lock_time_avg的值比较高，还可以通过设置InnoDB Monitors来进一步观察发生锁冲突的表、数据行等，并分析锁争用的原因 InnoDB的行锁模式及加锁方法InnoDB实现了以下两种类型的行锁。 共享锁（s）：又称读锁。允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 排他锁（Ｘ）：又称写锁。允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。若事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁。 对于共享锁大家可能很好理解，就是多个事务只能读数据不能改数据。对于排他锁大家的理解可能就有些差别，我当初就犯了一个错误，以为排他锁锁住一行数据后，其他事务就不能读取和修改该行数据，其实不是这样的。排他锁指的是一个事务在一行数据加上排他锁后，其他事务不能再在其上加其他的锁。mysql InnoDB引擎默认的修改数据语句：update,delete,insert都会自动给涉及到的数据加上排他锁，select语句默认不会加任何锁类型，如果加排他锁可以使用select …for update语句，加共享锁可以使用select … lock in share mode语句。所以加过排他锁的数据行在其他事务种是不能修改数据的，也不能通过for update和lock in share mode锁的方式查询数据，但可以直接通过select …from…查询数据，因为普通查询没有任何锁机制。 另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。 意向共享锁（IS）：事务打算给数据行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。 意向排他锁（IX）：事务打算给数据行加排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。 InnoDB行锁模式兼容性列表: 如果一个事务请求的锁模式与当前的锁兼容，InnoDB就请求的锁授予该事务；反之，如果两者两者不兼容，该事务就要等待锁释放。意向锁是InnoDB自动加的，不需用户干预。对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁（X)；对于普通SELECT语句，InnoDB不会加任何锁。事务可以通过以下语句显式给记录集加共享锁或排他锁： 共享锁（S）：SELECT * FROM table_name WHERE … LOCK IN SHARE MODE。 排他锁（X）：SELECT * FROM table_name WHERE … FOR UPDATE。 用SELECT … IN SHARE MODE获得共享锁，主要用在需要数据依存关系时来确认某行记录是否存在，并确保没有人对这个记录进行UPDATE或者DELETE操作。但是如果当前事务也需要对该记录进行更新操作，则很有可能造成死锁，对于锁定行记录后需要进行更新操作的应用，应该使用SELECT… FOR UPDATE方式获得排他锁。 InnoDB行锁实现方式 间隙锁（Next-Key锁）当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的 索引项加锁；对于键值在条件范围内但并不存在的记录，叫做“间隙（GAP)”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙锁 （Next-Key锁）。 1Select * from emp where empid &gt; 100 for update; 是一个范围条件的检索，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。 InnoDB使用间隙锁的目的，一方面是为了防止幻读，以满足相关隔离级别的要求，对于上面的例子，要是不使 用间隙锁，如果其他事务插入了empid大于100的任何记录，那么本事务如果再次执行上述语句，就会发生幻读；另外一方面，是为了满足其恢复和复制的需 要。有关其恢复和复制对锁机制的影响，以及不同隔离级别下InnoDB使用间隙锁的情况，在后续的章节中会做进一步介绍。 很显然，在使用范围条件检索并锁定记录时，InnoDB这种加锁机制会阻塞符合条件范围内键值的并发插入，这往往会造成严重的锁等待。因此，在实际应用开发中，尤其是并发插入比较多的应用，我们要尽量优化业务逻辑，尽量使用相等条件来访问更新数据，避免使用范围条件。 还要特别说明的是，InnoDB除了通过范围条件加锁时使用间隙锁外，如果使用相等条件请求给一个不存在的记录加锁，InnoDB也会使用间隙锁！下面这个例子假设emp表中只有101条记录，其empid的值分别是1,2,……,100,101。InnoDB存储引擎的间隙锁阻塞例子 小结本文重点介绍了MySQL中MyISAM表级锁和InnoDB行级锁的实现特点，并讨论了两种存储引擎经常遇到的锁问题和解决办法。 对于MyISAM的表锁，主要讨论了以下几点： 共享读锁（S）之间是兼容的，但共享读锁（S）与排他写锁（X）之间，以及排他写锁（X）之间是互斥的，也就是说读和写是串行的 在一定条件下，MyISAM允许查询和插入并发执行，我们可以利用这一点来解决应用中对同一表查询和插入的锁争用问题 MyISAM默认的锁调度机制是写优先，这并不一定适合所有应用，用户可以通过设置LOW_PRIORITY_UPDATES参数，或在INSERT、UPDATE、DELETE语句中指定LOW_PRIORITY选项来调节读写锁的争用 由于表锁的锁定粒度大，读写之间又是串行的，因此，如果更新操作较多，MyISAM表可能会出现严重的锁等待，可以考虑采用InnoDB表来减少锁冲突 对于InnoDB表，本文主要讨论了以下几项内容： InnoDB的行锁是基于索引实现的，如果不通过索引访问数据，InnoDB会使用表锁 介绍了InnoDB间隙锁（Next-key)机制，以及InnoDB使用间隙锁的原因。在不同的隔离级别下，InnoDB的锁机制和一致性读策略不同 在了解InnoDB锁特性后，用户可以通过设计和SQL调整等措施减少锁冲突和死锁，包括： 尽量使用较低的隔离级别； 精心设计索引，并尽量使用索引访问数据，使加锁更精确，从而减少锁冲突的机会； 选择合理的事务大小，小事务发生锁冲突的几率也更小 给记录集显式加锁时，最好一次性请求足够级别的锁。比如要修改数据的话，最好直接申请排他锁，而不是先申请共享锁，修改时再请求排他锁，这样容易产生死锁 不同的程序访问一组表时，应尽量约定以相同的顺序访问各表，对一个表而言，尽可能以固定的顺序存取表中的行。这样可以大大减少死锁的机会 尽量用相等条件访问数据，这样可以避免间隙锁对并发插入的影响； 不要申请超过实际需要的锁级别；除非必须，查询时不要显示加锁 对于一些特定的事务，可以使用表锁来提高处理速度或减少死锁的可能 本文转载自：https://blog.csdn.net/soonfly/article/details/70238902]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 客户端多账号管理]]></title>
    <url>%2F2018%2F02%2F25%2Fdocs%2F02-tools%2Fgit-multiple-accounts%2F</url>
    <content type="text"><![CDATA[前言在开发的过程中，经常会遇到这样的状况：需要在一台电脑上同时使用两个甚至多个 git 账号，负责不同的用途，比如：一个用来写个人项目，一个用来写公司的项目。为此我们需要为不同的账号生成不同的密钥，那对这些不同的账号和不同的密钥，我们该怎么处理呢？ SSH配置取消全局设置的用户名和邮箱12$ git config --global --unset user.name$ git config --global --unset user.email 生成私钥和公钥123456789101112131415$ cd ~/.ssh &amp;&amp; mkdir -pv &#123;github,company&#125;$ ssh-keygen -t rsa -C "youremail@example.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/Administrator/.ssh/id_rsa):/Users/Administrator/.ssh/github/id_rsa_github # 在回车提示中输入完整路径，如：/Users/Administrator/.ssh/github/id_rsa_github #文件命名后，按2次回车，密码为空 Enter passphrase (empty for no passphrase):Enter same passphrase again:$ ssh-keygen -t rsa -C "youremail@example.com"Generating public/private rsa key pair.Enter file in which to save the key (/Users/Administrator/.ssh/id_rsa):/Users/Administrator/.ssh/company/id_rsa_companyEnter passphrase (empty for no passphrase):Enter same passphrase again: 如果用户家目录中没有 .ssh 目录请自行创建在这里我创建了两个目录 github 和 company ，分别用来存储不同项目的密钥，进行分类管理 New SSH key 把 ~/.ssh/github/id_rsa_github.pub 的内容添加到Github的SSH keys中 把 ~/.ssh/company/id_rsa_company.pub 的内容添加到公司Gitlab的SSH keys中 新密钥添加到SSH agent中添加新密钥到SSH agent，因为默认只读取id_rsa，为了让SSH识别新的私钥，需将其添加到SSH agent中： 12$ ssh-add -K ~/.ssh/github/id_rsa_github$ ssh-add -K ~/.ssh/company/id_rsa_company 如果出现Could not open a connection to your authentication agent的错误，就试着用以下命令： 12$ ssh-agent bash$ ssh-add -K ~/.ssh/github/id_rsa_github 使用 ssh-add -l 查看 ssh key 的设置 修改 config 文件1234567891011121314151617181920$ vim ~/.ssh/configHost * KexAlgorithms +diffie-hellman-group1-sha1# default: myfirstHost github.com HostName github.com User myfirst PreferredAuthentications publickey IdentityFile ~/.ssh/github/id_rsa_github1# mysecondHost mysecond.github.com HostName github.com User mysecond PreferredAuthentications publickey IdentityFile ~/.ssh/github/id_rsa_github2Host company.com HostName company.com User company PreferredAuthentications publickey IdentityFile ~/.ssh/company/id_rsa_company 其规则就是：从上至下读取config的内容，在每个Host下寻找对应的私钥。这里将GitHub SSH仓库地址中的git@github.com替换成新建的Host别名如：mysecond.github.com，那么原地址是：git@github.com:test/Mywork.git，替换后应该是：mysecond.github.com:test/Mywork.git. 测试连通性123$ ssh -T git@github.comHi youremail! You've successfully authenticated, but GitHub does not provide shell access. 项目测试初始化项目a1234567891011121314$ cd ~/a$ git init$ echo "myfirst" &gt; README.md$ git add README.md$ git config user.name "myfirst"$ git config user.email "myfirst@gmail.com"$ git remote add github git@github.com:myfirst/test.git$ git push -u github masterCounting objects: 3, done.Writing objects: 100% (3/3), 213 bytes | 213.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To github.com:myfirst/test.git * [new branch] master -&gt; masterBranch master set up to track remote branch master from github. 初始化项目b1234567891011121314$ cd ~/b$ git init$ echo "mysecond" &gt; README.md$ git add README.md$ git config user.name "mysecond"$ git config user.email "mysecond@gmail.com"$ git remote add github git@mysecond.github.com:mysecond/test.git$ git push -u github masterCounting objects: 3, done.Writing objects: 100% (3/3), 218 bytes | 218.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To mysecond.github.com:mysecond/test.git * [new branch] master -&gt; masterBranch master set up to track remote branch master from github.]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>Git</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homestead 下安装Swoole扩展]]></title>
    <url>%2F2018%2F02%2F21%2Fdocs%2F06-Linux%2Fphp-extension-swoole-install%2F</url>
    <content type="text"><![CDATA[前言尽管PHP软件源提供了不少PHP扩展，但并不是提供所有的扩展，那么如果我们需要安装一个软件源没有提供的扩展怎么办呢？利用 php-dev 就可以很方便的进行自行编译 PHP 扩展了，但是由于Homestaed内置了多个PHP版本，安装方式略有不同，不能简单粗暴的使用 pecl 安装了，因为安装好了不知道是给谁用的。下面分别介绍单一PHP和多版本PHP如何安装PHP扩展 注意：命令权限不够时请自动在命令前加上sudo前缀提权； 单一版本PHP安装 php-dev ，如果不是 7.2 需要自己修改一下版本号1$ apt install php7.2-dev 安装Swoole扩展1$ pecl install swoole 添加配置文件123$ cd /etc/php/7.2/fpm/conf.d$ touch swoole.ini$ echo "extension=swoole.so" | tee -a swoole.ini 重启php-fpm生效1$ systemctl restart php7.2-fpm 多PHP版本安装php-dev1$ apt install php5.6-dev php7.2-dev 下载swoole源码1234$ cd /usr/src$ wget http://pecl.php.net/get/swoole-1.9.15.tgz$ tar xzf swoole-1.9.15.tgz$ cd swoole-1.9.15 为PHP5.6进行编译1234$ cd /usr/src/swoole-1.9.15$ /usr/bin/phpize5.6$ ./configure --with-php-config=/usr/bin/php-config5.6$ make &amp;&amp; make install 为PHP7.2进行编译1234$ cd /usr/src/swoole-1.9.15$ /usr/bin/phpize7.2$ ./configure --with-php-config=/usr/bin/php-config7.2$ make &amp;&amp; make install 编译完成后扩展在module目录中，它的文件名是swoole.so 查看php的extension_dir1234$ php -i|grep extension_dirextension_dir =&gt; /usr/lib/php/20180731 =&gt; /usr/lib/php/20180731sqlite3.extension_dir =&gt; no value =&gt; no value 这里可以将php替换成指定版本，就可以查看指定版本的extension_dir 添加php配置文件12$ cd /etc/php/7.2/mods-available/$ sudo touch swoole.ini 添加以下内容 123; configuration for php swoole module; priorit=20extension=swoole.so 建立链接文件 123$ sudo ln -s /etc/php/7.0/mods-available/swoole.ini /etc/php/7.2/cli/conf.d/20-swoole.ini $ sudo ln -s /etc/php/7.0/mods-available/swoole.ini /etc/php/7.2/fpm/conf.d/20-swoole.ini 重启php-fpm1$ sudo service php7.2-fpm restart 将7.2替换成5.6为php56添加swoole扩展 同理，什么mongodb、redis的扩展等等，也都能够通过类似的方法完成安装 扩展：Homestead下修改cli模式下默认php版本我们通过命令 ll /usr/bin/php 可以看到，php是/etc/alternatives/php 建立的链接文件 1lrwxrwxrwx 1 root root 21 Feb 3 19:53 /usr/bin/php -&gt; /etc/alternatives/php 然后通过命令 ll /etc/alternatives/php 可以看到是通过 usr/bin/php7.0 建立的链接文件 1lrwxrwxrwx 1 root root 15 Feb 19 05:15 /etc/alternatives/php -&gt; /usr/bin/php7.2* 所以我们只要修改 /etc/alternatives/php 的源文件即可修改cli模式下php的默认版本，命令如下： 12$ sudo mv /etc/alternatives/php$ sudo ln -s /usr/bin/php7.0 /etc/alternatives/php]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Linux</tag>
        <tag>Swoole</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于 RESTful API 设计的总结]]></title>
    <url>%2F2017%2F12%2F17%2Fdocs%2F09-pc-base%2Frestful-architecture%2F</url>
    <content type="text"><![CDATA[RESTful 是目前最流行的 API 设计规范，用于 Web 数据接口的设计。 为什么要用RESTfulRESTful 是一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。 RESTful 架构什么是RESTREST全称是Representational State Transfer，中文意思是表表现（编者注：通常译为表征）层状态转化。 它首次出现在2000年Roy Fielding的博士论文中，Roy Fielding是HTTP规范的主要编写者之一。 他在论文中提到：”我这篇文章的写作目的，就是想在符合架构原理的前提下，理解和评估以网络为基础的应用软件的架构设计，得到一个功能强、性能好、适宜通信的架构。REST指的是一组架构约束条件和原则。” 如果一个架构符合REST的约束条件和原则，我们就称它为RESTful架构。 REST本身并没有创造新的技术、组件或服务，而隐藏在RESTful背后的理念就是使用Web的现有特征和能力， 更好地使用现有Web标准中的一些准则和约束。虽然REST本身受Web技术的影响很深， 但是理论上REST架构风格并不是绑定在HTTP上，只不过目前HTTP是唯一与REST相关的实例。 所以我们这里描述的REST也是通过HTTP实现的REST。 理解RESTful要理解 RESTful 架构，最好的方法就是去理解 Representational State Transfer 这个词组到底是什么意思，它的每一个词代表了什么涵义。如果把这个名称搞懂了，也就不难体会 REST 是一种什么样的设计。 资源 （Resources）REST 的名称 “表现层状态转化” 中，省略了主语。”表现层” 其实指的是 “资源”（Resources）的 “表现层”。 所谓 “资源”，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的实在。你可以用一个 URI（统一资源定位符）指向它，每种资源对应一个特定的 URI 。要获取这个资源，访问它的 URI 就可以，因此 URI 就成了每一个资源的地址或独一无二的识别符。所谓 “上网”，就是与互联网上一系列的 “资源” 互动，调用它的 URI 表现层（Representation）“资源” 是一种信息实体，它可以有多种外在表现形式。我们把 “资源” 具体呈现出来的形式，叫做它的 “表现层”（Representation）。比如，文本可以用 txt 格式表现，也可以用 HTML 格式、 XML 格式、JSON 格式表现，甚至可以采用二进制格式；图片可以用 JPG 格式表现，也可以用 PNG 格式表现。URI 只代表资源的实体，不代表它的形式。严格地说，有些网址最后的 “.html” 后缀名是不必要的，因为这个后缀名表示格式，属于 “表现层” 范畴，而 URI 应该只代表 “资源” 的位置。它的具体表现形式，应该在 HTTP 请求的头信息中用 Accept 和 Content-Type 字段指定，这两个字段才是对 “表现层” 的描述。 状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，势必涉及到数据和状态的变化。互联网通信协议 HTTP 协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生 “状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是 “表现层状态转化”。客户端用到的手段，只能是 HTTP 协议。具体来说，就是 HTTP 协议里面，四个表示操作方式的动词：GET 、 POST 、 PUT 、 DELETE 。 它们分别对应四种基本操作： GET 用来获取资源， POST 用来新建资源，PUT 用来更新资源，DELETE 用来删除资源。 综述 每一个 URI 代表一种资源 客户端和服务器之间，传递这种资源的某种表现层 客户端通过四个 HTTP 动词，对服务器端资源进行操作，实现 “表现层状态转化” RESTful API 的设计协议如果能全站 HTTPS 当然是最好的，不能的话也请尽量将登录、注册等涉及密码的接口使用 HTTPS。 域名应该尽量将 API 部署在专用域名之下。如： 1https://api.example.com 如果确定 API 很简单，不会有进一步扩展，可以考虑放在主域名下。 1https://example.org/api/ 版本号API 的版本号和客户端 APP 的版本号是毫无关系的，不要让 APP 将它们用于提交应用市场的版本号传递到服务器，而是提供类似于 v1、v2 之类的 API 版本号。 版本号拼接在 URL 中。如 1api.example.com/v1/users 或是放在 Header 中: 123api.example.com/usersversion=v1 请求一般来说 API 的外在形式无非就是增删改查（当然具体的业务逻辑肯定要复杂得多），而查询又分为详情和列表两种，在 RESTful 中这就相当于通用的模板。例如针对文章（Article）设计 API，那么最基础的 URL 就是这几种： GET /articles： 文章列表 GET /articles/id：文章详情 POST /articles/： 创建文章 PUT /articles/id：修改文章 DELETE /articles/id：删除文章 Token 和 SignAPI 需要设计成无状态，所以客户端在每次请求时都需要提供有效的 Token 和 Sign，在我看来它们的用途分别是： Token 用于证明请求所属的用户，一般都是服务端在登录后随机生成一段字符串（UUID）和登录用户进行绑定，再将其返回给客户端。Token 的状态保持一般有两种方式实现：一种是在用户每次操作都会延长或重置 TOKEN 的生存时间（类似于缓存的机制），另一种是 Token 的生存时间固定不变，但是同时返回一个刷新用的 Token，当 Token 过期时可以将其刷新而不是重新登录 Sign 用于证明该次请求合理，所以一般客户端会把请求参数拼接后并加密作为 Sign 传给服务端，这样即使被抓包了，对方只修改参数而无法生成对应的 Sign 也会被服务端识破。当然也可以将时间戳、请求地址和 Token 也混入 Sign，这样 Sign 也拥有了所属人、时效性和目的地 业务参数在 RESTful 的标准中，PUT 和 PATCH 都可以用于修改操作，它们的区别是 PUT 需要提交整个对象，而 PATCH 只需要提交修改的信息。但是在我看来实际应用中不需要这么麻烦，所以我一律使用 PUT，并且只提交修改的信息。 另一个问题是在 POST 创建对象时，究竟该用表单提交更好些还是用 JSON 提交更好些。其实两者都可以，在我看来它们唯一的区别是 JSON 可以比较方便的表示更为复杂的结构（有嵌套对象）。另外无论使用哪种，请保持统一，不要两者混用。 还有一个建议是最好将过滤、分页和排序的相关信息全权交给客户端，包括过滤条件、页数或是游标、每页的数量、排序方式、升降序等，这样可以使 API 更加灵活。但是对于过滤条件、排序方式等，不需要支持所有方式，只需要支持目前用得上的和以后可能会用上的方式即可，并通过字符串枚举解析，这样可见性要更好些。例如： 搜索，客户端只提供关键词，具体搜索的字段，和搜索方式（前缀、全文、精确）由服务端决定 1/users/?query=ScienJus 过滤，只需要对已有的情况进行支持： 1/users/?gender=1 分页： 1/users/?page=2&amp;per_page=20 响应尽量使用 HTTP 状态码，常用的有： 200：请求成功 201：创建、修改成功 204：删除成功 400：参数错误 401：未登录 403：禁止访问 404：未找到 500：系统错误 但是有些时候仅仅使用 HTTP 状态码没有办法明确的表达错误信息，所以也可以在里面再包一层自定义的返回码，例如： 12345678910&#123; &quot;code&quot;: 100, &quot;msg&quot;: &quot;成功&quot;, &quot;data&quot;: &#123;&#125;&#125;&#123; &quot;code&quot;: -1000, &quot;msg&quot;: &quot;用户名或密码错误&quot;&#125; data 是真正需要返回的数据，并且只会在请求成功时才存在，msg 只用在开发环境，并且只为了开发人员识别。客户端逻辑只允许识别 code，并且不允许直接将 msg 的内容展示给用户。如果这个错误很复杂，无法使用一段话描述清楚，也可以在添加一个 doc 字段，包含指向该错误的文档的链接。 返回数据JSON 比 XML 可视化更好，也更加节约流量，所以尽量不要使用 XML。 创建和修改操作成功后，需要返回该资源的全部信息。 返回数据不要和客户端界面强耦合，不要在设计 API 时就考虑少查询一张关联表或是少查询 / 返回几个字段能带来多大的性能提升。并且一定要以资源为单位，即使客户端一个页面需要展示多个资源，也不要在一个接口中全部返回，而是让客户端分别请求多个接口。 最好将返回数据进行加密和压缩，尤其是压缩在移动应用中还是比较重要的。 参考文章 http://www.ruanyifeng.com/blog/2011/09/restful.htmlhttps://www.runoob.com/w3cnote/restful-architecture.htmlhttp://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>RESTful</tag>
        <tag>REST</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 自动加载原理解析]]></title>
    <url>%2F2017%2F10%2F21%2Fdocs%2F03-php%2Fphp-autoload%2F</url>
    <content type="text"><![CDATA[前言PHP 自5.3的版本之后，已经重焕新生，命名空间、性状（trait）、闭包、接口、PSR 规范、以及 composer 的出现已经让 PHP 变成了一门现代化的脚本语言。PHP 的生态系统也一直在演进，而 composer 的出现更是彻底的改变了以往构建 PHP 应用的方式，我们可以根据 PHP 的应用需求混合搭配最合适的 PHP 组件。当然这也得益于 PSR 规范的提出 PHP 自动加载功能PHP 自动加载功能的由来在 PHP 开发过程中，如果希望从外部引入一个 Class ，通常会使用 include 和 require 方法，去把定义这个 Class 的文件包含进来。这个在小规模开发的时候，没什么大问题。但在大型的开发项目中，使用这种方式会带来一些隐含的问题：如果一个 PHP 文件需要使用很多其它类，那么就需要很多的 require/include 语句，这样有可能会 造成遗漏 或者 包含进不必要的类文件。如果大量的文件都需要使用其它的类，那么要保证每个文件都包含正确的类文件肯定是一个噩梦， 况且 require或 incloud 的性能代价很大。 PHP5 为这个问题提供了一个解决方案，这就是 类的自动加载(autoload)机制。autoload机制 可以使得 PHP 程序有可能在使用类时才自动包含类文件，而不是一开始就将所有的类文件include进来，这种机制也称为 Lazy loading (惰性加载)。 自动加载功能的优点 使用类之前无需 include / require 使用类的时候才会 include / require 文件，实现了 lazy loading ，避免了 include / require 多余文件 无需考虑引入类的实际磁盘地址 ，实现了逻辑和实体文件的分离 PHP 自动加载函数 __autoload()从 PHP5 开始，当我们在使用一个类时，如果发现这个类没有加载，就会自动运行 __autoload() 函数，这个函数是我们在程序中自定义的，在这个函数中我们可以加载需要使用的类。下面是个简单的示例 12345&lt;?phpfunction __autoload($classname) &#123; require_once ($classname . ".class.php");&#125; 在我们这个简单的例子中，我们直接将类名加上扩展名 .class.php 构成了类文件名，然后使用 require_once 将其加载 从这个例子中，我们可以看出 __autoload 至少要做三件事情： 根据类名确定类文件名 确定类文件所在的磁盘路径 将类从磁盘文件中加载到系统中 第三步最简单，只需要使用 include / require 即可。要实现第一步，第二步的功能，必须在开发时约定类名与磁盘文件的映射方法，只有这样我们才能根据类名找到它对应的磁盘文件 当有大量的类文件要包含的时候，我们只要确定相应的规则，然后在 __autoload() 函数中，将类名与实际的磁盘文件对应起来，就可以实现 lazy loading 的效果 PHP autoload函数说明 __autoload() 函数存在的问题如果在一个系统的实现中，如果需要使用很多其它的类库，这些类库可能是由不同的开发人员编写的， 其类名与实际的磁盘文件的映射规则不尽相同。这时如果要实现类库文件的自动加载，就必须 在 __autoload() 函数中将所有的映射规则全部实现，这样的话 __autoload() 函数有可能会非常复杂，甚至无法实现。最后可能会导致 __autoload() 函数十分臃肿，这时即便能够实现，也会给将来的维护和系统效率带来很大的负面影响 那么问题出现在哪里呢？问题出现在 __autoload() 是全局函数只能定义一次 ，不够灵活，所以所有的类名与文件名对应的逻辑规则都要在一个函数里面实现，造成这个函数的臃肿。那么如何来解决这个问题呢？答案就是使用一个 __autoload调用堆栈 ，不同的映射关系写到不同的 __autoload函数 中去，然后统一注册统一管理，这个就是 PHP5 引入的 SPL Autoload 。 SPL AutoloadSPL是 Standard PHP Library(标准PHP库)的缩写。它是 PHP5 引入的一个扩展标准库，包括 spl autoload 相关的函数以及各种数据结构和迭代器的接口或类。spl autoload 相关的函数具体可见 php中spl_autoload 123456789101112131415161718192021222324252627282930313233343536&lt;?php// __autoload 函数//// function __autoload($class) &#123;// include 'classes/' . $class . '.class.php';// &#125;function my_autoloader($class) &#123; include 'classes/' . $class . '.class.php';&#125;spl_autoload_register('my_autoloader');// 定义的 autoload 函数在 class 里// 静态方法class MyClass &#123; public static function autoload($className) &#123; // ... &#125;&#125;spl_autoload_register(array('MyClass', 'autoload'));// 非静态方法class MyClass &#123; public function autoload($className) &#123; // ... &#125;&#125;$instance = new MyClass();spl_autoload_register(array($instance, 'autoload')); spl_autoload_register() 就是我们上面所说的__autoload调用堆栈，我们可以向这个函数注册多个我们自己的 autoload() 函数，当 PHP 找不到类名时，PHP就会调用这个堆栈，然后去调用自定义的 autoload() 函数，实现自动加载功能。如果我们不向这个函数输入任何参数，那么就会默认注册 spl_autoload() 函数。 PSR4 标准PSR-4 规范了如何指定文件路径从而自动加载类定义，同时规范了自动加载文件的位置。 一个完整的类名需具有以下结构 完整的类名必须要有一个顶级命名空间，被称为 “vendor namespace” 完整的类名可以有一个或多个子命名空间 完整的类名必须有一个最终的类名 完整的类名中任意一部分中的下滑线都是没有特殊含义的 完整的类名可以由任意大小写字母组成 所有类名都必须是大小写敏感的 根据完整的类名载入相应的文件 完整的类名中，去掉最前面的命名空间分隔符，前面连续的一个或多个命名空间和子命名空间，作为「命名空间前缀」，其必须与至少一个「文件基目录」相对应 紧接命名空间前缀后的子命名空间 必须 与相应的「文件基目录」相匹配，其中的命名空间分隔符将作为目录分隔符。 末尾的类名必须与对应的以 .php 为后缀的文件同名 自动加载器（autoloader）的实现一定不可抛出异常、一定不可触发任一级别的错误信息以及不应该有返回值 例子PSR-4风格 1234类名：ZendAbc 命名空间前缀：Zend 文件基目录：/usr/includes/Zend/ 文件路径：/usr/includes/Zend/Abc.php 1234类名：SymfonyCoreRequest 命名空间前缀：SymfonyCore 文件基目录：./vendor/Symfony/Core/ 文件路径：./vendor/Symfony/Core/Request.php 目录结构 1234567-vendor/| -vendor_name/| | -package_name/| | | -src/| | | | -ClassName.php # Vendor_Name\Package_Name\ClassName| | | -tests/| | | | -ClassNameTest.php # Vendor_Name\Package_Name\ClassNameTest Composer 自动加载过程Composer 做了哪些事情 你有一个项目依赖于若干个库。 其中一些库依赖于其他库。 你声明你所依赖的东西。 Composer 会找出哪个版本的包需要安装，并安装它们（将它们下载到你的项目中）。 例如，你正在创建一个项目，需要做一些单元测试。你决定使用 phpunit 。为了将它添加到你的项目中，你所需要做的就是在 composer.json 文件里描述项目的依赖关系。 12345&#123; "require": &#123; "phpunit/phpunit":"~6.0", &#125; &#125; 然后在 composer require 之后我们只要在项目里面直接 use phpunit 的类即可使用。 执行 composer require 时发生了什么 composer 会找到符合 PR4 规范的第三方库的源 将其加载到 vendor 目录下 初始化顶级域名的映射并写入到指定的文件里 如：&#39;PHPUnit\\Framework\\Assert&#39; =&gt; __DIR__ . &#39;/..&#39; . &#39;/phpunit/phpunit/src/Framework/Assert.php&#39; 写好一个 autoload 函数，并且注册到 spl_autoload_register()里 题外话：现在很多框架都已经帮我们写好了顶级域名映射了，我们只需要在框架里面新建文件，在新建的文件中写好命名空间，就可以在任何地方 use 我们的命名空间了 Composer 源码分析下面我们通过对源码的分析来看看 composer 是如何实现 PSR4标准 的自动加载功能。 很多框架在初始化的时候都会引入 composer 来协助自动加载的，以 Laravel 为例，它入口文件 index.php 第一句就是利用 composer 来实现自动加载功能 启动1234&lt;?php define('LARAVEL_START', microtime(true)); require __DIR__ . '/../vendor/autoload.php'; 去 vendor 目录下的 autoload.php ： 1234567&lt;?php// autoload.php @generated by Composerrequire_once __DIR__ . '/composer/autoload_real.php';return ComposerAutoloaderInit6ed409f9f3791196a1d5a1f407fb5184::getLoader(); 这里就是 Composer 真正开始的地方了 Composer自动加载文件Composer自动加载所用到的源文件。 autoload_real.php: 自动加载功能的引导类 composer 加载类的初始化(顶级命名空间与文件路径映射初始化)和注册(spl_autoload_register()) ClassLoader.php : composer 加载类 composer 自动加载功能的核心类 autoload_static.php : 顶级命名空间初始化类 用于给核心类初始化顶级命名空间 autoload_classmap.php : 自动加载的最简单形式 有完整的命名空间和文件目录的映射 autoload_files.php : 用于加载全局函数的文件 存放各个全局函数所在的文件路径名 autoload_namespaces.php : 符合 PSR0 标准的自动加载文件 存放着顶级命名空间与文件的映射 autoload_psr4.php : 符合 PSR4 标准的自动加载文件 存放着顶级命名空间与文件的映射 autoload_real 引导类在 vendor 目录下的 autoload.php 文件中我们可以看出，程序主要调用了引导类的静态方法 getLoader() ，我们接着看看这个函数 自动加载引导类分为 5 个部分。 第一部分——单例第一部分很简单，就是个最经典的单例模式，自动加载类只能有一个。 1234&lt;?php if (null !== self::$loader) &#123; return self::$loader; &#125; 第二部分——构造ClassLoader核心类第二部分 new 一个自动加载的核心类对象。 1234567891011&lt;?php /***********************获得自动加载核心类对象********************/ spl_autoload_register( array('ComposerAutoloaderInit7b790917ce8899df9af8ed53631a1c29', 'loadClassLoader'), true, true ); self::$loader = $loader = new \Composer\Autoload\ClassLoader(); spl_autoload_unregister( array('ComposerAutoloaderInit7b790917ce8899df9af8ed53631a1c29', 'loadClassLoader') ); 1234567&lt;?phppublic static function loadClassLoader($class)&#123; if ('Composer\Autoload\ClassLoader' === $class) &#123; require __DIR__ . '/ClassLoader.php'; &#125;&#125; 从程序里面我们可以看出，composer 先向 PHP 自动加载机制注册了一个函数，这个函数 require 了 ClassLoader 文件。成功 new 出该文件中核心类 ClassLoader() 后，又销毁了该函数 第三部分 —— 初始化核心类对象12345678910111213141516171819202122232425&lt;?php /***********************初始化自动加载核心类对象********************/ $useStaticLoader = PHP_VERSION_ID &gt;= 50600 &amp;&amp; !defined('HHVM_VERSION'); if ($useStaticLoader) &#123; require_once __DIR__ . '/autoload_static.php'; call_user_func( \Composer\Autoload\ComposerStaticInit7b790917ce8899df9af8ed53631a1c29::getInitializer($loader) ); &#125; else &#123; $map = require __DIR__ . '/autoload_namespaces.php'; foreach ($map as $namespace =&gt; $path) &#123; $loader-&gt;set($namespace, $path); &#125; $map = require __DIR__ . '/autoload_psr4.php'; foreach ($map as $namespace =&gt; $path) &#123; $loader-&gt;setPsr4($namespace, $path); &#125; $classMap = require __DIR__ . '/autoload_classmap.php'; if ($classMap) &#123; $loader-&gt;addClassMap($classMap); &#125; &#125; 这一部分就是对自动加载类的初始化，主要是给自动加载核心类初始化顶级命名空间映射。 初始化的方法有两种： 使用 autoload_static 进行静态初始化 调用核心类接口初始化 autoload_static 静态初始化 ( PHP &gt;= 5.6 )静态初始化只支持 PHP5.6 以上版本并且不支持 HHVM 虚拟机。我们深入 autoload_static.php 这个文件发现这个文件定义了一个用于静态初始化的类，名字叫 ComposerStaticInit7b790917ce8899df9af8ed53631a1c29，仍然为了避免冲突而加了 hash 值。这个类很简单 12345678910111213141516171819&lt;?php class ComposerStaticInit7b790917ce8899df9af8ed53631a1c29&#123; public static $files = array(...); public static $prefixLengthsPsr4 = array(...); public static $prefixDirsPsr4 = array(...); public static $prefixesPsr0 = array(...); public static $classMap = array (...); public static function getInitializer(ClassLoader $loader) &#123; return \Closure::bind(function () use ($loader) &#123; $loader-&gt;prefixLengthsPsr4 = ComposerStaticInit6ed409f9f3791196a1d5a1f407fb5184::$prefixLengthsPsr4; $loader-&gt;prefixDirsPsr4 = ComposerStaticInit6ed409f9f3791196a1d5a1f407fb5184::$prefixDirsPsr4; $loader-&gt;prefixesPsr0 = ComposerStaticInit6ed409f9f3791196a1d5a1f407fb5184::$prefixesPsr0; $loader-&gt;classMap = ComposerStaticInit6ed409f9f3791196a1d5a1f407fb5184::$classMap; &#125;, null, ClassLoader::class); &#125; &#125; 这个静态初始化类的核心就是 getInitializer() 函数，它将自己类中的顶级命名空间映射给了 ClassLoader 类。值得注意的是这个函数返回的是一个匿名函数，为什么呢？原因就是 ClassLoader类 中的 prefixLengthsPsr4 、prefixDirsPsr4等等变量都是 private的。利用匿名函数的绑定功能就可以将这些 private 变量赋给 ClassLoader 类 里的成员变量。 classMap（命名空间映射）1234567891011121314151617&lt;?php public static $classMap = array ( 'App\\Console\\Kernel' =&gt; __DIR__ . '/../..' . '/app/Console/Kernel.php', 'App\\Exceptions\\Handler' =&gt; __DIR__ . '/../..' . '/app/Exceptions/Handler.php', 'App\\Http\\Controllers\\Auth\\ForgotPasswordController' =&gt; __DIR__ . '/../..' . '/app/Http/Controllers/Auth/ForgotPasswordController.php', 'App\\Http\\Controllers\\Auth\\LoginController' =&gt; __DIR__ . '/../..' . '/app/Http/Controllers/Auth/LoginController.php', 'App\\Http\\Controllers\\Auth\\RegisterController' =&gt; __DIR__ . '/../..' . '/app/Http/Controllers/Auth/RegisterController.php', ...) 直接命名空间全名与目录的映射，简单粗暴，也导致这个数组相当的大 PSR4 标准顶级命名空间映射数组1234567891011121314151617181920212223242526&lt;?php public static $prefixLengthsPsr4 = array( 'p' =&gt; array ( 'phpDocumentor\\Reflection\\' =&gt; 25, ), 'S' =&gt; array ( 'Symfony\\Polyfill\\Mbstring\\' =&gt; 26, 'Symfony\\Component\\Yaml\\' =&gt; 23, 'Symfony\\Component\\VarDumper\\' =&gt; 28, ... ), ...); public static $prefixDirsPsr4 = array ( 'phpDocumentor\\Reflection\\' =&gt; array ( 0 =&gt; __DIR__ . '/..' . '/phpdocumentor/reflection-common/src', 1 =&gt; __DIR__ . '/..' . '/phpdocumentor/type-resolver/src', 2 =&gt; __DIR__ . '/..' . '/phpdocumentor/reflection-docblock/src', ), 'Symfony\\Polyfill\\Mbstring\\' =&gt; array ( 0 =&gt; __DIR__ . '/..' . '/symfony/polyfill-mbstring', ), 'Symfony\\Component\\Yaml\\' =&gt; array ( 0 =&gt; __DIR__ . '/..' . '/symfony/yaml', ), ...) PSR4 标准顶级命名空间映射用了两个数组，第一个是用命名空间第一个字母作为前缀索引，然后是 顶级命名空间，但是最终并不是文件路径，而是 顶级命名空间的长度。为什么呢？ 因为 PSR4 标准是用顶级命名空间目录替换顶级命名空间，所以获得顶级命名空间的长度很重要。 具体说明这些数组的作用： 假如我们找 Symfony\Polyfill\Mbstring\example 这个命名空间，通过前缀索引和字符串匹配我们得到了 12&lt;?php 'Symfony\\Polyfill\\Mbstring\\' =&gt; 26, 这条记录，键是顶级命名空间，值是命名空间的长度。拿到顶级命名空间后去 $prefixDirsPsr4数组 获取它的映射目录数组：(注意映射目录可能不止一条) 1234&lt;?php 'Symfony\\Polyfill\\Mbstring\\' =&gt; array ( 0 =&gt; __DIR__ . '/..' . '/symfony/polyfill-mbstring', ) 然后我们就可以将命名空间 Symfony\Polyfill\Mbstring\example 前26个字符替换成目录 DIR . ‘/..’ . ‘/symfony/polyfill-mbstring ，我们就得到了DIR . ‘/..’ . ‘/symfony/polyfill-mbstring/example.php，先验证磁盘上这个文件是否存在，如果不存在接着遍历。如果遍历后没有找到，则加载失败。 ClassLoader 接口初始化（ PHP &lt; 5.6 ）如果PHP版本低于 5.6 或者使用 HHVM 虚拟机环境，那么就要使用核心类的接口进行初始化。 1234567891011121314151617&lt;?php // PSR0 标准 $map = require __DIR__ . '/autoload_namespaces.php'; foreach ($map as $namespace =&gt; $path) &#123; $loader-&gt;set($namespace, $path); &#125; // PSR4 标准 $map = require __DIR__ . '/autoload_psr4.php'; foreach ($map as $namespace =&gt; $path) &#123; $loader-&gt;setPsr4($namespace, $path); &#125; $classMap = require __DIR__ . '/autoload_classmap.php'; if ($classMap) &#123; $loader-&gt;addClassMap($classMap); &#125; PSR4 标准的映射autoload_psr4.php 的顶级命名空间映射 123456789101112131415161718&lt;?php return array( 'XdgBaseDir\\' =&gt; array($vendorDir . '/dnoegel/php-xdg-base-dir/src'), 'Webmozart\\Assert\\' =&gt; array($vendorDir . '/webmozart/assert/src'), 'TijsVerkoyen\\CssToInlineStyles\\' =&gt; array($vendorDir . '/tijsverkoyen/css-to-inline-styles/src'), 'Tests\\' =&gt; array($baseDir . '/tests'), 'Symfony\\Polyfill\\Mbstring\\' =&gt; array($vendorDir . '/symfony/polyfill-mbstring'), ... ) PSR4 标准的初始化接口: 12345678910111213141516&lt;?php public function setPsr4($prefix, $paths) &#123; if (!$prefix) &#123; $this-&gt;fallbackDirsPsr4 = (array) $paths; &#125; else &#123; $length = strlen($prefix); if ('\\' !== $prefix[$length - 1]) &#123; throw new \InvalidArgumentException( "A non-empty PSR-4 prefix must end with a namespace separator." ); &#125; $this-&gt;prefixLengthsPsr4[$prefix[0]][$prefix] = $length; $this-&gt;prefixDirsPsr4[$prefix] = (array) $paths; &#125; &#125; 总结下上面的顶级命名空间映射过程： 12( 前缀 -&gt; 顶级命名空间，顶级命名空间 -&gt; 顶级命名空间长度 )( 顶级命名空间 -&gt; 目录 ) 这两个映射数组。具体形式也可以查看下面的 autoload_static 的 $prefixLengthsPsr4 、 $prefixDirsPsr4 。 命名空间映射autoload_classmap 123456789&lt;?phppublic static $classMap = array ( 'App\\Console\\Kernel' =&gt; __DIR__ . '/../..' . '/app/Console/Kernel.php', 'App\\Exceptions\\Handler' =&gt; __DIR__ . '/../..' . '/app/Exceptions/Handler.php', ...) addClassMap 123456789&lt;?php public function addClassMap(array $classMap) &#123; if ($this-&gt;classMap) &#123; $this-&gt;classMap = array_merge($this-&gt;classMap, $classMap); &#125; else &#123; $this-&gt;classMap = $classMap; &#125; &#125; 自动加载核心类 ClassLoader 的静态初始化到这里就完成了！ 其实说是5部分，真正重要的就两部分——初始化与注册。初始化负责顶层命名空间的目录映射，注册负责实现顶层以下的命名空间映射规则 第四部分 —— 注册讲完了 Composer 自动加载功能的启动与初始化，经过启动与初始化，自动加载核心类对象已经获得了顶级命名空间与相应目录的映射，也就是说，如果有命名空间 App\Console\Kernel，我们已经可以找到它对应的类文件所在位置。那么，它是什么时候被触发去找的呢？ 现在我们开始引导类的第四部分：注册自动加载核心类对象。我们来看看核心类的 register() 函数： 1234public function register($prepend = false)&#123; spl_autoload_register(array($this, 'loadClass'), true, $prepend);&#125; 其实奥秘都在自动加载核心类 ClassLoader 的 loadClass() 函数上： 12345678public function loadClass($class) &#123; if ($file = $this-&gt;findFile($class)) &#123; includeFile($file); return true; &#125; &#125; 这个函数负责按照 PSR 标准将顶层命名空间以下的内容转为对应的目录，也就是上面所说的将 ‘App\Console\Kernel 中’ Console\Kernel 这一段转为目录，至于怎么转的在下面 “运行”的部分讲。核心类 ClassLoader 将 loadClass() 函数注册到PHP SPL中的 spl_autoload_register() 里面去。这样，每当PHP遇到一个不认识的命名空间的时候，PHP会自动调用注册到 spl_autoload_register 里面的 loadClass() 函数，然后找到命名空间对应的文件 全局函数的自动加载Composer 不止可以自动加载命名空间，还可以加载全局函数。怎么实现的呢？把全局函数写到特定的文件里面去，在程序运行前挨个 require就行了。这个就是 composer 自动加载的第五步，加载全局函数。 12345678if ($useStaticLoader) &#123; $includeFiles = Composer\Autoload\ComposerStaticInit7b790917ce8899df9af8ed53631a1c29::$files;&#125; else &#123; $includeFiles = require __DIR__ . '/autoload_files.php';&#125;foreach ($includeFiles as $fileIdentifier =&gt; $file) &#123; composerRequire7b790917ce8899df9af8ed53631a1c29($fileIdentifier, $file);&#125; 跟核心类的初始化一样，全局函数自动加载也分为两种：静态初始化和普通初始化，静态加载只支持PHP5.6以上并且不支持HHVM。 静态初始化：ComposerStaticInit7b790917ce8899df9af8ed53631a1c29::$files： 12345public static $files = array ('0e6d7bf4a5811bfa5cf40c5ccd6fae6a' =&gt; __DIR__ . '/..' . '/symfony/polyfill-mbstring/bootstrap.php','667aeda72477189d0494fecd327c3641' =&gt; __DIR__ . '/..' . '/symfony/var-dumper/Resources/functions/dump.php',...); 普通初始化autoload_files: 12345678$vendorDir = dirname(dirname(__FILE__));$baseDir = dirname($vendorDir); return array('0e6d7bf4a5811bfa5cf40c5ccd6fae6a' =&gt; $vendorDir . '/symfony/polyfill-mbstring/bootstrap.php','667aeda72477189d0494fecd327c3641' =&gt; $vendorDir . '/symfony/var-dumper/Resources/functions/dump.php', ....); 其实跟静态初始化区别不大。 加载全局函数123456789101112131415161718class ComposerAutoloaderInit7b790917ce8899df9af8ed53631a1c29&#123; public static function getLoader()&#123; ... foreach ($includeFiles as $fileIdentifier =&gt; $file) &#123; composerRequire7b790917ce8899df9af8ed53631a1c29($fileIdentifier, $file); &#125; ... &#125;&#125;function composerRequire7b790917ce8899df9af8ed53631a1c29($fileIdentifier, $file) &#123; if (empty(\$GLOBALS['__composer_autoload_files'][\$fileIdentifier])) &#123; require $file; $GLOBALS['__composer_autoload_files'][$fileIdentifier] = true; &#125;&#125; 第五部分 —— 运行到这里，终于来到了核心的核心—— composer 自动加载的真相，命名空间如何通过 composer 转为对应目录文件的奥秘就在这一章。前面说过，ClassLoader 的 register() 函数将 loadClass() 函数注册到 PHP 的 SPL 函数堆栈中，每当 PHP 遇到不认识的命名空间时就会调用函数堆栈的每个函数，直到加载命名空间成功。所以 loadClass() 函数就是自动加载的关键了。 看下 loadClass() 函数: 1234567891011121314151617181920212223242526272829303132333435363738public function loadClass($class)&#123; if ($file = $this-&gt;findFile($class)) &#123; includeFile($file); return true; &#125;&#125;public function findFile($class)&#123; // work around for PHP 5.3.0 - 5.3.2 https://bugs.php.net/50731 if ('\\' == $class[0]) &#123; $class = substr($class, 1); &#125; // class map lookup if (isset($this-&gt;classMap[$class])) &#123; return $this-&gt;classMap[$class]; &#125; if ($this-&gt;classMapAuthoritative) &#123; return false; &#125; $file = $this-&gt;findFileWithExtension($class, '.php'); // Search for Hack files if we are running on HHVM if ($file === null &amp;&amp; defined('HHVM_VERSION')) &#123; $file = $this-&gt;findFileWithExtension($class, '.hh'); &#125; if ($file === null) &#123; // Remember that this class does not exist. return $this-&gt;classMap[$class] = false; &#125; return $file;&#125; 我们看到 loadClass() ，主要调用 findFile() 函数。findFile() 在解析命名空间的时候主要分为两部分：classMap 和 findFileWithExtension() 函数。classMap 很简单，直接看命名空间是否在映射数组中即可。麻烦的是 findFileWithExtension() 函数，这个函数包含了 PSR0 和 PSR4 标准的实现。还有个值得我们注意的是查找路径成功后 includeFile() 仍然是外面的函数，并不是 ClassLoader 的成员函数，原理跟上面一样，防止有用户写 $this 或 self。还有就是如果命名空间是以\开头的，要去掉\然后再匹配。 看下 findFileWithExtension 函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private function findFileWithExtension($class, $ext)&#123; // PSR-4 lookup $logicalPathPsr4 = strtr($class, '\\', DIRECTORY_SEPARATOR) . $ext; $first = $class[0]; if (isset($this-&gt;prefixLengthsPsr4[$first])) &#123; foreach ($this-&gt;prefixLengthsPsr4[$first] as $prefix =&gt; $length) &#123; if (0 === strpos($class, $prefix)) &#123; foreach ($this-&gt;prefixDirsPsr4[$prefix] as $dir) &#123; if (file_exists($file = $dir . DIRECTORY_SEPARATOR . substr($logicalPathPsr4, $length))) &#123; return $file; &#125; &#125; &#125; &#125; &#125; // PSR-4 fallback dirs foreach ($this-&gt;fallbackDirsPsr4 as $dir) &#123; if (file_exists($file = $dir . DIRECTORY_SEPARATOR . $logicalPathPsr4)) &#123; return $file; &#125; &#125; // PSR-0 lookup if (false !== $pos = strrpos($class, '\\')) &#123; // namespaced class name $logicalPathPsr0 = substr($logicalPathPsr4, 0, $pos + 1) . strtr(substr($logicalPathPsr4, $pos + 1), '_', DIRECTORY_SEPARATOR); &#125; else &#123; // PEAR-like class name $logicalPathPsr0 = strtr($class, '_', DIRECTORY_SEPARATOR) . $ext; &#125; if (isset($this-&gt;prefixesPsr0[$first])) &#123; foreach ($this-&gt;prefixesPsr0[$first] as $prefix =&gt; $dirs) &#123; if (0 === strpos($class, $prefix)) &#123; foreach ($dirs as $dir) &#123; if (file_exists($file = $dir . DIRECTORY_SEPARATOR . $logicalPathPsr0)) &#123; return $file; &#125; &#125; &#125; &#125; &#125; // PSR-0 fallback dirs foreach ($this-&gt;fallbackDirsPsr0 as $dir) &#123; if (file_exists($file = $dir . DIRECTORY_SEPARATOR . $logicalPathPsr0)) &#123; return $file; &#125; &#125; // PSR-0 include paths. if ($this-&gt;useIncludePath &amp;&amp; $file = stream_resolve_include_path($logicalPathPsr0)) &#123; return $file; &#125;&#125; 最后小结我们通过举例来说下上面代码的流程： 如果我们在代码中写下 new phpDocumentor\Reflection\Element()，PHP 会通过 SPL_autoload_register 调用 loadClass -&gt; findFile -&gt; findFileWithExtension。步骤如下： 将 \ 转为文件分隔符/，加上后缀php，变成 $logicalPathPsr4, 即 phpDocumentor/Reflection//Element.php; 利用命名空间第一个字母p作为前缀索引搜索 prefixLengthsPsr4 数组，查到下面这个数组： 12345p&apos; =&gt; array ( &apos;phpDocumentor\\Reflection\\&apos; =&gt; 25, &apos;phpDocumentor\\Fake\\&apos; =&gt; 19, ) 遍历这个数组，得到两个顶层命名空间 phpDocumentor\Reflection\ 和 phpDocumentor\Fake\ 在这个数组中查找 phpDocumentor\Reflection\Element，找出 phpDocumentor\Reflection\ 这个顶层命名空间并且长度为25。 在prefixDirsPsr4 映射数组中得到phpDocumentor\Reflection\ 的目录映射为： 123456&apos;phpDocumentor\\Reflection\\&apos; =&gt; array ( 0 =&gt; __DIR__ . &apos;/..&apos; . &apos;/phpdocumentor/reflection-common/src&apos;, 1 =&gt; __DIR__ . &apos;/..&apos; . &apos;/phpdocumentor/type-resolver/src&apos;, 2 =&gt; __DIR__ . &apos;/..&apos; . &apos;/phpdocumentor/reflection-docblock/src&apos;, ), 遍历这个映射数组，得到三个目录映射； 查看 “目录+文件分隔符//+substr(&dollar;logicalPathPsr4, &dollar;length)”文件是否存在，存在即返回。这里就是‘DIR/../phpdocumentor/reflection-common/src + substr(phpDocumentor/Reflection/Element.php,25)’ 如果失败，则利用 fallbackDirsPsr4 数组里面的目录继续判断是否存在文件 以上就是 composer 自动加载的原理解析]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>自动加载</tag>
        <tag>PSR4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）PHP 7 新特性]]></title>
    <url>%2F2017%2F08%2F21%2Fdocs%2F03-php%2Fphp7-new-features%2F</url>
    <content type="text"><![CDATA[标量类型声明PHP 7 中的函数的形参类型声明可以是标量了。在 PHP 5 中只能是类名、接口、array 或者 callable (PHP 5.4，即可以是函数，包括匿名函数)，现在也可以使用 string、int、float和 bool 了。 12345678&lt;?php// 强制模式function sumOfInts(int ...$ints)&#123; return array_sum($ints);&#125;var_dump(sumOfInts(2, '3', 4.1)); 以上实例会输出： 1int(9) 需要注意的是上文提到的严格模式的问题在这里同样适用：强制模式（默认，既强制类型转换）下还是会对不符合预期的参数进行强制类型转换，严格模式下则触发 TypeError 的致命错误。 返回值类型声明PHP 7 增加了对返回类型声明的支持。 类似于参数类型声明，返回类型声明指明了函数返回值的类型。可用的类型与参数声明中可用的类型相同。 12345678910&lt;?phpfunction arraysSum(array ...$arrays): array&#123; return array_map(function(array $array): int &#123; return array_sum($array); &#125;, $arrays);&#125;print_r(arraysSum([1,2,3], [4,5,6], [7,8,9])); 以上实例会输出： 123456Array( [0] =&gt; 6 [1] =&gt; 15 [2] =&gt; 24) NULL 合并运算符由于日常使用中存在大量同时使用三元表达式和 isset()的情况，NULL 合并运算符使得变量存在且值不为NULL， 它就会返回自身的值，否则返回它的第二个操作数。 实例如下 12345&lt;?php// 如果 $_GET['user'] 不存在返回 'nobody'，否则返回 $_GET['user'] 的值$username = $_GET['user'] ?? 'nobody';// 类似的三元运算符$username = isset($_GET['user']) ? $_GET['user'] : 'nobody'; 太空船操作符（组合比较符）太空船操作符用于比较两个表达式。当$a大于、等于或小于$b时它分别返回-1、0或1。 实例如下 123456789101112131415&lt;?php// 整型echo 1 &lt;=&gt; 1; // 0echo 1 &lt;=&gt; 2; // -1echo 2 &lt;=&gt; 1; // 1// 浮点型echo 1.5 &lt;=&gt; 1.5; // 0echo 1.5 &lt;=&gt; 2.5; // -1echo 2.5 &lt;=&gt; 1.5; // 1 // 字符串echo "a" &lt;=&gt; "a"; // 0echo "a" &lt;=&gt; "b"; // -1echo "b" &lt;=&gt; "a"; // 1 通过 define() 定义常量数组实例如下： 12345678&lt;?phpdefine('ANIMALS', [ 'dog', 'cat', 'bird']);echo ANIMALS[1]; // 输出 "cat" 匿名类现在支持通过new class 来实例化一个匿名类，实例如下： 12345678910111213141516171819202122232425&lt;?phpinterface Logger &#123; public function log(string $msg);&#125;class Application &#123; private $logger; public function getLogger(): Logger &#123; return $this-&gt;logger; &#125; public function setLogger(Logger $logger) &#123; $this-&gt;logger = $logger; &#125;&#125;$app = new Application;$app-&gt;setLogger(new class implements Logger &#123; public function log(string $msg) &#123; echo $msg; &#125;&#125;);var_dump($app-&gt;getLogger()); 以上实例会输出： 12object(class@anonymous)#2 (0) &#123;&#125; Unicode codepoint 转译语法这接受一个以16进制形式的 Unicode codepoint，并打印出一个双引号或heredoc包围的 UTF-8 编码格式的字符串。 可以接受任何有效的 codepoint，并且开头的 0 是可以省略的。 123echo "\u&#123;aa&#125;";echo "\u&#123;0000aa&#125;";echo "\u&#123;9999&#125;"; 以上实例会输出： 123ªª (same as before but with optional leading 0's)香 Closure::call()Closure::call() 现在有着更好的性能，简短干练的暂时绑定一个方法到对象上闭包并调用它。 1234567891011&lt;?phpclass A &#123;private $x = 1;&#125;// Pre PHP 7 代码$getXCB = function() &#123;return $this-&gt;x;&#125;;$getX = $getXCB-&gt;bindTo(new A, 'A'); // intermediate closureecho $getX();// PHP 7+ 代码$getX = function() &#123;return $this-&gt;x;&#125;;echo $getX-&gt;call(new A); 以上实例会输出： 1211 为unserialize()提供过滤这个特性旨在提供更安全的方式解包不可靠的数据。它通过白名单的方式来防止潜在的代码注入。 12345678910&lt;?php// 转换对象为 __PHP_Incomplete_Class 对象$data = unserialize($foo, ["allowed_classes" =&gt; false]);// 转换对象为 __PHP_Incomplete_Class 对象，除了 MyClass 和 MyClass2$data = unserialize($foo, ["allowed_classes" =&gt; ["MyClass", "MyClass2"]);// 默认接受所有类$data = unserialize($foo, ["allowed_classes" =&gt; true]); IntlChar新增加的 IntlChar 类旨在暴露出更多的 ICU 功能。这个类自身定义了许多静态方法用于操作多字符集的 unicode 字符。 1234&lt;?phpprintf('%x', IntlChar::CODEPOINT_MAX);echo IntlChar::charName('@');var_dump(IntlChar::ispunct('!')); 以上实例会输出： 12310ffffCOMMERCIAL ATbool(true) 若要使用此类，请先安装Intl扩展 预期预期是向后兼用并增强之前的 assert() 的方法。 它使得在生产环境中启用断言为零成本，并且提供当断言失败时抛出特定异常的能力。 123456&lt;?phpini_set('assert.exception', 1);class CustomError extends AssertionError &#123;&#125;assert(false, new CustomError('Some error message')); 以上实例会输出： 1Fatal error: Uncaught CustomError: Some error message use 加强从同一 namespace 导入的类、函数和常量现在可以通过单个 use 语句 一次性导入了。 12345678910111213141516171819&lt;?php// PHP 7 之前版本用法use some\namespace\ClassA;use some\namespace\ClassB;use some\namespace\ClassC as C;use function some\namespace\fn_a;use function some\namespace\fn_b;use function some\namespace\fn_c;use const some\namespace\ConstA;use const some\namespace\ConstB;use const some\namespace\ConstC;// PHP 7+ 用法use some\namespace\&#123;ClassA, ClassB, ClassC as C&#125;;use function some\namespace\&#123;fn_a, fn_b, fn_c&#125;;use const some\namespace\&#123;ConstA, ConstB, ConstC&#125;; Generator 加强增强了Generator的功能，这个可以实现很多先进的特性 1234567891011121314151617181920212223&lt;?php&lt;?phpfunction gen()&#123; yield 1; yield 2; yield from gen2();&#125;function gen2()&#123; yield 3; yield 4;&#125;foreach (gen() as $val)&#123; echo $val, PHP_EOL;&#125;?&gt; 以上实例会输出： 12341234 整除新增了整除函数 intdiv(),使用实例： 12&lt;?phpvar_dump(intdiv(10, 3)); 以上实例会输出： 1int(3) 更多内容可以参考：http://www.runoob.com/php/php7-new-features.html。原文地址：https://www.runoob.com/w3cnote/php7-new-features.html]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP7</tag>
        <tag>新特性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 分片]]></title>
    <url>%2F2017%2F06%2F14%2Fdocs%2F05-nosql%2Fmongodb-sharding%2F</url>
    <content type="text"><![CDATA[简介在Mongodb里面存在另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。当MongoDB存储海量的数据时，一台机器可能不足以存储数据，也可能不足以提供可接受的读写吞吐量。这时，我们就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 为什么使用片 复制所有的写入操作到主节点 延迟的敏感数据会在主节点查询 单个副本集限制在12个节点 当请求量巨大时会出现内存不足。 本地磁盘不足 垂直扩展价格昂贵 MongoDB分片集群组成MongoDB分片群集主要有如下三个主要组件： Shard：分片服务器，用于存储实际的数据块，实际生产环境中一个shard server角色可由几台服务器组成一个Replica Set 承担，防止主机节点故障。 Config Server：配置服务器，存储了整个分片群集的配置信息，其中包括chunk信息。 Routers：前端路由，客户端由此接入，且让整个群集看上去像单一数据库，前端应用可以透明使用。 下图展示了在MongoDB中使用分片集群结构分布： 分片集群的简单配置在这里在一台物理服务器上部署一个简单结构的MongoDB分片集群： 1台路由实例（端口27017）1台配置实例（端口37017）3台Shard实例（端口47017、47018、47019） 安装Mongodb具体安装参考MongoDB 基础教程 123456$ mkdir -p /usr/local/mongodb/data/db&#123;1,2,3,4&#125; # 创建数据存储目录 $ mkdir /usr/local/mongodb/data/logs # 创建日志文件存储目录 $ touch /usr/local/mongodb/data/logs/mongodb&#123;1,2,3,4&#125;.log # 创建日志文件 $ chmod -R 777 /usr/local/mongodb/data/logs/*.log $ ulimit -n 25000 # 最多打开文件个数，重启后失效 $ ulimit -u 25000 # 最多打开进程数，重启后失效 配置服务器123$ cd /usr/local/mongodb/data$ mkdir conf$ vim mongodb1.conf 12345678port=37017 # 端口号 dbpath=/usr/local/mongodb/data/db/db1 # 数据存储目录 logpath=/usr/local/mongodb/data/logs/mongodb1.log # 日志文件存储目录 logappend=truefork=truemaxConns=5000storageEngine=mmapv1configsvr=true 分片服务器三台分片服务器配置相同，只需更改端口号、数据存储目录和日志存储目录即可； 123$ cp -p mongodb1.conf mongodb2.conf$ vim mongodb2.conf 12345678port=47017 # 端口号dbpath=/usr/local/mongodb/data/db/db2 # 数据存储目录logpath=/usr/local/mongodb/data/logs/mongodb2.log # 日志文件存储目录logappend=truefork=truemaxConns=5000storageEngine=mmapv1shardsvr=true 重复以上步骤分别配置其他分片服务器 启动服务器 1234$ mongod -f /usr/local/mongodb/data/conf/mongodb1.conf$ mongod -f /usr/local/mongodb/data/conf/mongodb2.conf$ mongod -f /usr/local/mongodb/data/conf/mongodb3.conf$ mongod -f /usr/local/mongodb/data/conf/mongodb4.conf 启动路由服务器123456$ ./mongos --port 27017 --fork --logpath=/usr/local/mongodb/data/log/route.log --configdb 192.168.33.12:37017 --chunkSize 1#--port指定对方连接入口#--fork后台运行#--logpath指定日志文件存储路径#--configdb指定给谁处理 启用分片服务器12345$ ./bin/mongo&gt; sh.addShard("192.168.27.153:47017")&gt; sh.addShard("192.168.27.153:47018")&gt; sh.status() 启用分片存储功能123456&gt; use kgc&gt; for (var i=1;i&lt;=50000;i++)db.users.insert(&#123;"id":i,"name":"zhangsan"+i&#125;)&gt; db.users.createIndex(&#123;"id":1&#125;) #对users表创建索引&gt; sh.enableSharding("kgc") #启用kgc数据库分片&gt; sh.shardCollection("kgc.users",&#123;"id":1&#125;) #表分片&gt; sh.status() 给分片添加标签123&gt; sh.addShardTag("shard0000","test01")&gt; sh.addShardTag("shard0001","test02")&gt; sh.status() 添加或删除分片服务器1234&gt; sh.addShard("192.168.33.12:47019")&gt; use admin&gt; db.runCommand(&#123;"removeshard":"192.168.27.153:47019"&#125;)&gt; sh.status() 基本操作查看数据分布 12&gt; use kgc&gt; db.users.getShardDistribution() 查看集合是否分片 1&gt; db.collectionName.stats().sharded # 简单的返回true或者false TODO分片策略 参考文档 https://www.runoob.com/mongodb/mongodb-sharding.htmlhttps://blog.51cto.com/13659182/2149307https://www.jianshu.com/p/cb55bb333e2d]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 复制（副本集）]]></title>
    <url>%2F2017%2F06%2F13%2Fdocs%2F05-nosql%2Fmongodb-replication%2F</url>
    <content type="text"><![CDATA[简介MongoDB复制是将数据同步在多个服务器的过程。复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。复制还允许您从硬件故障和服务中断中恢复数据。Mongodb复制集由一组Mongod实例（进程）组成，包含一个Primary节点和多个Secondary节点。Mongodb Driver（客户端）的所有数据都写入Primary，Secondary从Primary同步写入的数据，以保持复制集内所有成员存储相同的数据集，实现数据的高可用。 使用场景 数据冗余，用做故障恢复使用，当发生硬件故障或者其它原因造成的宕机时，可以使用副本进行恢复。 读写分离，读的请求分流到副本上，减轻主节点的读压力。 mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。mongodb各个节点常见的搭配方式为：一主一从、一主多从。主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 一个典型的副本集架构如下图所示： 以上结构图中，客户端从主节点读取数据，在客户端写入数据到主节点时， 主节点与从节点进行数据交互保障数据的一致性。 MongoDB副本集设置通过指定 –replSet 选项来启动mongoDB。–replSet 基本语法格式如下： 1mongod --port &quot;PORT&quot; --dbpath &quot;YOUR_DB_DATA_PATH&quot; --replSet &quot;REPLICA_SET_INSTANCE_NAME&quot; 实例 1./bin/mongod --port=27017 --dbpath=./data/db/ --replSet=rs0 以上实例会启动一个名为rs0的MongoDB实例，其端口号为27017。启动后打开命令提示框并连接上mongoDB服务。在Mongo客户端使用命令rs.initiate()来启动一个新的副本集。我们可以使用rs.conf()来查看副本集的配置。查看副本集状态使用rs.status()命令 。 副本集特征： N 个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复 副本集添加成员添加副本集的成员，我们需要使用多台服务器来启动mongo服务。进入Mongo客户端，并使用rs.add()方法来添加副本集的成员。 1&gt; rs.add(HOST_NAME:PORT) MongoDB中你只能通过主节点将Mongo服务添加到副本集中， 判断当前运行的Mongo服务是否为主节点可以使用命令db.isMaster() 。MongoDB的副本集与我们常见的主从有所不同，主从在主机宕机后所有服务将停止，而副本集在主机宕机后，副本会接管主节点成为主节点，不会出现宕机的情况。 副本集角色 主节点（Primary） 接收所有的写请求，然后把修改同步到所有Secondary。一个Replica Set只能有一个Primary节点，当Primary挂掉后，其他Secondary或者Arbiter节点会重新选举出来一个主节点。默认读请求也是发到Primary节点处理的，可以通过修改客户端连接配置以支持读取Secondary节点。 副本节点（Secondary） 与主节点保持同样的数据集。当主节点挂掉的时候，参与选主。 仲裁者（Arbiter） 不保有数据，不参与选主，只进行选主投票。使用Arbiter可以减轻数据存储的硬件需求，Arbiter几乎没什么大的硬件资源需求，但重要的一点是，在生产环境下它和其他数据节点不要部署在同一台机器上。 两种架构模式 PSS Primary + Secondary + Secondary模式，通过Primary和Secondary搭建的Replica SetDiagram of a 3 member replica set that consists of a primary and two secondaries. 该模式下 Replica Set节点数必须为奇数，目的是选主投票的时候要出现大多数才能进行选主决策。 PSA Primary + Secondary + Arbiter模式，使用Arbiter搭建Replica Set 偶数个数据节点，加一个Arbiter构成的Replica Set 选举机制复制集通过 replSetInitiate 命令或 rs.initiate() 命令进行初始化。初始化后各个成员间开始发送心跳消息，并发起 Primary 选举操作，获得大多数成员投票支持的节点，会成为 Primary，其余节点成为 Secondary。 123456789config = &#123; _id : &quot;my_replica_set&quot;, members : [ &#123;_id : 0, host : &quot;rs1.example.net:27017&quot;&#125;, &#123;_id : 1, host : &quot;rs2.example.net:27017&quot;&#125;, &#123;_id : 2, host : &quot;rs3.example.net:27017&quot;&#125;, ]&#125;rs.initiate(config) 大多数假设复制集内投票成员（后续介绍）数量为 N，则大多数为 N/2 + 1，当复制集内存活成员数量不足大多数时，整个复制集将无法选举出 Primary，复制集将无法提供写服务，处于只读状态 关于大多数的计算如下表所示 投票成员数 大多数 容忍失效数 1 1 0 2 2 0 3 2 1 4 3 1 5 3 2 Mongodb副本集的选举基于Bully算法，这是一种协调者竞选算法，详细解析可以参考这里Primary 的选举受节点间心跳、优先级、最新的 oplog 时间等多种因素影响。官方文档对于选举机制的说明选举机制的说明 特殊角色 ArbiterArbiter 节点只参与投票，不能被选为 Primary，并且不从 Primary 同步数据。当节点宕机导致复制集无法选出 Primary时，可以给复制集添加一个 Arbiter 节点，即使有节点宕机，仍能选出 Primary。Arbiter 本身不存储数据，是非常轻量级的服务，当复制集成员为偶数时，最好加入一个 Arbiter 节点，以提升复制集可用性。 Priority0Priority0节点的选举优先级为0，不会被选举为 Primary。比如你跨机房 A、B 部署了一个复制集，并且想指定 Primary 必须在 A 机房，这时可以将 B 机房的复制集成员 Priority 设置为0，这样 Primary 就一定会是 A 机房的成员。（注意：如果这样部署，最好将大多数节点部署在 A 机房，否则网络分区时可能无法选出 Primary。） Vote0Mongodb 3.0里，复制集成员最多50个，参与 Primary 选举投票的成员最多7个，其他成员（Vote0）的 vote 属性必须设置为0，即不参与投票。 HiddenHidden 节点不能被选为主（Priority 为0），并且对 Driver 不可见。因 Hidden 节点不会接受 Driver 的请求，可使用 Hidden 节点做一些数据备份、离线计算的任务，不会影响复制集的服务。 DelayedDelayed 节点必须是 Hidden 节点，并且其数据落后与 Primary 一段时间（可配置，比如1个小时）。因 Delayed 节点的数据比 Primary 落后一段时间，当错误或者无效的数据写入 Primary 时，可通过 Delayed 节点的数据来恢复到之前的时间点。 触发选举条件 初始化一个副本集时。 从库不能连接到主库(默认超过10s，可通过heartbeatTimeoutSecs参数控制)，由从库发起选举 主库放弃primary 角色，比如执行rs.stepdown 命令 Mongodb副本集通过心跳检测实现自动failover机制，进而实现高可用 MongoDB复制流程Primary 与 Secondary 之间通过 oplog 来同步数据，Primary 上的写操作完成后，会向特殊的 local.oplog.rs 特殊集合写入一条 oplog，Secondary 不断的从 Primary 取新的 oplog 并应用。因 oplog 的数据会不断增加，local.oplog.rs 被设置成为一个 capped 集合，当容量达到配置上限时，会将最旧的数据删除掉。另外考虑到 oplog 在 Secondary 上可能重复应用，oplog 必须具有幂等性，即重复应用也会得到相同的结果。如下 oplog 的格式，包含 ts、h、op、ns、o 等字段。 123456789101112&#123; &quot;ts&quot; : Timestamp(1446011584, 2), &quot;h&quot; : NumberLong(&quot;1687359108795812092&quot;), &quot;v&quot; : 2, &quot;op&quot; : &quot;i&quot;, &quot;ns&quot; : &quot;test.nosql&quot;, &quot;o&quot; : &#123; &quot;_id&quot; : ObjectId(&quot;563062c0b085733f34ab4129&quot;), &quot;name&quot; : &quot;mongodb&quot;, &quot;score&quot; : &quot;100&quot; &#125;&#125; 属性 说明 ts 操作时间，当前 timestamp + 计数器，计数器每秒都被重置 h 操作的全局唯一标识 v oplog 版本信息 op 操作类型 op.i 插入操作 op.u 更新操作 op.d 删除操作 op.c 执行命令（如 createDatabase，dropDatabase） op.n 空操作，特殊用途 ns 操作针对的集合 o 操作内容 o2 操作查询条件，仅 update 操作包含该字段。 Secondary 初次同步数据时，会先执行 init sync，从 Primary（或其他数据更新的 Secondary）同步全量数据，然后不断通过执行tailable cursor从 Primary 的 local.oplog.rs 集合里查询最新的 oplog 并应用到自身。 异常回滚 当 Primary 宕机时，如果有数据未同步到 Secondary，当 Primary 重新加入时，如果新的 Primary 上已经发生了写操作，则旧 Primary 需要回滚部分操作，以保证数据集与新的 Primary 一致。旧 Primary 将回滚的数据写到单独的 rollback 目录下，数据库管理员可根据需要使用 mongorestore 进行恢复 读写配置默认情况下，复制集的所有读请求都发到 Primary，Driver 可通过设置 Read Preference 来将读请求路由到其他的节点。 primary：默认规则，所有读请求发到 Primary； primaryPreferred：Primary 优先，如果 Primary 不可达，请求 Secondary； secondary：所有的读请求都发到 secondary； secondaryPreferred：Secondary 优先，当所有 Secondary 不可达时，请求 Primary； nearest：读请求发送到最近的可达节点上（通过 ping 探测得出最近的节点）。 关于read-preference Write Concern默认情况下，Primary 完成写操作即返回，Driver 可通过设置 Write Concern 来设置写成功的规则。如下的 write concern 规则设置写必须在大多数节点上成功，超时时间为5s。 1234db.products.insert( &#123; item: &quot;envelopes&quot;, qty : 100, type: &quot;Clasp&quot; &#125;, &#123; writeConcern: &#123; w: majority, wtimeout: 5000 &#125; &#125;) 关于write-concern 参考文档 https://www.runoob.com/mongodb/mongodb-replication.htmlhttps://www.cnblogs.com/littleatp/p/8562842.htmlhttp://www.mongoing.com/archives/5200]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 基础教程]]></title>
    <url>%2F2017%2F06%2F11%2Fdocs%2F05-nosql%2Fmongodb-tutorial-base%2F</url>
    <content type="text"><![CDATA[MongoDB 简介什么是MongoDBMongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。在高负载的情况下，添加更多地节点，可以保证服务器的性能。MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。MongoDB 将数据存储为一个文档，数据结构由键值（key=&gt;value）对组成。MongoDB 文档类似于JSON对象。字段值可以包含其他文档，数组以及文档数据。 主要特点 MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。 你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。 你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。 如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。 Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。 MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。 Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。 Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。 Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。 GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。 MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。 MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 MongoDB安装简单。 安装Linux平台安装MongoDBMongoDB 提供了linux各发行版本64位的安装包，你可以在官网下载安装包。下载地址：https://www.mongodb.com/download-center/community 下载完安装包，并解压 tgz（以下演示的是 64 位 Linux上的安装） 。 123$ wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.6.tgz$ tar -zxvf mongodb-linux-x86_64-3.0.6.tgz$ mv mongodb-linux-x86_64-3.0.6/ mongodb MongoDB 的可执行文件位于 bin 目录下，所以可以将其添加到 PATH 路径中： 1$ export PATH=mongodb-install-directory/bin:$PATH mongodb-install-directory 为你 MongoDB 的安装路径。如本文的 /usr/local/mongodb 。 创建数据目录MongoDB的数据存储在data目录的db目录下，但是这个目录在安装过程不会自动创建，所以你需要手动创建data目录，并在data目录中创建db目录。 以下实例中我们将data目录创建于/usr/local/mongodb/目录下。 1$ mkdir -p /usr/local/mongodb/data/db 启动MongoDB服务你可以再命令行中执行mongodb安装目录中的bin目录执行mongod命令来启动mongdb服务。 注意：如果你的数据库目录不是/data/db（MongoDB 默认的启动的数据库路径），可以通过 –dbpath 来指定。 1$ mongod --dbpath=/usr/local/mongodb/data/db MongoDB后台管理Shell如果你需要进入MongoDB后台管理，你需要先打开mongodb装目录的下的bin目录，然后执行mongo命令文件。MongoDB Shell是MongoDB自带的交互式Javascript shell,用来对MongoDB进行操作和管理的交互式环境。当你进入mongoDB后台后，它默认会链接到 test 文档（数据库）： 1234567891011$ mongoMongoDB shell version: 3.0.6connecting to: testWelcome to the MongoDB shell.For interactive help, type "help".For more comprehensive documentation, see http://docs.mongodb.org/Questions? Try the support group http://groups.google.com/group/mongodb-user&gt; 现在让我们插入一些简单的数据，并对插入的数据进行检索： 12345&gt; db.runoob.insert(&#123;test:1&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.runoob.find()&#123; &quot;_id&quot; : ObjectId(&quot;5d26b208168b6593d96e6387&quot;), &quot;test&quot; : 1 &#125;&gt; OK，至此我们的MongoDB已经安装完成。 概念解析不管我们学习什么数据库都应该学习其中的基础概念，在mongodb中基本的概念是文档、集合、数据库，下面我们挨个介绍。下表将帮助您更容易理解Mongo中的一些概念： SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table join 表连接，MongoDB不支持 primary key primary key 主键，MongoDB自动将_id字段设置为主键 数据库一个mongodb中可以建立多个数据库。MongoDB的默认数据库为”db”，该数据库存储在data目录中。MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。 有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。 admin： 从权限的角度来看，这是”root”数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。 local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合 config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 文档(Document)文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。 一个简单的文档例子如下： 1&#123;"site":"www.axkeson.com", "name":"Axkeson"&#125; 需要注意的是： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 集合集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。 集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 比如，我们可以将以下不同数据结构的文档插入到集合中： 123&#123;&quot;site&quot;:&quot;www.baidu.com&quot;&#125;&#123;&quot;site&quot;:&quot;www.google.com&quot;,&quot;name&quot;:&quot;Google&quot;&#125;&#123;&quot;site&quot;:&quot;www.axkeson.com&quot;, &quot;name&quot;:&quot;Axkeson&quot;&#125; 当第一个文档插入时，集合就会被创建。 capped collections Capped collections 就是固定大小的collection。它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 “RRD” 概念类似。Capped collections 是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能和标准的 collection 不同，你必须要显式的创建一个capped collection，指定一个 collection 的大小，单位是字节。collection 的数据存储空间值提前分配的。Capped collections 可以按照文档的插入顺序保存到集合中，而且这些文档在磁盘上存放位置也是按照插入顺序来保存的，所以当我们更新Capped collections 中文档的时候，更新后的文档不可以超过之前文档的大小，这样话就可以确保所有文档在磁盘上的位置一直保持不变。由于 Capped collection 是按照文档的插入顺序而不是使用索引确定插入位置，这样的话可以提高增添数据的效率。MongoDB 的操作日志文件 oplog.rs 就是利用 Capped Collection 来实现的。要注意的是指定的存储大小包含了数据库的头信息。 1&gt; db.createCollection(&quot;mycoll&quot;, &#123;capped:true, size:100000&#125;) 在 capped collection 中，你能添加新的对象。 能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。 使用 Capped Collection 不能删除一个文档，可以使用 drop() 方法删除 collection 所有的行。 删除之后，你必须显式的重新创建这个 collection。 常用操作数据库创建数据库 1use &lt;DATABASE_NAME&gt; 查看所有数据库 1show dbs 查看当数据库 1db 删除数据库 1db.dropDatabase() 注意: 在 MongoDB 中，集合只有在内容插入后才会创建! 就是说，创建集合(数据表)后要再插入一个文档(记录)，集合才会真正创建。 集合创建集合 1db.createCollection(COLLECTION_NAME, options) 查看已有集合 1show collections 或 show tables 删除集合 1db.COLLECTION_NAME.drop() 实例 1234&gt; db.createCollection(&quot;mycol&quot;, &#123; capped : true, autoIndexId : true, size : 6142800, max : 10000 &#125; )&#123; &quot;ok&quot; : 1 &#125;&gt; 在 MongoDB 中，你不需要创建集合。当你插入一些文档时，MongoDB 会自动创建集合。 文档插入文档 1db.COLLECTION_NAME.insert(document) 更新文档 123456789db.COLLECTION_NAME.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;) 参数说明： query : update的查询条件，类似sql update查询内where后面的。 update : update的对象和一些更新的操作符（如$,$inc…）等，也可以理解为sql update查询内set后面的 upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入 multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 writeConcern :可选，抛出异常的级别。 实例 1234567891011121314151617181920212223242526&gt;db.col.insert(&#123; title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;Axkeson教程&apos;, url: &apos;http://www.axkeson.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;)&gt;db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.col.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;56064f89ade2f21f36b03136&quot;), &quot;title&quot; : &quot;MongoDB&quot;, &quot;description&quot; : &quot;MongoDB 是一个 Nosql 数据库&quot;, &quot;by&quot; : &quot;Axkeson教程&quot;, &quot;url&quot; : &quot;http://www.axkeson.com&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;database&quot;, &quot;NoSQL&quot; ], &quot;likes&quot; : 100&#125;&gt; save() 方法 通过传入的文档来替换已有文档。语法格式如下：参数说明： document : 文档数据。 writeConcern :可选，抛出异常的级别。 123456db.COLLECTION_NAME.save( &lt;document&gt;, &#123; writeConcern: &lt;document&gt; &#125;) 更多实例 12345db.col.update( &#123; &quot;count&quot; : &#123; $gt : 1 &#125; &#125; , &#123; $set : &#123; &quot;test2&quot; : &quot;OK&quot;&#125; &#125; ); // 只更新第一条记录：db.col.update( &#123; &quot;count&quot; : &#123; $gt : 3 &#125; &#125; , &#123; $set : &#123; &quot;test2&quot; : &quot;OK&quot;&#125; &#125;,false,true ); // 全部更新：db.col.update( &#123; &quot;count&quot; : &#123; $gt : 5 &#125; &#125; , &#123; $set : &#123; &quot;test5&quot; : &quot;OK&quot;&#125; &#125;,true,true ); // 全部添加进去:db.col.update( &#123; &quot;count&quot; : &#123; $gt : 15 &#125; &#125; , &#123; $inc : &#123; &quot;count&quot; : 1&#125; &#125;,false,true ); // 全部更新：db.col.update( &#123; &quot;count&quot; : &#123; $gt : 10 &#125; &#125; , &#123; $inc : &#123; &quot;count&quot; : 1&#125; &#125;,false,false ); // 只更新第一条记录 删除文档 1234db.COLLECTION_NAME.remove( &lt;query&gt;, &lt;justOne&gt;) 查询文档 123db.COLLECTION_NAME.findOne(query, projection)db.COLLECTION_NAME.find(query, projection)db.COLLECTION_NAME.find(query, projection).pretty() 索引创建索引 1db.COLLECTION_NAME.createIndex(keys, options) 参考文档 https://docs.mongodb.com/manual/https://www.runoob.com/mongodb/mongodb-replication.html]]></content>
      <categories>
        <category>NoSQL</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[限制IP某个时间段内访问的次数]]></title>
    <url>%2F2017%2F05%2F21%2Fdocs%2F03-php%2Fphp-restricted-access%2F</url>
    <content type="text"><![CDATA[PHP 结合Redis限制IP某个时间段内访问的次数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?php/** * PHP 结合Redis限制IP某个时间段内访问的次数 */$redis = new Redis();$redis-&gt;connect('127.0.0.1');$key = getRealIp();//限制次数为10次。$limit = 10;if ($redis-&gt;exists($key)) &#123; $redis-&gt;incr($key); $count = $redis-&gt;get($key); // 超出请求次数限制 冻结该IP访问2分钟 if ($count == $limit) &#123; $redis-&gt;expire($key, 120); &#125; if ($count &gt; $limit) &#123; exit('超出访问限制次数'); &#125;&#125; else &#123; $redis-&gt;incr($key); // 限制时间为60秒 $redis-&gt;expire($key, 60);&#125;$count = $redis-&gt;get($key);exit(sprintf('Hello World！ %s request', $count));function getRealIp()&#123; if (isset($_SERVER)) &#123; if (isset($_SERVER['HTTP_X_FORWARDED_FOR'])) &#123; $realip = $_SERVER['HTTP_X_FORWARDED_FOR']; &#125; else &#123; if (isset($_SERVER['HTTP_CLIENT_IP'])) &#123; $realip = $_SERVER['HTTP_CLIENT_IP']; &#125; else &#123; $realip = $_SERVER['REMOTE_ADDR']; &#125; &#125; &#125; else &#123; if (getenv('HTTP_X_FORWARDED_FOR')) &#123; $realip = getenv('HTTP_X_FORWARDED_FOR'); &#125; else &#123; if (getenv('HTTP_CLIENT_IP')) &#123; $realip = getenv('HTTP_CLIENT_IP'); &#125; else &#123; $realip = getenv('REMOTE_ADDR'); &#125; &#125; &#125; return $realip;&#125; NGINXnginx可以通过ngx_http_limit_conn_module和ngx_http_limit_req_module配置来限制ip在同一时间段的访问次数 ngx_http_limit_conn_module：该模块用于限制每个定义的密钥的连接数，特别是单个IP​​地址的连接数．使用limit_conn_zone和limit_conn指令． ngx_http_limit_req_module：用于限制每一个定义的密钥的请求的处理速率，特别是从一个单一的IP地址的请求的处理速率。使用“泄漏桶”方法进行限制．指令：limit_req_zone和limit_req． ngx_http_limit_conn_module：限制单个IP的连接数示例 12345678910111213141516http &#123; limit_conn_zone $binary_remote_addr zone=addr：10m; #定义一个名为addr的limit_req_zone用来存储session，大小是10M内存， #以$binary_remote_addr 为key, #nginx 1.18以后用limit_conn_zone替换了limit_conn, #且只能放在http&#123;&#125;代码段． ... server &#123; ... location /download/ &#123; limit_conn addr 1; #连接数限制 #设置给定键值的共享内存区域和允许的最大连接数。超出此限制时，服务器将返回503（服务临时不可用）错误. ＃如果区域存储空间不足，服务器将返回503（服务临时不可用）错误 &#125; &#125;&#125; 可能有几个limit_conn指令,以下配置将限制每个客户端IP与服务器的连接数，同时限制与虚拟服务器的总连接数： 12345678910http &#123; limit_conn_zone $binary_remote_addr zone=perip：10m; limit_conn_zone $server_name zone=perserver：10m ... server &#123; ... limit_conn perip 10; #单个客户端ip与服务器的连接数． limit_conn perserver 100; ＃限制与服务器的总连接数 &#125;&#125; 参考文档：http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html ngx_http_limit_req_module：限制某一时间内，单一IP的请求数 123456789101112131415161718192021222324http &#123; limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; ... #定义一个名为one的limit_req_zone用来存储session，大小是10M内存， #以$binary_remote_addr 为key,限制平均每秒的请求为1个， #1M能存储16000个状态，rete的值必须为整数， server &#123; ... location /search/ &#123; limit_req zone=one burst=5; #限制每ip每秒不超过1个请求，漏桶数burst为5,也就是队列． #nodelay，如果不设置该选项，严格使用平均速率限制请求数，超过的请求被延时处理． #举个栗子： ＃设置rate=20r/s每秒请求数为２０个，漏桶数burst为5个， #brust的意思就是，如果第1秒、2,3,4秒请求为19个，第5秒的请求为25个是被允许的，可以理解为20+5 #但是如果你第1秒就25个请求，第2秒超过20的请求返回503错误． ＃如果区域存储空间不足，服务器将返回503（服务临时不可用）错误 ＃速率在每秒请求中指定（r/s）。如果需要每秒少于一个请求的速率，则以每分钟的请求（r/m）指定。 &#125; &#125;&#125; 还可以限制来自单个IP地址的请求的处理速率，同时限制虚拟服务器的请求处理速率： 12345678910http &#123; limit_req_zone $binary_remote_addr zone=perip:10m rate=1r/s; limit_req_zone $server_name zone=perserver:10m rate=10r/s; ... server &#123; ... limit_req zone=perip burst=5 nodelay; #漏桶数为５个．也就是队列数．nodelay:不启用延迟． limit_req zone=perserver burst=10; #限制nginx的处理速率为每秒10个 &#125;&#125;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>访问限制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 垃圾回收机制]]></title>
    <url>%2F2017%2F01%2F19%2Fdocs%2F03-php%2Fphp-gc%2F</url>
    <content type="text"><![CDATA[简介在PHP中，没有任何变量指向这个对象时，这个对象就成为垃圾；PHP会将其在内存中销毁。这是PHP的 GC 垃圾回收机制，目的是防止内存溢出； PHP进行内存管理的核心算法一共两项：一是引用计数，二是写时拷贝。当你声明一个PHP变量的时候，C语言就在底层给你搞了一个叫做zval的struct（结构体）；如果你还给这个变量赋值了，比如“hello world”，那么C语言就在底层再给你搞一个叫做zend_value的union（联合体） PHP垃圾回收机制是 php5 之后才有的这个东西，php5.3 之前使用的垃圾回收机制是单纯的“引用计数”，就是每个内存对象都分配一个计数器，当内存对象被变量引用时，计数器+ 1;当变量引用撇掉后，计数器 -1 ；当计数器 =0 时，表名内存中对象没有被使用，该内存对象进行销毁，垃圾回收完成； php5.3开始，使用了新的垃圾回收机制，在引用计数基础上，实现了一种复杂的算法，来检测内存对象中 引用环 的存在，以避免内存泄露； 引用计数基本知识每个php变量存在一个叫”zval”的变量容器中。一个zval变量容器，除了包含变量的类型和值，还包括两个字节的额外信息。第一个是”is_ref”，是个bool值，用来标识这个变量是否是属于引用集合(reference set)。通过这个字节，php引擎才能把普通变量和引用变量区分开来，由于php允许用户通过使用&amp;来使用自定义引用，zval变量容器中还有一个内部引用计数机制，来优化内存使用。第二个额外字节是”refcount”，用以表示指向这个zval变量容器的变量(也称符号即symbol)个数。所有的符号存在一个符号表中，其中每个符号都有作用域(scope)，那些主脚本(比如：通过浏览器请求的的脚本)和每个函数或者方法也都有作用域。 12345678910zval &#123; string &quot;a&quot; //变量的名字是a value zend_value //变量的值 type string //变量是字符串类型&#125;zend_value &#123; string &quot;hello916&quot; //值的内容 refcount 1 //引用计数&#125; 12345678910111213141516171819&lt;?php $a = 'hello'. mt_rand( 1, 1000 );xdebug_debug_zval( 'a');$b = $a;xdebug_debug_zval('a');$c = $a;xdebug_debug_zval('a');unset( $c );xdebug_debug_zval( 'a');// 输出结果a: (refcount=1, is_ref=0)='hello517'a: (refcount=2, is_ref=0)='hello517'a: (refcount=3, is_ref=0)='hello517'a: (refcount=2, is_ref=0)='hello517' 其中，zval struct结构体用于保存$a，zend_value union联合体用于保存数据内容也就是’hello517’。由于后面又声明了b和c，所以C不得不又在底层给你搞出两个zval struct结构体来。 那么写时拷贝是什么意思呢？看下面代码： 123456789101112&lt;?php $a = 'hello'. mt_rand( 1, 1000 );$b = $a;xdebug_debug_zval( 'a');$a = 'world'. mt_rand( 2, 2000 );xdebug_debug_zval( 'a');// 输出结果a: (refcount=2, is_ref=0)='hello834'a: (refcount=1, is_ref=0)='world1198' 引用计数和写时拷贝，那么垃圾回收也该来了。当一个zval在被unset的时候、或者从一个函数中运行完毕出来（就是局部变量）的时候等等很多地方，都会产生zval与zend_value发生断开的行为，这个时候zend引擎需要检测的就是zend_value的refcount是否为0，如果为0，则直接KO free空出内容来。如果zend_value的recount不为0（废话一定是大于0），这个value不能被释放，但是也不代表这个zend_value是清白的，因为此zend_value依然可能是个垃圾。 什么样的情况会导致zend_value的refcount不为0，但是这个zend_value却是个垃圾呢？PHP7种两种情况： 数组：a数组的某个成员使用&amp;引用a自己对象：对象的某个成员引用对象自己 12345&lt;?php$arr = [ 1 ];$arr[] = &amp;$arr;unset( $arr ); 这种情况下，zend_value不会能释放，但也不能放过它，不然一定会产生内存泄漏，所以这会儿zend_value会被扔到一个叫做垃圾回收堆中，然后zend引擎会依次对垃圾回收堆中的这些zend_value进行二次检测，检测是不是由于上述两种情况造成的refcount为1但是自身却确实没有人再用了，如果一旦确定是上述两种情况造成的，那么就会将zend_value彻底抹掉释放内存。 那么垃圾回收发生在什么时候？有些同学可能有疑问，就是php不是运行一次就销毁了吗，我要着gc有何用？并不是啦，首先当一次fpm运行完毕后，最后一定还有gc的，这个销毁就是gc；其次是，内存都是即用即释放的，而不是攒着非得到最后，你想想一个典型的场景，你的控制器里的某个方法里用了一个函数，函数需要一个巨大的数组参数，然后函数还需要修改这个巨大的数组参数，你们应该是函数的运行范围里面修改这个数组，所以此时会发生写时拷贝了，当函数运行完毕后，就得赶紧释放掉这块儿内存以供给其他进程使用，而不是非得等到本地fpm request彻底完成后才销毁。 PHP5 和 PHP7的来及回收机制有什么不同PHP5 和 PHP7 的垃圾回收机制都属于引用计数，但是复杂数据类型的算法处理上：在PHP7中zval有了新的实现方式。 最基础的变化是*zval需要的内存不再是单独从堆上分配，不再自己存储引用计数复杂数据类型（比如字符串、数组和对象）的引用计数由其自身来存储 这种实现方式有以下好处： 简单数据类型不需要单独分配内存，也不需要计数； 不会再有两次计数的情况。在对象中，只有对象自身存储的计数是有效的； 由于现在计数由数值自身存储，所以也就可以和非 zval 结构的数据共享，比如 zval 和 hashtable key 之间； 相关文档 https://www.php.net/manual/zh/features.gc.phphttps://www.sohu.com/a/252341086_470018]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>垃圾回收</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 抽象类和接口的区别]]></title>
    <url>%2F2016%2F12%2F02%2Fdocs%2F03-php%2Fphp-abstract-and-interface-diff%2F</url>
    <content type="text"><![CDATA[在学习PHP面向对象时，都会在抽象类与接口上迷惑，作用差不多为什么还那么容易混淆，何不留一去一？但是事实上两者的区别还是很大的，如果能够很好地运用PHP的两个方法，面向对象的程序设计将会更加合理、清晰高效。 抽象类 abstract class是基于类来说，其本身就是类，只是一种特殊的类，不能直接实例，可以在类里定义方法，属性。类似于模版，规范后让子类实现详细功能 在抽象类中可以写非抽象的方法，从而避免在子类中重复书写他们，这样可以提高代码的复用性，这是抽象类的优势；接口中只能有抽象的方法 抽象类和方法要加 abstract 来修饰 抽象类内，即便全是具体方法，也不能够实例化，只要新建类来继承后，实例继承类才可以 接口主要基于方法的规范，有点像抽象类里的抽象方法，只是其相对于抽象方法来说，更加独立。可让某个类通过组合多个方法来形成新的类 接口可以让一个类一次性实现多个不同的方法 接口本身就是抽象的，但注意不是抽象类，因为接口不是类，只是其方法是抽象的。所以，其也是抽象的 1234567891011121314151617181920212223242526272829303132abstract class A &#123; abstract public function method1(); abstract public function method2();&#125;interface M &#123; public function method1(); public function method2();&#125;class N implements M &#123; public function method1()&#123;echo 'method1';&#125; public function method2()&#123;echo 'method2';&#125;&#125;$n = new N;$n-&gt;method1();$n-&gt;method2();abstract class B implements M &#123; public function method1()&#123;echo 'B method1';&#125; abstract public function method2();&#125;class C extends B &#123; public function method2()&#123;echo 'method2';&#125;&#125;$c = new C();$c-&gt;method1();$c-&gt;method2(); 总结相同点 都是上层的抽象层 都是抽象类，都不能被实例化 都包含抽象方法（抽象方法没有方法体，需要被实现，不能定义为 private） 不同点 对接口的使用方式是通过关键字implements来实现的，而对于抽象类的操作是使用类继承的关键字exotends实现的，使用时要特别注意 一个类可以实现多个 interface ，但一个类只能继承一个 abstract class interface 强调特定功能的实现，而 abstract class 强调所属关系 接口没有数据成员，但是抽象类有数据成员，抽象类可以实现数据的封装 接口没有构造函数，抽象类可以有构造函数 接口中的方法都是public类型，而抽象类中的方法可以使用protected或public来修饰 应用场合interface 类与类之间需要特定的接口进行协调，而不在乎其如何实现 作为能够实现特定功能的标识存在，也可以是什么接口方法都没有的纯粹标识 需要将一组类视为单一的类，而调用者只通过接口来与这组类发生联系 需要实现特定的多项功能，而这些功能之间可能完全没有任何联系 abstract class一句话，在既需要统一的接口，又需要实例变量或缺省的方法的情况下，就可以使用它。最常见的有 定义了一组接口，但又不想强迫每个实现类都必须实现所有的接口。可以用 abstract class 定义一组方法体，甚至可以是空方法体，然后由子类选择自己所感兴趣的方法来覆盖 某些场合下，只靠纯粹的接口不能满足类与类之间的协调，还必需类中表示状态的变量来区别不同的关系。 abstract 的中介作用可以很好地满足这一点 规范了一组相互协调的方法，其中一些方法是共同的，与状态无关的，可以共享的，无需子类分别实现；而另一些方法却需要各个子类根据自己特定的状态来实现特 定的功能]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>抽象类</tag>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）比较全面的MySQL优化参考]]></title>
    <url>%2F2016%2F11%2F05%2Fdocs%2F04-mysql%2Fmysql-optimization-reference%2F</url>
    <content type="text"><![CDATA[本文整理了一些MySQL的通用优化方法，做个简单的总结分享，旨在帮助那些没有专职MySQL DBA的企业做好基本的优化工作，至于具体的SQL优化，大部分通过加适当的索引即可达到效果，更复杂的就需要具体分析了，可以参考本站的一些优化案例。 硬件层相关优化CPU相关在服务器的BIOS设置中，可调整下面的几个配置，目的是发挥CPU最大性能，或者避免经典的NUMA问题 选择Performance Per Watt Optimized(DAPC)模式，发挥CPU最大性能，跑DB这种通常需要高运算量的服务就不要考虑节电了； 关闭C1E和C States等选项，目的也是为了提升CPU效率； Memory Frequency（内存频率）选择Maximum Performance（最佳性能）； 内存设置菜单中，启用Node Interleaving，避免NUMA问题； 磁盘I/O相关下面几个是按照IOPS性能提升的幅度排序，对于磁盘I/O可优化的一些措施： 使用SSD或者PCIe SSD设备，至少获得数百倍甚至万倍的IOPS提升； 购置阵列卡同时配备CACHE及BBU模块，可明显提升IOPS（主要是指机械盘，SSD或PCIe SSD除外。同时需要定期检查CACHE及BBU模块的健康状况，确保意外时不至于丢失数据）； 有阵列卡时，设置阵列写策略为WB，甚至FORCE WB（若有双电保护，或对数据安全性要求不是特别高的话），严禁使用WT策略。并且闭阵列预读策略，基本上是鸡肋，用处不大； 尽可能选用RAID-10，而非RAID-5； 使用机械盘的话，尽可能选择高转速的，例如选用15KRPM，而不是7.2KRPM的盘，不差几个钱的； 系统层相关优化文件系统层优化在文件系统层，下面几个措施可明显提升IOPS性能： 使用deadline/noop这两种I/O调度器，千万别用cfq（它不适合跑DB类服务）； 使用xfs文件系统，千万别用ext3；ext4勉强可用，但业务量很大的话，则一定要用xfs； 文件系统mount参数中增加：noatime, nodiratime, nobarrier几个选项（nobarrier是xfs文件系统特有的）； 其他内核参数优化针对关键内核参数设定合适的值，目的是为了减少swap的倾向，并且让内存和磁盘I/O不会出现大幅波动，导致瞬间波峰负载： 将vm.swappiness设置为5-10左右即可，甚至设置为0（RHEL 7以上则慎重设置为0，除非你允许OOM kill发生），以降低使用SWAP的机会； 将vm.dirty_background_ratio设置为5-10，将vm.dirty_ratio设置为它的两倍左右，以确保能持续将脏数据刷新到磁盘，避免瞬间I/O写，产生严重等待（和MySQL中的innodb_max_dirty_pages_pct类似）； 将net.ipv4.tcp_tw_recycle、net.ipv4.tcp_tw_reuse都设置为1，减少TIME_WAIT，提高TCP效率； 至于网传的read_ahead_kb、nr_requests这两个参数，我经过测试后，发现对读写混合为主的OLTP环境影响并不大（应该是对读敏感的场景更有效果），不过没准是我测试方法有问题，可自行斟酌是否调整； MySQL层相关优化关于版本选择官方版本我们称为ORACLE MySQL，这个没什么好说的，相信绝大多数人会选择它。 我个人强烈建议选择Percona分支版本，它是一个相对比较成熟的、优秀的MySQL分支版本，在性能提升、可靠性、管理型方面做了不少改善。它 和官方ORACLE MySQL版本基本完全兼容，并且性能大约有20%以上的提升，因此我优先推荐它，我自己也从2008年一直以它为主。 另一个重要的分支版本是MariaDB，说MariaDB是分支版本其实已经不太合适了，因为它的目标是取代ORACLE MySQL。它主要在原来的MySQL Server层做了大量的源码级改进，也是一个非常可靠的、优秀的分支版本。但也由此产生了以GTID为代表的和官方版本无法兼容的新特性（MySQL 5.7开始，也支持GTID模式在线动态开启或关闭了），也考虑到绝大多数人还是会跟着官方版本走，因此没优先推荐MariaDB。 关于最重要的参数选项调整建议建议调整下面几个关键参数以获得较好的性能（可使用my.cnf生成器生成配置文件模板）： 选择Percona或MariaDB版本的话，强烈建议启用thread pool特性，可使得在高并发的情况下，性能不会发生大幅下降。此外，还有extra_port功能，非常实用，关键时刻能救命的。还有另外一个重要特色是 QUERY_RESPONSE_TIME 功能，也能使我们对整体的SQL响应时间分布有直观感受； 设置default-storage-engine=InnoDB，也就是默认采用InnoDB引擎，强烈建议不要再使用MyISAM引擎了，InnoDB引擎绝对可以满足99%以上的业务场景； 调整innodb_buffer_pool_size大小，如果是单实例且绝大多数是InnoDB引擎表的话，可考虑设置为物理内存的50% ~ 70%左右； 根据实际需要设置innodb_flush_log_at_trx_commit、sync_binlog的值。如果要求数据不能丢失，那么两个都设为1。如果允许丢失一点数据，则可分别设为2和10。而如果完全不用care数据是否丢失的话（例如在slave上，反正大不了重做一次），则可都设为0。这三种设置值导致数据库的性能受到影响程度分别是：高、中、低，也就是第一个会另数据库最慢，最后一个则相反； 设置innodb_file_per_table = 1，使用独立表空间，我实在是想不出来用共享表空间有什么好处了； 设置innodb_data_file_path = ibdata1:1G:autoextend，千万不要用默认的10M，否则在有高并发事务时，会受到不小的影响； 设置innodb_log_file_size=256M，设置innodb_log_files_in_group=2，基本可满足90%以上的场景； 设置long_query_time = 1，而在5.5版本以上，已经可以设置为小于1了，建议设置为0.05（50毫秒），记录那些执行较慢的SQL，用于后续的分析排查； 根据业务实际需要，适当调整max_connection（最大连接数）、max_connection_error（最大错误数，建议设置为10万以上，而open_files_limit、innodb_open_files、table_open_cache、table_definition_cache这几个参数则可设为约10倍于max_connection的大小； 常见的误区是把tmp_table_size和max_heap_table_size设置的比较大，曾经见过设置为1G的，这2个选项是每个连接会话都会分配的，因此不要设置过大，否则容易导致OOM发生；其他的一些连接会话级选项例如：sort_buffer_size、join_buffer_size、read_buffer_size、read_rnd_buffer_size等，也需要注意不能设置过大； 由于已经建议不再使用MyISAM引擎了，因此可以把key_buffer_size设置为32M左右，并且强烈建议关闭query cache功能； 关于Schema设计规范及SQL使用建议下面列举了几个常见有助于提升MySQL效率的Schema设计规范及SQL使用建议： 所有的InnoDB表都设计一个无业务用途的自增列做主键，对于绝大多数场景都是如此，真正纯只读用InnoDB表的并不多，真如此的话还不如用TokuDB来得划算； 字段长度满足需求前提下，尽可能选择长度小的。此外，字段属性尽量都加上NOT NULL约束，可一定程度提高性能； 尽可能不使用TEXT/BLOB类型，确实需要的话，建议拆分到子表中，不要和主表放在一起，避免SELECT * 的时候读性能太差。 读取数据时，只选取所需要的列，不要每次都SELECT *，避免产生严重的随机读问题，尤其是读到一些TEXT/BLOB列； 对一个VARCHAR(N)列创建索引时，通常取其50%（甚至更小）左右长度创建前缀索引就足以满足80%以上的查询需求了，没必要创建整列的全长度索引； 通常情况下，子查询的性能比较差，建议改造成JOIN写法； 多表联接查询时，关联字段类型尽量一致，并且都要有索引； 多表连接查询时，把结果集小的表（注意，这里是指过滤后的结果集，不一定是全表数据量小的）作为驱动表； 多表联接并且有排序时，排序字段必须是驱动表里的，否则排序列无法用到索引； 多用复合索引，少用多个独立索引，尤其是一些基数（Cardinality）太小（比如说，该列的唯一值总数少于255）的列就不要创建独立索引了； 类似分页功能的SQL，建议先用主键关联，然后返回结果集，效率会高很多； 其他建议关于MySQL的管理维护的其他建议有： 通常地，单表物理大小不超过10GB，单表行数不超过1亿条，行平均长度不超过8KB，如果机器性能足够，这些数据量MySQL是完全能处理的过来的，不用担心性能问题，这么建议主要是考虑ONLINE DDL的代价较高； 不用太担心mysqld进程占用太多内存，只要不发生OOM kill和用到大量的SWAP都还好； 在以往，单机上跑多实例的目的是能最大化利用计算资源，如果单实例已经能耗尽大部分计算资源的话，就没必要再跑多实例了； 定期使用pt-duplicate-key-checker检查并删除重复的索引。定期使用pt-index-usage工具检查并删除使用频率很低的索引； 定期采集slow query log，用pt-query-digest工具进行分析，可结合Anemometer系统进行slow query管理以便分析slow query并进行后续优化工作； 可使用pt-kill杀掉超长时间的SQL请求，Percona版本中有个选项 innodb_kill_idle_transaction 也可实现该功能； 使用pt-online-schema-change来完成大表的ONLINE DDL需求； 定期使用pt-table-checksum、pt-table-sync来检查并修复mysql主从复制的数据差异； 写在最后：这次的优化参考，大部分情况下我都介绍了适用的场景，如果你的应用场景和本文描述的不太一样，那么建议根据实际情况进行调整，而不是生搬硬套。欢迎质疑拍砖，但拒绝不经过大脑的习惯性抵制。 本文转载自：http://imysql.com/2015/05/24/mysql-optimization-reference-1.shtml]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 事务详解]]></title>
    <url>%2F2016%2F10%2F06%2Fdocs%2F04-mysql%2Fmysql-transaction%2F</url>
    <content type="text"><![CDATA[简介MySQL事务主要用于处理操作量大，复杂度高的数据。由一步或几步数据库操作序列组成逻辑执行单元，这系列操作要么全部执行，要么全部放弃执行。在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。事务用来管理 insert,update,delete 语句。 事务的基本特征（ACID）一般来说，事务是必须满足4个条件的（ACID） 原子性： Atomicity，或称不可分割性。一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务的执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性： Consistency，在事务开始之前和事务结束以后，数据库的完整性没有破坏。这表示写入的资料必须完全符合所有的预设规则，这包括资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性： Isolation，又称独立性。数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同的级别，包扩未提交（Read uncommitted）、读提交（Read committed）、可重复读（Repeatable read）和串行化（Serializable）。 持久性： Durability，事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务的隔离级别为什么需要隔离性如果事务之间不是互相隔离的，可能将会出现以下问题。 脏读 脏读（dirty read），简单来说，就是一个事务在处理过程中读取了另外一个事务未提交的数据。这种未提交的数据我们称之为脏数据。依据脏数据所做的操作肯定是不正确的。 不可重复读 不可重复读（non-repeatable read），是指一个事务范围内，多次查询某个数据，却得到不同的结果。在第一个事务中的两次读取数据之间，由于第二个事务的修改，第一个事务两次读到的数据可能就是不一样的。 幻读 幻读（plantom read），是事务非独立执行时发生的一种现象。例如事务 T1 对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作，这时事务 T2 又对这个表中插入了一行数据项为“1”的数据，并且提交给数据库，而操作事务 T1 的用户如果再查看刚刚修改的数据，会发现数据怎么还是 1？其实这行是从事务 T2 中添加的，就好像产生幻觉一样，这就是发生了幻读。 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 四种隔离级别为了解决上面可能出现的问题，我们就需要设置隔离级别，也就是事务之间按照什么规则进行隔离，将事务隔离到什么程度。 首先，需要明白一点，隔离程度越强，越能保证数据的完整性和一致性，但是付出的代价却是并发执行效率的低下。 ANSI/ISO SQL 定义了 4 种标准隔离级别： Serializable（串行化） 我的事务尚未提交，别人就别想改数据。 花费最高代价但最可靠的事务隔离级别。“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。事务 100% 隔离，可避免脏读、不可重复读、幻读的发生。 Repeatable read（可重复读，默认级别） 别人改数据的事务已经提交，我在我的事务中也不去读。 多次读取同一范围的数据会返回第一次查询的快照，即使其他事务对该数据做了更新修改。事务在执行期间看到的数据前后必须是一致的。但如果这个事务在读取某个范围内的记录时，其他事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行，这就是幻读。 Read committed (读已提交) 别人改数据的事务已经提交，我在我的事务中才能读到。 保证一个事务提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。可避免脏读的发生，但是可能会造成不可重复读。大多数数据库的默认级别就是 Read committed，比如 Sql Server , Oracle。 Read uncommitted (读未提交) 别人改数据的事务尚未提交，我在我的事务中也能读到。 最低的事务隔离级别，一个事务还没提交时，它做的变更就能被别的事务看到。任何情况都无法保证。 隔离级别与一致性关系 隔离级别 脏读 不可重复读 幻读 Read uncommitted 可能 可能 可能 Read committed 不可能 可能 可能 Repeatable Read 不可能 不可能 可能 Serializable 不可能 不可能 不可能 隔离级别的一些基本操作设置事务隔离级别 可以在my.ini文件中使用transaction-isolation选项来设置服务器的缺省事务隔离级别 该选项值可以是 12345678– READ-UNCOMMITTED– READ-COMMITTED– REPEATABLE-READ– SERIALIZABLE# 例如：[mysqld]transaction-isolation = READ-COMMITTED 通过命令动态设置隔离级别 隔离级别也可以在运行的服务器中动态设置，应使用SET TRANSACTION ISOLATION LEVEL语句 123456789101112131415SET [GLOBAL | SESSION] TRANSACTION ISOLATION LEVEL &lt;isolation-level&gt; 其中的&lt;isolation-level&gt;可以是： – READ UNCOMMITTED – READ COMMITTED – REPEATABLE READ – SERIALIZABLE# 例如： • 事务隔离级别的作用范围分为两种： – 全局级：对所有的会话有效 – 会话级：只对当前的会话有效 SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED； # 会话级SET GLOBAL TRANSACTION ISOLATION LEVEL READ COMMITTED； # 全局级 查看隔离级别1mysql&gt; select @@tx_isolation; 隔离级别的实现事务的机制是通过视图（read-view）来实现的并发版本控制（MVCC），不同的事务隔离级别创建读视图的时间点不同。 可重复读是每个事务重建读视图，整个事务存在期间都用这个视图。 读已提交是每条 SQL 创建读视图，在每个 SQL 语句开始执行的时候创建的。隔离作用域仅限该条 SQL 语句。 读未提交是不创建，直接返回记录上的最新值 串行化隔离级别下直接用加锁的方式来避免并行访问。 这里的视图可以理解为数据副本，每次创建视图时，将当前已持久化的数据创建副本，后续直接从副本读取，从而达到数据隔离效果。 事务控制语句 BEGIN 或 START TRANSACTION 显式地开启一个事务； COMMIT 也可以使用 COMMIT WORK，不过二者是等价的。COMMIT 会提交事务，并使已对数据库进行的所有修改成为永久性的； ROLLBACK 也可以使用 ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier，SAVEPOINT 允许在事务中创建一个保存点，一个事务中可以有多个 SAVEPOINT； RELEASE SAVEPOINT identifier 删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier 把事务回滚到标记点； SET TRANSACTION 用来设置事务的隔离级别。InnoDB 存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748mysql&gt; use RUNOOB;Database changedmysql&gt; CREATE TABLE runoob_transaction_test( id int(5)) engine=innodb; # 创建数据表Query OK, 0 rows affected (0.04 sec) mysql&gt; select * from runoob_transaction_test;Empty set (0.01 sec) mysql&gt; begin; # 开始事务Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into runoob_transaction_test value(5);Query OK, 1 rows affected (0.01 sec) mysql&gt; insert into runoob_transaction_test value(6);Query OK, 1 rows affected (0.00 sec) mysql&gt; commit; # 提交事务Query OK, 0 rows affected (0.01 sec) mysql&gt; select * from runoob_transaction_test;+------+| id |+------+| 5 || 6 |+------+2 rows in set (0.01 sec) mysql&gt; begin; # 开始事务Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into runoob_transaction_test values(7);Query OK, 1 rows affected (0.00 sec) mysql&gt; rollback; # 回滚Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from runoob_transaction_test; # 因为回滚所以数据没有插入+------+| id |+------+| 5 || 6 |+------+2 rows in set (0.01 sec) mysql&gt; PHP中使用事务实例 1234567891011121314151617181920212223242526&lt;?php$dbhost = 'localhost:3306'; // mysql服务器主机地址$dbuser = 'root'; // mysql用户名$dbpass = '123456'; // mysql用户名密码$conn = mysqli_connect($dbhost, $dbuser, $dbpass);if(! $conn )&#123; die('连接失败: ' . mysqli_error($conn));&#125;// 设置编码，防止中文乱码mysqli_query($conn, "set names utf8");mysqli_select_db( $conn, 'RUNOOB' );mysqli_query($conn, "SET AUTOCOMMIT=0"); // 设置为不自动提交，因为MYSQL默认立即执行mysqli_begin_transaction($conn); // 开始事务定义 if(!mysqli_query($conn, "insert into runoob_transaction_test (id) values(8)"))&#123; mysqli_query($conn, "ROLLBACK"); // 判断当执行失败时回滚&#125; if(!mysqli_query($conn, "insert into runoob_transaction_test (id) values(9)"))&#123; mysqli_query($conn, "ROLLBACK"); // 判断执行失败时回滚&#125;mysqli_commit($conn); //执行事务mysqli_close($conn); 什么是大事务 定义 运行时间比较长，操作的数据比较多的事务。 大事务风险 锁定太多的数据，造成大量的阻塞和锁超时，回滚所需要的时间比较长。执行时间长，容易造成主从延迟 如何处理大事务 避免一次处理太多大数据。移出不必要在事务中的select操作。 相关文章 http://blog.itpub.net/31559358/viewspace-2221931https://www.runoob.com/mysql/mysql-transaction.htmlhttps://blog.csdn.net/w_linux/article/details/79666086https://blog.csdn.net/changudeng1992/article/details/81988927]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 面向对象和面向过程的区别]]></title>
    <url>%2F2016%2F09%2F24%2Fdocs%2F03-php%2Fphp-oop-pop-diff%2F</url>
    <content type="text"><![CDATA[摘要PHP程序编写分为面向过程和面向对象。两者在功能实现上没有区别，但是在代码编写上区别很大，面向过程的代码很散，不易管理和维护，而面向对象把常用的功能封装为一个类，易于维护和管理。 PHP编程风格之争面向对象与面向过程在许多编程语言中只能使用二者之一来进行编程，但是PHP语言与其他编程语言有不同之处，那就是我们可以自由的选择或者将PHP面向对象和PHP面向过程二者混用。这就经常引起开发者们讨论选择哪种开发风格。 面向对象优点在于扩展性和封装。仅仅用面向对象的方式来写代码不会为你的代码产生文档，但它可以鼓励你为之添加文档。并且，为了易于扩展，你可能会写一个API。 当下比较流行的两个面向对象编写的php程序：Smarty和FPDF。主要使用面向对象的编码方式。Smarty和FPDF都提供了带有良好文档的API来扩展主类。这说明了在类的内部组织方法和数据的必要性–有时同样的功能可以用函数和全局变量来完成，但这样不易于扩展。并且，使用对象对跟踪和保持PDF或HTML文档的风格非常有帮助，你可以将同样的数据用不同的格式来发布。Smarty和FPDF都是使用对象来建立灵活实用的类库的极好的例子。 面向对象面向对象，是把一些常用的操作进行类封装起来，方便调用，需要用的地方，调用一下即可，这样开发方便，维护也方便！修改这个封装的类，即可达到修改全站的目的！ 面向对象三大特性（封装，继承，多态）使得在做复杂的事情的时候效率和正确率得到保证。 面向过程面向过程，就是把代码封装成子过程或函数，在每一个地方都使用单独的代码进行操作，如果开发代码重复多就比较累赘，维护的时候相对麻烦些，你修改了哪里，就只在哪里起作用。 面向过程优点在于运行起来快。当下比较流行的两个面向过程的PHP程序：OsCommerce和PhpMyAdmin。主要使用面向过程的编码方式。它们构建起来很快，运行起来也很快。两者都很自然地采用嵌入HTML的方法。 以上两个使用面向过程风格的程序都有非常好的文档和代码注释。OsCommerce提供的开发框架可以增加维护性和扩展性。但是两者都没有提供API，不能扩展程序到另外的体系中。 如果你想把OsCommerce整合到一个帐单程序中，需要花费大量的时间和精力，就像扩展PhpMyAdmin成一个供客户使用的后端管理工具。不过从它们设计的目的来看，确实在各自的领域中都表现地很出色。 两种编程方式各有千秋 Smarty和FPDF的实用性和扩展性 osCommerce和phpMyAdmin的运行速度和良好表现 这种选择还包括对PHP的一些基础开发。PECL和PEAR都收到了很多赞扬和批评。我想这两个项目为阐明PHP面向过程和面向对象编程的区别提供了很好的例子。 PECl提供了PHP的扩展库，用C和面向过程的方式开发，注重速度和简洁精炼。通常，这些都是从已经存在的LGPL软件中移植而来，其中许多有趣的特性已经加入PHP。毕竟，PHP是用C写的。 PEAR则贡献了很多有趣的类如建立Excel表或改变DNS记录等。使用PEAR类库可以为你节约大量时间，甚至可以让你在不怎么熟悉PHP的情况进行开发—“我不理解但它能用！”。 哪我们编写程序时选择哪方式为主呢？一个项目开始的时候，首先要寻求实际的编码目的和方向。这个项目的实现目标是什么？下面是可能是答案。 开发快，发布快（开发效率） 尽可能快地运行（运行效率） 易于维护，改进和扩展（维护效率） 发布一个API 第1、2个方向倾向于使用过程化的风格，而最后两个倾向于使用PHP面向对象的风格。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>OOP</tag>
        <tag>面向对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 中 echo print print_r() var_dump() 的区别]]></title>
    <url>%2F2016%2F08%2F27%2Fdocs%2F03-php%2Fphp-output-way%2F</url>
    <content type="text"><![CDATA[简述echo可以一次输出多个值，多个值之间用逗号分隔。echo是语言结构(language construct)，而并不是真正的函数，因此不能作为表达式的一部分使用。 print()函数print()打印一个值（它的参数），如果字符串成功显示则返回true，否则返回false。 print_r()可以把字符串和数字简单地打印出来，而数组则以括起来的键和值得列表形式显示，并以Array开头。但print_r()输出布尔值和NULL的结果没有意义，因为都是打印”\n”。因此用var_dump()函数更适合调试。 var_dump()判断一个变量的类型与长度,并输出变量的数值,如果变量有值输的是变量的值并回返数据类型。此函数显示关于一个或多个表达式的结构信息，包括表达式的类型与值。数组将递归展开值，通过缩进显示其结构。 疑问它们都有返回值吗 echo 没有返回值，返回会报错 print 有返回值 print_r() 有返回值 var_dump()有返回值，但是返回值是NULL 关于带不带括号 echo 可带，可不带 print 可带，可不带 print_r() 必须带 var_dump() 必须带 可否输出多个变量 echo 可以（不可加括号） print 不可以 print_r() 不可以 var_dump() 可以 可以打印的数据类型 echo: 可以输出「字符型，整形，浮点型，布尔型，资源」；不可以输出「数组，对象」（会报错）不会输出「NULL」（不会报错） print: 可以输出「字符型，整形，浮点型，布尔型，资源」；不可以输出「数组，对象」（会报错）不会输出「NULL」（不会报错）和echo表现一致 print_r: 可以输出「字符型，整形，浮点型，布尔型，数组，对象，资源」不会输出“「NULL」（不会报错） var_dump: 可以输出「字符型，整形，浮点型，布尔型，数组，对象，资源，NULL」]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>echo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 获取指定文件下所有文件]]></title>
    <url>%2F2016%2F08%2F09%2Fdocs%2F03-php%2Fphp-list-file%2F</url>
    <content type="text"><![CDATA[scandir123456789101112131415161718&lt;?phpfunction getFiles($path)&#123; foreach (scandir($path) as $file) &#123; if ($file == '.' || $file == '..') &#123; continue; &#125; if (is_dir($file)) &#123; getFiles($path.'/'.$file); &#125; else &#123; echo $path.'/'.$file.PHP_EOL; &#125; &#125;&#125;getFiles(__DIR__); glob12345678910111213function getFiles($path)&#123; foreach (glob($path) as $file) &#123; if (is_dir($file)) &#123; getFiles($file.'/*'); &#125; else &#123; echo $file.PHP_EOL; &#125; &#125;&#125;getFiles(__DIR__); readdir123456789101112131415161718192021function getFiles($path)&#123; echo $path; $handle = opendir($path); while (false !== ($file = readdir($handle))) &#123; if ($file == "." || $file == "..") &#123; continue; &#125; if (is_dir($file)) &#123; getFiles($path.'/'.$file); &#125; echo $path.'/'.$file.PHP_EOL; &#125; closedir($handle);&#125;getFiles(__DIR__); 迭代器12345678910111213141516function getFiles($path)&#123; $iterator = new RecursiveIteratorIterator(new RecursiveDirectoryIterator($path)); foreach ($iterator as $file =&gt; $fileInfo) &#123; $name = $fileInfo-&gt;getFilename(); if ($name == '.' || $name == '..') &#123; continue; &#125; echo $name.PHP_EOL; &#125;&#125;getFiles(__DIR__);]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>file</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHP 截取文件后缀的几种方法]]></title>
    <url>%2F2016%2F05%2F12%2Fdocs%2F03-php%2Fphp-get-file-ext%2F</url>
    <content type="text"><![CDATA[expload 函数1234567&lt;?phpfunction getExt($path)&#123; $ext = explode('.', $path); return end($ext);&#125; strrpos 函数12345&lt;?phpfunction getExt($path)&#123; return substr($path, strrpos($path, '.')+1);&#125; strrchr 函数12345&lt;?phpfunction getExt($path)&#123; return substr(strrchr($path, '.'), 1);&#125; pathinfo 函数12345&lt;?phpfunction getExt($path)&#123; return pathinfo($path, PATHINFO_EXTENSION);&#125;]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>ext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL Explain详解]]></title>
    <url>%2F2016%2F03%2F29%2Fdocs%2F04-mysql%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[简介使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理你的SQL语句的。分析你的查询语句或是表结构的性能瓶颈。 通过EXPLAIN，我们可以分析出以下结果 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 使用方式如下EXPLAIN +SQL语句 123456789mysql&gt; explain select * from test where name = 10000;+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | test | NULL | ALL | NULL | NULL | NULL | NULL | 378000 | 10.00 | Using where |+----+-------------+-------+------------+------+---------------+------+---------+------+--------+----------+-------------+1 row in set, 1 warning (0.00 sec)mysql&gt; 执行计划各字段含义概要描述 字段 描述 id 选择标识符 select_type 表示查询的类型 table 输出结果集的表 partitions 匹配的分区 type 表示表的连接类型 possible_keys 表示查询时，可能使用的索引 key 表示实际使用的索引 key_len 索引字段的长度 ref 列与索引的比较 rows 扫描出的行数(估算的行数) filtered 按表条件过滤的行百分比 Extra 执行情况的描述和说明 idselect查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序 id相同时，执行顺序由上至下 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 select_type表示查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询 select_type 描述 SIMPLE 简单SELECT，不使用UNION或子查询等 PRIMARY 子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY UNION UNION中的第二个或后面的SELECT语句 DEPENDENT UNION UNION中的第二个或后面的SELECT语句，取决于外面的查询 UNION RESULT UNION的结果，union语句中第二个select开始后面所有select SUBQUERY 子查询中的第一个SELECT，结果不依赖于外部查询 DEPENDENT SUBQUERY 子查询中的第一个SELECT，依赖于外部查询 DERIVED 派生表的SELECT, FROM子句的子查询 UNCACHEABLE SUBQUERY 一个子查询的结果不能被缓存，必须重新评估外链接的第一行 table指的就是当前执行的表 type对表访问方式，表示MySQL在表中找到所需行的方式，又称”访问类型”。常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从左到右，性能从差到好） type 描述 ALL Full Table Scan， MySQL将遍历全表以找到匹配的行 index Full Index Scan，index与ALL区别为index类型只遍历索引树 range 只检索给定范围的行，使用一个索引来选择行 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 eq_ref 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件 const 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，MySQL就能将该查询转换为一个常量，system是const类型的特例，当查询的表只有一行的情况下，使用system system 表只有一行记录（等于系统表），这是const类型的特列，平时不会出现，这个也可以忽略不计 NULL MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 possible_keys指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用（该查询可以利用的索引，如果没有任何索引显示 null） 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询 Keykey列显示MySQL实际决定使用的键（索引），必然包含在possible_keys中 如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的） 不损失精确性的情况下，长度越短越好 ref列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数 Extra该列包含MySQL解决查询的详细信息,有以下几种情况： Using where:不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤 Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询，常见 group by ; order by Using filesort：当Query中包含 order by 操作，而且无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行（通过收集统计信息不可能存在结果）。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行 No tables used：Query语句中使用from dual 或不含任何from子句 总结 EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况 EXPLAIN不考虑各种Cache EXPLAIN不能显示MySQL在执行查询时所作的优化工作 部分统计信息是估算的，并非精确值 EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>Explain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 索引详解]]></title>
    <url>%2F2016%2F03%2F28%2Fdocs%2F04-mysql%2Fmysql-index%2F</url>
    <content type="text"><![CDATA[介绍什么是索引？一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，在生产环境中，我们遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，因此对查询语句的优化显然是重中之重。说起加速查询，就不得不提到索引了。 为什么要有索引呢？索引在MySQL中也叫做“键”，是存储引擎用于快速找到记录的一种数据结构。索引对于良好的性能非常关键，尤其是当表中的数据量越来越大时，索引对于性能的影响愈发重要。索引优化应该是对查询性能优化最有效的手段了。索引能够轻易将查询性能提高好几个数量级。索引相当于字典的音序表，如果要查某个字，如果不使用音序表，则需要从几百页中逐页去查。 索引的分类 普通索引 唯一索引 联合索引 全文索引 空间索引 索引的优缺点 优势： 可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序 劣势： 索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的1.5倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表 索引的实现原理哈希索引只有memory（内存）存储引擎支持哈希索引，哈希索引用索引列的值计算该值的hashCode，然后在hashCode相应的位置存执该值所在行数据的物理位置，因为使用散列算法，因此访问速度非常快，但是一个值只能对应一个hashCode，而且是散列的分布方式，因此哈希索引不支持范围查找和排序的功能。 全文索引FULLTEXT（全文）索引，仅可用于MyISAM和InnoDB，针对较大的数据，生成全文索引非常的消耗时间和空间。对于文本的大对象，或者较大的CHAR类型的数据，如果使用普通索引，那么匹配文本前几个字符还是可行的，但是想要匹配文本中间的几个单词，那么就要使用LIKE %word%来匹配，这样需要很长的时间来处理，响应时间会大大增加，这种情况，就可使用时FULLTEXT索引了，在生成FULLTEXT索引时，会为文本生成一份单词的清单，在索引时及根据这个单词的清单来索引。FULLTEXT可以在创建表的时候创建，也可以在需要的时候用ALTER或者CREATE INDEX来添加。 BTree索引和B+Tree索引BTree索引BTree是平衡搜索多叉树，设树的度为2d（d&gt;1），高度为h，那么BTree要满足以一下条件： 每个叶子结点的高度一样，等于h； 每个非叶子结点由n-1个key和n个指针point组成，其中d&lt;=n&lt;=2d,key和point相互间隔，结点两端一定是key； 叶子结点指针都为null； 非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据； B+Tree索引B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于： B+Tree中的非叶子结点不存储数据，只存储键值； B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址； B+Tree的每个非叶子节点由n个键值key和n个指针point组成； 聚簇索引和非聚簇索引分析了MySQL的索引结构的实现原理，然后我们来看看具体的存储引擎怎么实现索引结构的，MySQL中最常见的两种存储引擎分别是MyISAM和InnoDB，分别实现了非聚簇索引和聚簇索引。 聚簇索引的解释是:聚簇索引的顺序就是数据的物理存储顺序 非聚簇索引的解释是:索引顺序与数据物理排列顺序无关 索引的使用策略什么时候要使用索引 主键自动建立唯一索引； 经常作为查询条件在WHERE或者ORDER BY 语句中出现的列要建立索引； 作为排序的列要建立索引； 查询中与其他表关联的字段，外键关系建立索引 高并发条件下倾向组合索引； 用于聚合函数的列可以建立索引，例如使用了max(column_1)或者count(column_1)时的column_1就需要建立索引 什么时候不要使用索引 经常增删改的列不要建立索引； 有大量重复的列不建立索引； 表记录太少不要建立索引。只有当数据库里已经有了足够多的测试数据时，它的性能测试结果才有实际参考价值。如果在测试数据库里只有几百条数据记录，它们往往在执行完第一条查询命令之后就被全部加载到内存里，这将使后续的查询命令都执行得非常快–不管有没有使用索引。只有当数据库里的记录超过了1000条、数据总量也超过了MySQL服务器上的内存总量时，数据库的性能测试结果才有意义。 索引失效的情况 在组合索引中不能有列的值为NULL，如果有，那么这一列对组合索引就是无效的。 在一个SELECT语句中，索引只能使用一次，如果在WHERE中使用了，那么在ORDER BY中就不要用了。 LIKE操作中，’%aaa%’不会使用索引，也就是索引会失效，但是‘aaa%’可以使用索引。 在索引的列上使用表达式或者函数会使索引失效，例如：select * from users where YEAR(adddate)&lt;2007，将在每个行上进行运算，这将导致索引失效而进行全表扫描，因此我们可以改成：select * from users where adddate&lt;’2007-01-01′。其它通配符同样，也就是说，在查询条件中使用正则表达式时，只有在搜索模板的第一个字符不是通配符的情况下才能使用索引。 在查询条件中使用不等于，包括&lt;符号、&gt;符号和！=会导致索引失效。特别的是如果对主键索引使用！=则不会使索引失效，如果对主键索引或者整数类型的索引使用&lt;符号或者&gt;符号不会使索引失效。（经erwkjrfhjwkdb同学提醒，不等于，包括&lt;符号、&gt;符号和！，如果占总记录的比例很小的话，也不会失效） 在查询条件中使用IS NULL或者IS NOT NULL会导致索引失效。 字符串不加单引号会导致索引失效。更准确的说是类型不一致会导致失效，比如字段email是字符串类型的，使用WHERE email=99999 则会导致失败，应该改为WHERE email=’99999’。 在查询条件中使用OR连接多个条件会导致索引失效，除非OR链接的每个条件都加上索引，这时应该改为两次查询，然后用UNION ALL连接起来。 如果排序的字段使用了索引，那么select的字段也要是索引字段，否则索引失效。特别的是如果排序的是主键索引则select * 也不会导致索引失效。尽量不要包括多列排序，如果一定要，最好为这队列构建组合索引； 索引的语法查看一张表的索引123用法：show index from TABLE_NAME;mysql&gt; show index from users; 建立索引1234567891011121314151617181920用法# 普通索引 create index INDEX_NAME on TABLE_NAME(字段名);# 唯一索引 create unique INDEX_NAME on TABLE_NAME(字段名);# 全文索引 create fulltext INDEX_NAME on TABLE_NAME(字段名);# 多列索引 create index INDEX_NAME on TABLE_NAME(字段名,字段名);mysql&gt; create index index_name on user(name, age);创建表时直接创建索引CREATE TABLE school( NAME VARCHAR(8) NOT NULL , sid INT PRIMARY KEY auto_increment NOT NULL , age INT NOT NULL , sex ENUM(&apos;F&apos; , &apos;M&apos;) , INDEX(sid , NAME));在已存在的表上添加索引mysql&gt; alter table school add index index_name(name); 删除索引1mysql&gt; drop index INDEX_NAME on TABLE_NAME; 查看查询语句使用索引的情况1mysql&gt; explain SELECT * FROM TABLE_NAME WHERE COLUMN_NAME = &apos;123&apos;; 索引测试准备 创建表 123456CREATE TABLE test( id INT , name VARCHAR(20) , gender CHAR(6) , email VARCHAR(50)); 创建存储过程，实现批量插入记录 123456789101112131415161718delimiter $$ #声明存储过程的结束符号为$$CREATE PROCEDURE auto_insert()BEGINDECLARE i INT DEFAULT 1 ;WHILE(i &lt; 1000000) DO INSERT INTO s1VALUES ( i , concat(&apos;egon&apos; , i) , &apos;male&apos; , concat(&apos;egon&apos; , i , &apos;@oldboy&apos;) ) ;SET i = i + 1 ;ENDWHILE ; END$$ #$$结束delimiter ; #重新声明分号为结束符号 调用存储过程 1call auto_insert(); 在没有索引的前提下测试查询速度123456789mysql&gt; select * from test where id = 10000;+-------+-----------+--------+------------------+| id | name | gender | email |+-------+-----------+--------+------------------+| 10000 | egon10000 | male | egon10000@oldboy |+-------+-----------+--------+------------------+1 row in set (0.03 sec)mysql&gt; 加上索引12345678910111213mysql&gt; create index idx on test(id);Query OK, 0 rows affected (0.60 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; select * from test where id = 10000;+-------+-----------+--------+------------------+| id | name | gender | email |+-------+-----------+--------+------------------+| 10000 | egon10000 | male | egon10000@oldboy |+-------+-----------+--------+------------------+1 row in set (0.00 sec)mysql&gt; 相关文章 https://www.runoob.com/mysql/mysql-index.htmlhttps://www.cnblogs.com/bypp/p/7755307.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP状态码]]></title>
    <url>%2F2016%2F03%2F01%2Fdocs%2F09-pc-base%2Fhttp-status-codes%2F</url>
    <content type="text"><![CDATA[简介当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。 HTTP状态码的英文为HTTP Status Code。 分类HTTP状态码（HTTP Status Code）是用以表示网页服务器HTTP响应状态的3位数字代码。它由 RFC 2616 规范定义的，并得到RFC 2518、RFC 2817、RFC 2295、RFC 2774、RFC 4918等规范扩展。 所有状态码的第一个数字代表了响应的五种状态之一。 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 HTTP状态码列表 状态码 含义 100 客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101 服务器已经理解了客户端的请求，并将通过Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。 只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 200 请求已成功，请求所希望的响应头或数据体将随此响应返回。 201 请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 ‘202 Accepted’。 202 服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。 返回202状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回202状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203 服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。例如，包含资源的元数据可能导致原始服务器知道元信息的超级。使用此状态码不是必须的，而且只有在响应不使用此状态码便会返回200 OK的情况下才是合适的。 204 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。 如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。 由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 服务器成功处理了请求，且没有返回任何内容。但是与204响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 与204响应一样，该响应也被禁止包含任何消息体，且以消息头后的第一个空行结束。 206 服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。 该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 响应必须包含如下的头部域： Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。 Date ETag 和/或 Content-Location，假如同样的请求本应该返回200响应。 Expires, Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回200响应中应当返回的所有实体头部域。 假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将206响应返回的内容与之前任何缓存过的内容组合在一起。 任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存206响应返回的内容。 207 由WebDAV(RFC 2518)扩展的状态码，代表之后的消息体将是一个XML消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 300 被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 除非这是一个 HEAD 请求，否则该响应应当包括一个资源特性及地址的列表的实体，以便用户或浏览器从中选择最合适的重定向地址。这个实体的格式由 Content-Type 定义的格式所决定。浏览器可能根据响应的格式以及浏览器自身能力，自动作出最合适的选择。当然，RFC 2616规范并没有规定这样的自动选择该如何进行。 如果服务器本身已经有了首选的回馈选择，那么在 Location 中应当指明这个回馈的 URI；浏览器可能会将这个 Location 值作为自动重定向的地址。此外，除非额外指定，否则这个响应也是可缓存的。 301 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 新的永久性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，因此浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：对于某些使用 HTTP/1.0 协议的浏览器，当它们发送的 POST 请求得到了一个301响应的话，接下来的重定向请求将会变成 GET 方式。 302 请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 如果这不是一个 GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 注意：虽然RFC 1945和RFC 2068规范不允许客户端在重定向时改变请求的方法，但是很多现存的浏览器将302响应视作为303响应，并且使用 GET 方式访问在 Location 中规定的 URI，而无视原先请求的方法。状态码303和307被添加了进来，用以明确服务器期待客户端进行何种反应。 303 对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303响应禁止被缓存。当然，第二个请求（重定向）可能被缓存。 新的 URI 应当在响应的 Location 域中返回。除非这是一个 HEAD 请求，否则响应的实体中应当包含指向新的 URI 的超链接及简短说明。 注意：许多 HTTP/1.1 版以前的 浏览器不能正确理解303状态。如果需要考虑与这些浏览器之间的互动，302状态码应该可以胜任，因为大多数的浏览器处理302响应时的方式恰恰就是上述规范要求客户端处理303响应时应当做的。 304 如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 该响应必须包含以下的头信息： Date，除非这个服务器没有时钟。假如没有时钟的服务器也遵守这些规则，那么代理服务器以及客户端可以自行将 Date 字段添加到接收到的响应头中去（正如RFC 2068中规定的一样），缓存机制将会正常工作。 ETag 和/或 Content-Location，假如同样的请求本应返回200响应。 Expires, Cache-Control，和/或Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。 假如本响应请求使用了强缓存验证，那么本次响应不应该包含其他实体头；否则（例如，某个带条件的 GET 请求使用了弱缓存验证），本次响应禁止包含其他实体头；这避免了缓存了的实体内容和更新了的实体头信息之间的不一致。 假如某个304响应指明了当前某个实体没有缓存，那么缓存系统必须忽视这个响应，并且重复发送不包含限制条件的请求。 假如接收到一个要求更新某个缓存条目的304响应，那么缓存系统必须更新整个条目以反映所有在响应中被更新的字段的值。 305 被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立305响应。 注意：RFC 2068中没有明确305响应是为了重定向一个单独的请求，而且只能被原始服务器建立。忽视这些限制可能导致严重的安全后果。 306 在最新版的规范中，306状态码已经不再被使用。 307 请求的资源现在临时从不同的URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 新的临时性的URI 应当在响应的 Location 域中返回。除非这是一个HEAD 请求，否则响应的实体中应当包含指向新的URI 的超链接及简短说明。因为部分浏览器不能识别307响应，因此需要添加上述必要信息以便用户能够理解并向新的 URI 发出访问请求。 如果这不是一个GET 或者 HEAD 请求，那么浏览器禁止自动进行重定向，除非得到用户的确认，因为请求的条件可能因此发生变化。 400 1、语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 2、请求参数有误。 401 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么401响应代表着服务器验证已经拒绝了那些证书。如果401响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。参见RFC 2617。 402 该状态码是为了将来可能的需求而预留的。 403 服务器已经理解请求，但是拒绝执行它。与401响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。如果这不是一个 HEAD 请求，而且服务器希望能够讲清楚为何请求不能被执行，那么就应该在实体内描述拒绝的原因。当然服务器也可以返回一个404响应，假如它不希望让客户端获得任何信息。 404 请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。假如服务器知道情况的话，应当使用410状态码来告知旧资源因为某些内部的配置机制问题，已经永久的不可用，而且没有任何可以跳转的地址。404这个状态码被广泛应用于当服务器不想揭示到底为何请求被拒绝或者没有其他适合的响应可用的情况下。 405 请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 鉴于 PUT，DELETE 方法会对服务器上的资源进行写操作，因而绝大部分的网页服务器都不支持或者在默认配置下不允许上述请求方法，对于此类请求均会返回405错误。 406 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 除非这是一个 HEAD 请求，否则该响应就应当返回一个包含可以让用户或者浏览器从中选择最合适的实体特性以及地址列表的实体。实体的格式由 Content-Type 头中定义的媒体类型决定。浏览器可以根据格式及自身能力自行作出最佳选择。但是，规范中并没有定义任何作出此类自动选择的标准。 407 与401响应类似，只不过客户端必须在代理服务器上进行身份验证。代理服务器必须返回一个 Proxy-Authenticate 用以进行身份询问。客户端可以返回一个 Proxy-Authorization 信息头用以验证。参见RFC 2617。 408 请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409 由于和被请求的资源的当前状态之间存在冲突，请求无法完成。这个代码只允许用在这样的情况下才能被使用：用户被认为能够解决冲突，并且会重新提交新的请求。该响应应当包含足够的信息以便用户发现冲突的源头。 冲突通常发生于对 PUT 请求的处理中。例如，在采用版本检查的环境下，某次 PUT 提交的对特定资源的修改请求所附带的版本信息与之前的某个（第三方）请求向冲突，那么此时服务器就应该返回一个409错误，告知用户请求无法完成。此时，响应实体中很可能会包含两个冲突版本之间的差异比较，以便用户重新提交归并以后的新版本。 410 被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。这样的状况应当被认为是永久性的。如果可能，拥有链接编辑功能的客户端应当在获得用户许可后删除所有指向这个地址的引用。如果服务器不知道或者无法确定这个状况是否是永久的，那么就应该使用404状态码。除非额外说明，否则这个响应是可缓存的。 410响应的目的主要是帮助网站管理员维护网站，通知用户该资源已经不再可用，并且服务器拥有者希望所有指向这个资源的远端连接也被删除。这类事件在限时、增值服务中很普遍。同样，410响应也被用于通知客户端在当前服务器站点上，原本属于某个个人的资源已经不再可用。当然，是否需要把所有永久不可用的资源标记为’410 Gone’，以及是否需要保持此标记多长时间，完全取决于服务器拥有者。 411 服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412 服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。这个状态码允许客户端在获取资源时在请求的元信息（请求头字段数据）中设置先决条件，以此避免该请求方法被应用到其希望的内容以外的资源上。 413 服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。此种情况下，服务器可以关闭连接以免客户端继续发送此请求。 如果这个状况是临时的，服务器应当返回一个 Retry-After 的响应头，以告知客户端可以在多少时间以后重新尝试。 414 请求的URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。这比较少见，通常的情况包括： 本应使用POST方法的表单提交变成了GET方法，导致查询字符串（Query String）过长。 重定向URI “黑洞”，例如每次重定向把旧的 URI 作为新的 URI 的一部分，导致在若干次重定向后 URI 超长。 客户端正在尝试利用某些服务器中存在的安全漏洞攻击服务器。这类服务器使用固定长度的缓冲读取或操作请求的 URI，当 GET 后的参数超过某个数值后，可能会产生缓冲区溢出，导致任意代码被执行[1]。没有此类漏洞的服务器，应当返回414状态码。 415 对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416 如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回416状态码。 假如 Range 使用的是字节范围，那么这种情况就是指请求指定的所有数据范围的首字节位置都超过了当前资源的长度。服务器也应当在返回416状态码的同时，包含一个 Content-Range 实体头，用以指明当前资源的长度。这个响应也被禁止使用 multipart/byteranges 作为其 Content-Type。 417 在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 从当前客户端所在的IP地址到服务器的连接数超过了服务器许可的最大范围。通常，这里的IP地址指的是从服务器上看到的客户端地址（比如用户的网关或者代理服务器地址）。在这种情况下，连接数的计算可能涉及到不止一个终端用户。 422 请求格式正确，但是由于含有语义错误，无法响应。（RFC 4918 WebDAV）423 Locked 当前资源被锁定。（RFC 4918 WebDAV） 424 由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。（RFC 4918 WebDAV） 425 在WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426 客户端应当切换到TLS/1.0。（RFC 2817） 449 由微软扩展，代表请求应当在执行完适当的操作后进行重试。 499 An Nginx HTTP server extension. This code is introduced to log the case when the connection is closed by client while HTTP server is processing its request, making server unable to send the HTTP header back.。 500 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 注意：某些代理服务器在DNS查询超时时会返回400或者500错误 505 服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506 由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507 服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509 服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510 获取资源所需要的策略并没有没满足。（RFC 2774） 相关文章 http://tool.oschina.net/commons?type=5http://www.httpstatus.cnhttps://www.runoob.com/http/http-status-codes.html]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AB 性能测试工具使用说明]]></title>
    <url>%2F2016%2F02%2F25%2Fdocs%2F02-tools%2Fab%2F</url>
    <content type="text"><![CDATA[前言在学习ab工具之前，我们需了解几个关于压力测试的概念 吞吐率（Requests per second）概念：服务器并发处理能力的量化描述，单位是reqs/s，指的是某个并发用户数下单位时间内处理的请求数。某个并发用户数下单位时间内能处理的最大请求数，称之为最大吞吐率。计算公式：总请求数 / 处理完成这些请求数所花费的时间，即Request per second = Complete requests / Time taken for tests 并发连接数（The number of concurrent connections）概念：某个时刻服务器所接受的请求数目，简单的讲，就是一个会话。 并发用户数（The number of concurrent users，Concurrency Level）概念：要注意区分这个概念和并发连接数之间的区别，一个用户可能同时会产生多个会话，也即连接数。 用户平均请求等待时间（Time per request）计算公式：处理完成所有请求数所花费的时间/ （总请求数 / 并发用户数），即Time per request = Time taken for tests /（ Complete requests / Concurrency Level） 服务器平均请求等待时间（Time per request: across all concurrent requests）计算公式：处理完成所有请求数所花费的时间 / 总请求数，即Time taken for / testsComplete requests可以看到，它是吞吐率的倒数。同时，它也=用户平均请求等待时间/并发用户数，即Time per request / Concurrency Level 简介ab是Apache自带的压力测试工具。ab命令会创建很多的并发访问线程，模拟多个访问者同时对某一URL地址进行访问。它的测试目标是基于URL的，因此，既可以用来测试Apache的负载压力，也可以测试nginx、lighthttp、tomcat、IIS等其它Web服务器的压力。 ab命令对发出负载的计算机要求很低，既不会占用很高CPU，也不会占用很多内存，但却会给目标服务器造成巨大的负载，其原理类似CC攻击。自己测试使用也须注意，否则一次上太多的负载，可能造成目标服务器因资源耗完，严重时甚至导致死机 安装AB是一条命令，集成在了HTTPD-Tools工具中。 系统版本：CentOS Linux release 7.3.1611 (Core)软件版本：httpd-tools-2.4.6硬件要求：无 安装YUM-EPEL源 1$ yum -y install epel-release.noarch 安装HTTPD-Tools 1$ yum -y install httpd-tools AB命令详解1234567891011121314151617181920212223242526272829语法: ab [选项] 访问地址选项: -n requests 要执行的请求总数。 -c concurrency 一次发出多个请求的数量。 -t timelimit 设置测试的最大时长，单位为秒。 -s timeout 设置响应超时时间，单位为秒，默认为30秒。 -b windowsize 设置TCP发送/接受缓冲区的大小，单位为字节。 -B address 当有多张网卡时，设置进行传出连接时绑定的网卡地址。 -p postfile 设置POST请求包含的数据文件。需要与&quot;-T&quot;选项一起连用。 -u putfile 设置PUT请求所包含的数据文件。需要与&quot;-T&quot;选项一起连用。 -T content-type 设置POST/PUT请求时指定的数据MIME类型的头部信息,默认值为&quot;text/plain&quot;。 -w 以HTML网页的方式显示结果。 -i 使用HEAD方法请求而不是GET方法。 -C attribute 添加cookie,例如:&apos;Apache=1234&apos;。 -H attribute 添加任意标头，例如:&apos;Accept-Encoding:gzip&apos;,在所有正常标题行之后插入此信息。 -A attribute 添加基本的网络身份验证，属性是以冒号分割的用户名和密码。 -P attribute 添加基本的代理身份验证，属性是以冒号分割的用户名和密码。 -X proxy:port 设置要使用的代理服务器地址和端口。 -V 显示软件版本号并退出。 -k 使用HTTP Keepalive保持长连接功能。 -d 不显示百分比报告信息。 -S 不显示置信度估算值和警告信息。 -q 执行超过150个请求不要显示进度。 -g filename 将收集到的数据输出到Gnuplot格式的文件中。 -e filename 将收集到的数据输出到已提供百分比的CSV文件中。 -r 在接受到套接字错误不会退出。 -h 显示使用信息。 -Z ciphersuite 设置SSL/TLS密码套件（请参阅OpenSSL）。 -f protocol 设置支持的SSL/TLS协议(SSL3, TLS1, TLS1.1, TLS1.2 or ALL)。 开始测试我们在进行压力测试的时候应合理的从小到大一段一段测试WEB服务器负载，并通过压力测试了解WEB服务器的资源瓶颈，根据业务需求合理的采购服务器。 调整系统限制系统默认情况下允许一个进程同时打开的文件描述符数量是1024，若我们需要进行超过1024并发测试的话，可能需要调高此值，客户端和服务端都需要调整。 12345678910111213$ ulimit -n 65535 =&gt; 设置进程并发文件数（当前生效）$ vim /etc/security/limits.conf =&gt; 设置进程并发文件数（永久生效）# 用户 软硬限制 限制类型 值root soft nofile 65535root hard nofile 65535* soft nofile 65535* hard nofile 65535# * 表示所有用户。# soft 软限制，当用户使用超出设定值系统会发出告警。# hard 硬限制，绝对限制，用户使用绝对不能超出设置的值。# nofile 限制类型，进程可同时打开的并发文件描述符数量。$ ulimit -n 65535 输入命令1$ ab -n 100 -c 10 ab -n 100 -c 10 https://www.baidu.com/ 其中－n表示请求数，－c表示并发数 测试结果分析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071This is ApacheBench, Version 2.3 &lt;$Revision: 1826891 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking www.baidu.com (be patient).....done// 请求返回header类型，可能是nginx、apache、IIs等Server Software: BWS/1.1 // 请求ip或者域名Server Hostname: www.baidu.com// 请求端口，当前请求为https所以端口为443，请求https端口80Server Port: 443// https端口协议SSL/TLS Protocol: TLSv1.2,ECDHE-RSA-AES128-GCM-SHA256,2048,128TLS Server Name: www.baidu.com// 路径Document Path: /// 第一个成功返回的文档的字节大小Document Length: 227 bytes// 并发数！！！Concurrency Level: 10// 从建立连接到最后接受完成总时间Time taken for tests: 0.818 seconds// 总请求数成功的Complete requests: 100// 失败的Failed requests: 0// 从服务器接收的字节总数Total transferred: 89300 bytes// HTML接收字节数HTML transferred: 22700 bytes// 每秒请求数（总请求数/总时间）Requests per second: 122.26 [#/sec] (mean)// 用户平均请求等待时间=concurrency * timetaken * 1000 / doneTime per request: 81.793 [ms] (mean)// 服务器处理每个请求平均响应时间=timetaken * 1000 / doneTime per request: 8.179 [ms] (mean, across all concurrent requests)Transfer rate: 106.62 [Kbytes/sec] received// 网络连接情况Connection Times (ms) min mean[+/-sd] median maxConnect: 27 45 20.7 40 159Processing: 7 17 9.5 15 89Waiting: 7 15 8.8 13 89Total: 40 61 23.2 55 179// 整体响应时间的分布比Percentage of the requests served within a certain time (ms) 50% 55 66% 59 75% 63 80% 65 90% 73 95% 122 98% 174 99% 179 100% 179 (longest request) 总结总的来说ab工具ab小巧简单，上手学习较快，可以提供需要的基本性能指标，但是没有图形化结果，不能监控。因此ab工具可以用作临时紧急任务和简单测试。同类型的压力测试工具还有：webbench、siege、http_load等]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>AB</tag>
        <tag>测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 修改root密码的几种方法]]></title>
    <url>%2F2016%2F01%2F25%2Fdocs%2F04-mysql%2Fmysql-root-password-update%2F</url>
    <content type="text"><![CDATA[SET PASSWORD 命令123格式:mysql&gt; SET PASSWORD FOR 用户名@localhost = PASSWORD(&apos;新密码&apos;) mysql&gt; SET PASSWORD FOR root@localhost = PASSWORD(&apos;123456&apos;); 使用 mysqladmin 命令123格式:mysqladmin -u用户名 -p旧密码 password 新密码; $ mysqladmin -uroot -p123456 password 12345678; 使用 update 直接编辑 user 表123mysql&gt; USE mysql; mysql&gt; UPDATE user SET password = password(&apos;123456&apos;) WHERE user = &apos;root&apos; AND host = &apos;loclhoast&apos;; mysql&gt; flush privileges; 在忘记root密码的时候,可以这样Linux12345678910111213# Stop MySQLsudo service mysql stop# Make MySQL service directory.sudo mkdir /var/run/mysqld# Give MySQL user permission to write to the service directory.sudo chown mysql: /var/run/mysqld# Start MySQL manually, without permission checks or networking.sudo mysqld_safe --skip-grant-tables --skip-networking &amp;# Log in without a password.mysql -uroot mysqlUPDATE mysql.user SET authentication_string=PASSWORD('YOURNEWPASSWORD'), plugin='mysql_native_password' WHERE User='root' AND Host='%';EXIT; windows 关闭正在运行的MySQL服务 打开DOS窗口,转到mysql/bin目录 输入mysqld --skip-grant-tables回车。--skip-grant-tables的意思是启动MySQL服务的时候跳过权限表认证 再打开一个DOS窗口(因为刚才那个DOS窗口已经不能动啦),转到mysql/bin目录 输入mysql回车,如果成功,将出现MySQL提示符 &gt; 修改密码:UPDATE user SET password = password(&#39;123456&#39;) WHERE user = &#39;root&#39; 刷新权限(必须):flush priviliges; 退出:quit 注销系统,再进入使用户名root和刚才设置的新密码123456登录 相关文章 https://coderwall.com/p/j9btlg/reset-the-mysql-5-7-root-password-in-ubuntu-16-04-lts]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 数据库安装]]></title>
    <url>%2F2016%2F01%2F18%2Fdocs%2F04-mysql%2Fmysql-install%2F</url>
    <content type="text"><![CDATA[安装Oracle官方的yum源12$ wget http://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm$ rpm -ivh mysql-community-release-el6-5.noarch.rpm 安装MySQL服务器端和客户端1$ yum -y install mysql-server mysql 配置服务启动1$ chkconfig mysqld on 初始化安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768$ service mysqld start第一次启动时会自动创建初始化数据库，启动后执行 /usr/bin/mysql_secure_installation回车后显示如下信息： NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MySQL to secure it, we'll need the current password for the root user. If you've just installed MySQL, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none):（直接回车）执行后会提示输入当前密码，当前密码为空，直接回车 Set root password? [Y/n] （输入Y）回车后会提示是否设置root密码，输入“Y”回车，然后输入root的密码 New password: Re-enter new password:输入两次密码，回车后显示如下信息： By default, a MySQL installation has an anonymous user, allowing anyone to log into MySQL without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n]（输入Y）输入Y移除匿名用户 Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n]（输入n）输入n，不禁止root远程登录 By default, MySQL comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n]（输入Y）输入Y移除测试数据库，可能有错误信息，可以忽略 Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n]（输入Y）输入Y重载数据库权限 All done! If you've completed all of the above steps, your MySQL installation should now be secure. Thanks for using MySQL! Cleaning up...安装成功 配置文件1$ find / -name my.cnf 查找my.cnf的位置，确保只有/etc/my.cnf一个文件，如果在其他目录下还有该文件，将其他目录下的文件删除，只保留/etc/my.cnf 修改配置文件中主要是innodb_buffer_pool_size项的配置，如果该服务器为数据库专用服务器，则将该项配置为服务器内存的70%左右，例如服务器内存为16G的，可以将此项配置为12G，然后服务器内存为32G的，可以将此项配置为24G、26G或28G 删除/var/lib/mysql/下的ib_logfile0、ib_logfile1文件 123456$ service mysqld restart#进入mysql命令行$ mysql -uroot -p#查看InnoDB引擎是否启用mysql&gt; show engines; Engine Support MRG_MYISAM YES CSV YES MyISAM DEFAULT InnoDB YES MEMORY YES 如果看到InnoDB为yes即正常启用了，如果不为yes，则说明未启用，说明安装有问题。 完成安装 配置远程登录1234567$ service mysqld startmysql -uroot -p（不允许直接在-p后面直接带上密码，这样容易导致密码泄露，因为可以通过history看历史命令获取该密码）输入密码use mysql//将Host为localhost的记录改为允许任意主机访问%update user set Host='%' where Host='localhost';flush privileges; 注意：远程访问必须确认主机的防火墙中已开启3306端口的访问，否则将无法远程访问数据库]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在IDE上安装代码规范检查工具]]></title>
    <url>%2F2016%2F01%2F01%2Fdocs%2F02-tools%2Fphpcs-ide-setting%2F</url>
    <content type="text"><![CDATA[我相信每个公司都有一套完备的代码规范标准，但标准是标准，如何能有效的让所有人遵守，那就要工具的辅助和实时提醒了。 安装phpcs使用composer全局安装phpcs1$ composer global require "squizlabs/php_codesniffer=*" 具体可参考：https://github.com/squizlabs/PHP_CodeSniffer IDE集成PHPStorm 设置 (适用mac) 打开PHPStorm点击 PhpStorm -&gt; Preference; 点击 Languages &amp; Frameworks -&gt; PHP -&gt; Code Sniffer; 点击 Configuration 右侧的按钮; 选择 PHP Code Sniffer（phpcs）path：的路径，就是刚才composer之后生成的那个phpcs(/vendor/squizlabs/php_codesniffer/bin/phpcs)的路径; 选择之后点击 Validate 验证成功; 继续点击 Editor -&gt; Inspections 展开点击右侧的PHP; 勾选 PHP Code Sniffer Validation 选择右侧的PSR2; 勾选 PHP Mess Detector Validation 右侧 Options 全部勾选; 点击Code Style -&gt; PHP -&gt; Set from... -&gt; Predefiend Style 选择 PSR1/PSR2 现在笔者使用phpstorm的格式化，将会自动格式化成psr-2的风格。 Sublime Text (适用mac) 安装Package Control command + shift + p 调出 安装界面 install package Preferences-&gt;Package Settings-&gt;PHP Code Sniffer-&gt;Settings - User(Default) 配置phpcs 路径 “phpcs_executable_path”: “/usr/local/bin/phpcs” 配置phpcbf 路径 “phpcbf_executable_path”: “/usr/local/bin/phpcbf” VSCode TODO 如果写的代码不符合PSR-2编码风格规范的时候，该行代码会有波浪线，点击波浪线可以查看提示信息，根据信息我们修改就可以写出优雅的代码了。 参考文章：https://segmentfault.com/a/1190000015971297]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
        <tag>phpcs</tag>
        <tag>代码检查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转）开发效率低？造成代码难以维护的35个恶习]]></title>
    <url>%2F2016%2F01%2F01%2Fdocs%2F01-program-life%2F01001%2F</url>
    <content type="text"><![CDATA[代码组织 总是说”一会弄好”，但从来不兑现。(缺乏任务管理和时间管理能力) 坚持所谓的高效、优雅的”一行代码流”，事实上，可读性才是最重的，聪明是第二位的。 无意义的优化。(类似网页大小之类的优化最后在做) 不注重代码样式和风格的严谨。 使用无意义的命名。 忽略经过验证的最佳实践。(例如代码审核、TDD、QA、自动化部署等，推荐阅读软件开发必读经典著作：Making Software：What Really Works，and Why We Believe It) 给自己埋雷。(例如使用不会报错的类库或者忽略例外) 团队工作 过早放弃计划。 坚持一个无效的计划。 总是单打独斗。(必须强迫自己于团队分享进度和想法，避免错觉，提高效率) 拒绝写糟糕的代码。(日程紧迫的时候可以写一些”糟糕”的代码，这是程序员的能力而不是bug，当然，有时间的时候一定要回头偿还”技术债”) 抱怨他人。 不与团队分享所学。 向主管/客户反馈的速度过慢。 不会充分利用Google。 看重个人编码风格。 带着个人情绪看待他人对自己代码的评论和注释。 写代码 不懂优化策略。 使用错误的工具。 不追求对开发工具和IDE的精熟。 忽略报错信息 迷恋趁手的开发工具。(不同类型的开发任务需要匹配对应的最佳开发工具，例如Sublime适合动态语言，而Eclipse适合Java，如果你喜欢vim或emacs，并不意味着能用这些工具干所有事) 不注重代码中赋值的可配置型。(不养成把代码中的活动部件分离出来的习惯，会导致技术债暴增) 喜欢重新发明车轮。 盲目的剪切/粘贴代码。 应付差事，不求甚解，不花时间搞清楚项目运作的机理。 对自己写的代码过度的自信。 不去考虑每一个设计、方案或者代码库的”副作用”。(一个成功的用例并不意味着”万灵药”) 在一个地方卡住了但坚持不呼救。 测试与维护 只去写能通过的测试。 重要项目中忽略性能测试。 不去核实代码是否真的可用，没有养成开发中及时快速测试的习惯。 重大改进延迟推送。 抛弃和逃避自己的代码。 忽略其他非功能性需求。(例如安全和性能，准备一份这方面的清单，忽略这些会毁掉你的所有成果) 点击查看原文]]></content>
      <categories>
        <category>程序人生</category>
      </categories>
      <tags>
        <tag>程序人生</tag>
        <tag>开发效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文章列表]]></title>
    <url>%2F2016%2F01%2F01%2Fdocs%2FREADME%2F</url>
    <content type="text"><![CDATA[目录 程序人生 开发效率低？造成代码难以维护的35个恶习 工具 在IDE上安装代码规范检查工具 AB 性能测试工具使用说明 PsySh PHP交互控制台 vagrant+phpstorm+xdebug断点调试 Git 客户端多账号管理 CentOS7.4搭建shadowsocks，以及配置BBR加速 TODO Docker 入门 TODO 使用 Docker 快速搭建PHP开发环境 PHP PHP 面向对象和面向过程的区别 PHP 垃圾回收机制 PHP 截取文件后缀的几种方法 PHP 关于 self 和 static PHP 自动加载原理解析 PHP 7 新特性 限制IP某个时间段内访问的次数 Laravel Facades 原理解析 PHP 抽象类和接口的区别 PHP 获取指定文件下所有文件 PHP 中 echo、print、print_r()、var_dump() 的区别 MySQL MySQL 数据库安装 MySQL 修改root密码的几种方法 MySQL 事务详解 MySQL 索引详解 MySQL 数据库字符集 utf8 和 utf8mb4 的区别 MySQL Explain详解 比较全面的MySQL优化参考 （转）MySQL优化原理 MySQL 锁机制总结 MySQL InnoDB行级锁实现 MySQL 如何解决脏读 MySQL 如何解决主从延迟 MySQL 分布式事务 NoSQL MongoDB 基础教程 MongoDB 复制（副本集） MongoDB 分片 Linux Homestead 下安装Swoole扩展 TODO Nginx+php-fpm 运行原理 AWK 简明教程 Docker+Nginx+Keepalived实现高可用架构 Redis Redis 应用场景 一文理解 Redis 持久化 使用 Docker 搭建 Redis 主从集群哨兵监控 Redis 为什么选择单线程模型 计算机基础 HTTP状态码 HTTP和HTTPS的区别与联系 TODO 从输入 URL 到页面加载完成的过程中都发生了什么事情？ 正则表达式速查表 COOKIE、SESSION、TOKEN各自的优缺点都有哪些？ 关于 RESTful API 设计的总结 单点登录（SSO） 数据结构与算法]]></content>
  </entry>
</search>
